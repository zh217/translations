
\part{The theory of exterior differential systems}
\label{part:theory-exter-diff}

\chapter{Exterior forms}
\label{cha:exterior-forms}

\section{Symmetric and alternating bilinear forms. Algebraic and exterior quadratic forms}
\label{sec:symm-altern-bilin}

\fsec In classical algebra,  a bilinear form \index{bilinear form} is an expression of two series of variables $u^{1}, u^{2}, \dots, u^{n}$ and $v^{1}, v^{2}, \dots, v^{n}$ of the same number $n$, of the form\footnote{We adopt the following now widespread convention from now on:  the summation sign in front of an expression that contains the same summation index repeated twice is suppressed, {e.g.}, in formula \eqref{eq:1},  the summation indices $i$ and $j$ are each repeated twice.}
\begin{equation}
  \label{eq:1}  
F(u,v)=a_{ij}u^{i}v^{j},
\end{equation}
where the coefficients $a_{ij}$ belong to a number field which, for simplicity, we assume to be the field of real numbers.

Such a form is  symmetric \index{bilinear form!symmetric} if it remains unchanged when we replace the variables $u^{i}$ with the variables $v^{i}$ with the corresponding indices and \emph{vice versa}, {i.e.}, if we have
\[
a_{ij}=a_{ji}\qquad(i,j=1,2,\dots,n).
\]

To a symmetric bilinear form we can associate a quadratic form \index{quadratic form}
\begin{equation}
  \label{eq:2}
  F(u)=a_{ij}u^{i}u^{j}
\end{equation}
with only a single series of variables. The form \eqref{eq:1} is the \emph{polar form} \index{polar form} of the quadratic form \eqref{eq:2}:
\[
F(u,v)=v^{i}\frac{\pd F}{\pd u^{i}}=u^{i}\frac{\pd F}{\pd v^{i}}.
\]

There is an \emph{intrinsic} relation between the form \eqref{eq:1} and the form \eqref{eq:2} in the sense that if we perform on the variables $u^{i}$ and $v^{i}$ the same linear transformation
\[
u^{i}=A^{i}{}_{k}U^{k},\qquad v^{i}=A^{i}{}_{k}v^{k},
\] 
then the bilinear form \eqref{eq:1} transforms into a bilinear form $\Phi(U,V)$, still symmetric, and the quadratic form \eqref{eq:2} into an associated quadratic form $\Phi(U)$. We have,
\begin{align*}
\Phi(U,V)&=a_{ij}A^{i}{}_{k}A^{j}{}_{h}U^{k}V^{h},\\
\Phi(U)&=a_{ij}A^{i}{}_{k}A^{j}{}_{h}U^{k}U^{h}.
\end{align*}

It can be verified easily that the form $\Phi(U,V)$ is symmetric, since the coefficient of $U^{k}V^{h}$, being the sum $a_{ij}A^{i}{}_{k}A^{j}{}_{h}$ where $i$ and $j$ are two independent summation indices, can be written as $a_{ji}A^{j}{}_{k}A^{i}{}_{h}=a_{ij}A^{i}{}_{h}A^{j}{}_{k}$, which is precisely the coefficient of $U^{h}V^{k}$. This verification can be avoided if we observe that exchanging the two series of variables $U^{i}$ and $V^{i}$ entails exchanging the two variables $u^{i}$ and $v^{i}$, which does not change the initial form $F(u,v)$.

\vspace{12pt}\fsec The form \eqref{eq:1} is  alternating \index{bilinear form!alternating} if it changes sign when the two series of variables $u^{i}$ and $v^{i}$ are swapped:
\[
F(v,u)=-F(u,v);
\]
and this translates into the antisymmetry of the coefficients:
\[
a_{ij}=-a_{ji}.
\]

If we now substitute $v^{i}=u^{i}$ we obtain a form that vanishes identically. Nonetheless we can still associate with an alternating form $F(u,v)$ a quadratic form,  but with \emph{anti-commutative} multiplication. To proceed, note that if we combine the two terms of $F(u,v)$
\[
a_{12}u^{1}v^{2}+a_{21}u^{2}v^{1},
\]
we obtain
\[
a_{12}(u^{1}v^{2}-u^{2}v^{1})=a_{12}
\begin{vmatrix}
  u^{1}&u^{2}\\
  v^{1}&v^{2}
\end{vmatrix}
,
\]
which we will write as $a_{12}[u^{1}u^{2}]$, the notation $[u^{1}u^{2}]$ denoting the determinant whose first line consists of the two variables $u^{1}, u^{2}$, and whose second line two other variables $v^{1}, v^{2}$. The expression $a_{12}[u^{1}u^{2}]$ may then  be regarded as a quadratic monomial formed by Grassmann's non-commutative multiplication, in which the product of two variables $u^{1}, u^{2}$ changes sign when the order of factors changes. We can, therefore, in this case associate with an alternating bilinear form
\[
F(u,v)=a_{ij}u^{i}v^{j}\qquad(a_{ij}=-a_{ji})
\]
a quadratic form of exterior multiplication, or, more briefly, an \emph{exterior quadratic form} \index{exterior form!quadratic}
\[
F(u)=\frac{1}{2}a_{ij}[u^{i}u^{j}]\qquad(a_{ij}=-a_{ji}).
\]
We have put in the numerical factor $\frac{1}{2}$ because in the second expression the product $[u^{1}u^{2}]$ enters twice, once in the form $[u^{1}u^{2}]$ and once in the form $[u^{2}u^{1}]=-[u^{1}u^{2}]$, which gives the total coefficient
\[
\frac{1}{2}a_{12}-\frac{1}{2}a_{21}=a_{12}.
\]

There is an intrinsic correspondence between alternating bilinear forms and quadratic exterior forms, and this correspondence persists when a linear change of variables is performed  to the two series of variables $u^{i}$ and $v^{i}$ at the same time. If we put
\[
u^{i}=A^{i}{}_{k}U^{k},\qquad v^{i}=A^{i}{}_{k}V^{k},
\]
the form $F(u,v)$ becomes
\[
\Phi(u,v)=a_{ij}A^{i}{}_{k}A^{j}{}_{h}U^{k}V^{h},
\]
with the coefficients of the products $U^{i}V^{j}$ antisymmetric, and the form $F(u)$ becomes
\[
\Phi(U)=\frac{1}{2}a_{ij}A^{i}{}_{k}A^{j}{}_{h}[U^{k}U^{h}];
\]
this formula can be deduced by replacing every $u^{i}$ in the form $F(u)$ by $A^{i}{}_{k}U^{k}$ and then multiplying the coefficients by following the usual rules of algebraic multiplication,  taking care not to invert the order of the variables $U^{i}$ in the resulting products.

\vspace{12pt}\fsec There are certain analogies between the classical quadratic forms, which we will call algebraic, and exterior quadratic forms. Let us define the partial derivative of an exterior quadratic form $F(u)$ by the relation
\[
\frac{\pd F}{\pd u^{i}}=a_{ik}u^{k}.
\]

The derivative of each monomial vanishes if $u^{i}$ does not appear in the factors of the monomial; if $u^{i}$ appears as the first factor, as in the term $a_{ij}[u^{i}u^{j}]$, the derivative is $a_{ij}u^{j}$; if $u^{i}$ appears as the second factor, we first pass $u^{i}$ to the front and  then apply the same rule: the derivative of $a_{ji}[u^{j}u^{i}]$ with respect to $u^{i}$ being the derivative of $-a_{ji}[u^{i}u^{j}]$, {i.e.}, $-a_{ji}u^{j}=a_{ij}u^{j}$. The derivative with respect to first $u^{i}$ and then $u^{j}$ is $a_{ij}$ and we write \footnote{The order of taking the derivative is wrong in the original work. --- \textsc{Translator.}}
\[
\frac{\pd ^{2}F}{\pd u^{j}\pd u^{i}}=a_{ij}=-\frac{\pd^{2}F}{\pd u^{i}\pd u^{j}}.
\]
Observe that the sum $u^{i}\pd F/\pd u^{i}=a_{ik}u^{i}u^{k}$ vanishes whereas, if the quadratic form is algebraic, the sum $u^{i}\pd F/\pd u^{i}$ equals $2F$, according to a theorem of Euler.

Suppose now that instead of multiplying $u^{i}$ and $\pd F/\pd u^{i}$ by ordinary multiplication, we perform exterior multiplication. As it is easy to see, we have, according to whether the form $F$ is algebraic or exterior,
\begin{equation}
  \label{eq:3}
  \left[u^{i}\frac{\pd F}{\pd u^{i}}\right]=0,\qquad
  \left[u^{i}\frac{\pd F}{\pd u^{i}}\right]=2F.
\end{equation}
\begin{thm*}
  An algebraic quadratic form $F$ satisfies the relations
\[
u^{i}\frac{\pd F}{\pd u^{i}}=2F,\qquad\left[u^{i}\frac{\pd F}{\pd u^{i}}\right]=0,
\]
whereas an exterior quadratic form $F$ satisfies the relations
\[
u^{i}\frac{\pd F}{\pd u^{i}}=0,\qquad\left[u^{i}\frac{\pd F}{\pd u^{i}}\right]=2F.
\]
\end{thm*}

\vspace{12pt}\fsec The exterior multiplication of two linear forms of the same variables $f(u), \varphi(u)$ gives rise to an exterior quadratic form associated to an alternating bilinear form
\[
f(u)\varphi(v)-\varphi(u)f(v)=
\begin{vmatrix}
  f(u)&\varphi(u)\\
  f(v)&\varphi(v)
\end{vmatrix}
;
\]
we denote by the notation $[f(u)\varphi(u)]$ the quadratic form $F$ which results from the term-by-term multiplication of two forms $f(u)$ and $\varphi(u)$ without inverting the order of the factors:
\begin{equation}
  \label{eq:4}
  F=[a_{i}u^{i}\cdot b_{j}u^{j}]=a_{i}b_{j}[u^{i}u^{j}].
\end{equation}

Observe that on the right side the coefficient $a_{i}b_{j}$ is not antisymmetric, even though the form is exterior. We can write, by exchanging two indices of summation,
\begin{equation}
  \label{eq:5}
  F=a_{i}b_{j}[u^{i}u^{j}]=a_{j}b_{i}[u^{j}u^{i}]=-a_{j}b_{i}[u^{i}u^{j}]\quad\text{where}\quad F=\frac{1}{2}(a_{i}b_{j}-a_{j}b_{i})[u^{i}u^{j}],
\end{equation}
and now the coefficients of $[u^{i}u^{j}]$ are antisymmetric.

We again have two corresponding theorems.
\begin{thm*}
  If $f^{1}, f^{2},\dots,f^{p}$ are $p$ independent linear forms of $n$ variables $u^{1}$, $u^{2}$,\dots, $u^{n}$, the relation
\[
f^{1}\varphi_{1}+f^{2}\varphi_{2}+\dots+f^{p}\varphi_{p}=0,
\]
where $\varphi_{1},\varphi_{2},\dots,\varphi_{p}$ are $p$ forms of the same variables, implies that  $\varphi_{i}$ are  linear combinations of the forms $f^{i}$ with antisymmetric coefficients
\[
\varphi_{i}=\alpha_{ih}f^{h}\qquad(\alpha_{ij}=-\alpha_{ji});
\]
on the other hand the relation
\[
[f^{1}\varphi_{1}]+[f^{2}\varphi_{2}]+\dots+[f^{p}\varphi_{p}]=0,
\]
implies that  $\varphi_{i}$ are  linear combinations of the forms $f^{i}$ with symmetric coefficients $\alpha_{ij}$.
\end{thm*}

Suppose first that $p=n$. In this case the forms $f^{i}$ are independent and all linear forms can be expressed as linear combinations of $f^{i}$. We can write
\[
\varphi_{i}=\alpha_{ik}f^{k},
\]
and we have
\begin{align*}
  f^{i}\varphi_{i}&=\alpha_{ik}f^{i}f^{k},\\
  [f^{i}\varphi_{i}]&=\alpha_{ik}[f^{i}f^{k}].
\end{align*}

The sum $f^{i}\varphi_{i}$ vanishes if $\alpha_{ij}=-\alpha_{ji}$ and the sum $[f^{i}\varphi_{i}]$ vanishes if $\alpha_{ij}=\alpha_{ji}$, which proves the theorem.

Suppose now $p<n$. Introduce $n-p$ new forms $f^{p+1},\dots,f^{n}$, independent among themselves and of the previous $p$ forms. We can apply the already proven part of the theorem for the case $p=n$ by taking the forms $\varphi_{p+1},\dots,\varphi_{n}$ to vanish identically. We then have, since the form $\varphi_{p+j}$ vanishes, $\alpha_{p+j,k}=0$ $(j=1,2,\dots,n-p;k=1,2,\dots,n)$ and hence $\alpha_{k,p+j}=0$ as well. The indices $p+1,p+2,\dots,n$ do not appear in the coefficients $\alpha_{ij}$ that have effects on the expressions of $\varphi_{i}$ in terms of $f^{j}$, and thus the theorem is established for the general case.

\begin{rmk*}
  Suppose both equations
\[
f^{i}\varphi_{i}=0\qquad[f^{i}\varphi_{i}]=0
\]
hold, then consequently
\[
\varphi_{i}=0.
\]
\end{rmk*}

\vspace{12pt}\fsec We know that all algebraic quadratic forms can be put into canonical forms in which all the off-diagonal terms vanish. There also exists canonical forms for exterior quadratic forms.

\begin{thm*}
  All exterior quadratic forms can be reduced to the form
  \begin{equation}
    \label{eq:6}
    F=[U^{1}U^{2}]+[U^{3}U^{4}]+\dots+[U^{2p-1}U^{2p}],
  \end{equation}
where $U^{i}$ are $2p$ independent linear forms.
\end{thm*}

The proof is very simple. Suppose the form $F(u)$ does not vanish identically and, for example, $a_{12}\neq 0$. We can write
\[
F=\left[\left(u^{1}+\frac{a_{23}}{a_{21}}u^{3}+\dots+\frac{a^{2n}}{a_{21}}u^{n}\right)(a_{12}u^{2}+a_{13}u^{3}+\dots+a_{1n}u^{n})\right]+\Phi,
\]
where the form $\Phi$ contains only the variables $u^{3},u^{4}, \dots, u^{n}$. It  then suffices to put
\[
u^{1}+\frac{a_{23}}{a_{21}}u^{3}+\dots+\frac{a^{2n}}{a_{21}}u^{n}=U^{1},\qquad a_{12}u^{2}+a_{13}u^{3}+\dots+a_{1n}u^{n}=U^{2}
\]
to obtain an exterior quadratic form $F-[U^{1}U^{2}]$ that depends only on the variables $u^{3},\dots,u^{n}$, the $n$ forms $U^{1},U^{2},u^{3},\dots,u^{n}$ being independent. If the form $\Phi$ vanishes identically then the theorem is proved, and the integer $p$ equals $1$. Otherwise we perform on $\Phi$ the same operation that we performed on $F$ and continue.

We can find the integer $p$ without going through the previous reduction. Indeed, note that for any linear change of  variables, the system of linear equations
\begin{equation}
  \label{eq:7}
  \frac{\pd F}{\pd u^{1}}=0,\quad \frac{\pd F}{\pd u^{2}}=0,\quad \dots\quad\frac{\pd F}{\pd u^{n}}=0
\end{equation}
is conserved, since if we set
\[
u^{i}=A_{k}{}^{i}U^{k},
\]
where the determinant of the coefficients $A_{k}{}^{i}$ is non-zero, we immediately  have
\[
\frac{\pd F}{\pd U^{i}}=A_{i}{}^{k}\frac{\pd F}{\pd u^{k}},
\]
and since the determinant of $A_{i}{}^{k}$ does not vanish, the vanishing of the partial derivatives $\pd F/\pd u^{i}$ implies to the vanishing of $\pd F/\pd U^{i}$ and \emph{vice versa}.

Now if we put $F$ in the canonical form
\[
F=[U^{1}U^{2}]+\dots+[U^{2p-1}U^{2p}],
\]
the system \eqref{eq:7} becomes, with the variables $U^{i}$,
\[
U^{1}=U^{2}=\dots=U^{2p-1}=U^{2p}=0.
\]
The integer $2p$ is hence the rank of the matrix of coefficients of the forms $\pd F/\pd u^{i}$. This matrix is skew-symmetric, and it is well known that the rank of such a matrix is always even.

The rank $2p$ also gives the minimum number of variables that must appear in the form $F$, if we perform a suitable linear transformation on the variables.

There are analogous theorems for algebraic quadratic forms where the rank of the matrix of coefficients of the forms $\pd F/\pd u^{i}$ also gives the minimum number of variables that must appear in the form $F$, and the simplest way to see it is to use the decomposition of the form into a sum of squares.

\section{Exterior forms of general degrees}
\label{sec:exter-forms-unsp}
\fsec We consider generic \emph{exterior forms}\index{exterior form} of general degrees. A cubic exterior form, for example, can be written as
\begin{equation}
  \label{eq:8}
  F=\frac{1}{6}a_{ijk}[u^{i}u^{j}u^{k}],
\end{equation}
where the coefficients $a_{ijk}$ are \emph{antisymmetric}: this means that if we perform a permutation of the three indices $i,j,k$, the coefficients remain unchanged or suffer a change of sign depending on whether the permutation is even or odd. The symbol $[u^{i}u^{j}u^{k}]$ can be regarded as a product, but one that changes sign when we permute two factors. The product remains equal to itself under an even permutation of the three factors, and becomes equal to its negative under an odd permutation. With these conventions we see that if we count the different monomials that appear, ordered by the three factors $u^{1},u^{2},u^{3}$, we obtain six different terms, whose sum being $a_{123}[u^{1}u^{2}u^{3}]$. Obviously monomials with two identical factors vanish.

We can associate to the form $F$ an alternating trilinear form of three series of variables $u^{i},v^{i},w^{i}$
\[
\frac{1}{6}a_{ijk}
\begin{vmatrix}
  u^{i}&u^{j}&u^{k}\\
  v^{i}&v^{j}&v^{k}\\
  w^{i}&w^{j}&w^{k}
\end{vmatrix}.
\]

More generally we can consider exterior forms of degrees $4,5$, {etc.}, which we may associate with alternating forms of $4,5$, {etc.}, series of variables.

\vspace{12pt}\fsec \emph{Addition and multiplication of exterior forms.} The sum of two exterior forms \emph{of the same degree} is the exterior form of the same degree whose coefficients are the sums of the coefficients of the two given forms. For example, the sum of the form \eqref{eq:8} and the form
\begin{equation}
  \label{eq:9}
  \Phi=\frac{1}{6}b_{ijk}[u^{i}u^{j}u^{k}]
\end{equation}
is
\[
F+\Phi=\frac{1}{6}(a_{ijk}+b_{ijk})[u^{i}u^{j}u^{k}].
\]

The \emph{exterior product}\index{exterior product} of two forms, of  equal degrees or not, for example the two forms
\begin{align*}
  F=&\frac{1}{2}a_{ij}[u^{i}u^{j}],\\
  \Phi=&=\frac{1}{6}b_{ijk}[u^{i}u^{j}u^{k}]
\end{align*}
is the form obtained by exterior multiplying in every possible way each monomial of the first  with each monomial of the second, taking care to respect the order of the two monomials, and adding the results obtained \footnote{The numerical coefficients of the forms can be put in any position, the law of commutativity being valid for them.}
\begin{equation}
  \label{eq:10}
  [F\cdot\Phi]=\frac{1}{12}a_{ij}b_{khl}[u^{i}u^{j}u^{k}u^{h}u^{l}].
\end{equation}

On the right hand side of equation \eqref{eq:10} the coefficients are not antisymmetric with respect to the $5$ indices $i,j,k,h,l$. However we can make them antisymmetric  as we did for the exterior product of two linear forms.

The exterior product of two forms $F,\Phi$ may depend on the order in which the factors $F$ and $\Phi$ appear. Indeed, changing the order of the factors boils down to replacing the monomial $[u^{i}u^{j}u^{k}u^{h}u^{l}]$ by the monomial $[u^{k}u^{h}u^{l}u^{i}u^{j}]$, and to effect this change we advance each of the three factors two places to the left, which requires $2\times 3$ successive changes of sign. In general we have, if $F$ and $\Phi$ are of degrees $p$ and $q$ respectively, the relation
\begin{equation}
  \label{eq:11}
  [\Phi\cdot F]=(-1)^{pq}[F\cdot\Phi],
\end{equation}
and hence the
\begin{thm*}
  The exterior product of two exterior forms of degrees $p$ and $q$ remains unchanged when we invert the order of the two factors, unless the two forms are both of odd degrees, in which case the product suffers a change of sign.
\end{thm*}

Another important theorem concerns the distributivity of multiplication with respect to addition. This theorem leads to the general equality
\[
  [(F_{1}+F_{2}+\dots+F_{h})(\Phi_{1}+\Phi_{2}+\dots+\Phi_{k})]=\sum_{ij}[F_{i}\Phi_{j}],
\]
where we suppose that the forms $F_{1},F_{2},\dots,F_{h}$ are of the same degree $p$ and the forms $\Phi_{1},\Phi_{2},\dots,\Phi_{k}$ are of the same degree $q$.

\vspace{12pt}\fsec \emph{Monomic exterior forms}. An exterior form of degree $p$ is \emph{monomic} if it can be written as an exterior product of $p$ linear forms. We will investigate what conditions the coefficients of a form must satisfy for it to be monomic.

Consider the system of linear equations (\emph{the associated system}) \index{associated system!of exterior forms} obtained by setting to zero all  partial derivatives of order $p-1$ of the form. These derivatives are defined in the case of general degree analogously to the case of a quadratic form, and depend on the order that we perform the derivations, but in fact in the present case the order does not matter since the only possible change in the result is a change of sign.

The associated system we consider have an \emph{intrinsic} significance, in the sense that if we perform a linear change of variables with non-vanishing determinant on the form $F(u^{1},u^{2},\dots,u^{p})$ to make it into $\Phi(U^{1},U^{2},\dots,U^{p})$, the same linear transformation will transform the associated system of $F$ into the associated system of $\Phi$, the reason being that the linear transformation
\[
u^{i}=A_{k}{}^{i}U^{k}
\]
entails
\[
\frac{\pd\Phi}{\pd U^{i}}=A_{k}{}^{i}\frac{\pd F}{\pd u^{k}},\qquad \frac{\pd^{2}\Phi}{\pd U^{i}\pd U^{j}}=A_{k}{}^{i}A_{h}{}^{j}\frac{\pd^{2}F}{\pd u^{k}u^{h}},\qquad\dots
\]

Now suppose we have a monomic exterior form. We can perform a linear transformation on the variables to make it contain only the single term $a[u^{1}u^{2}u^{3}\dots u^{p}]$ (we exclude here the case where the form vanishes identically). The associated system of this form is evidently
\[
u^{1}=0,\quad u^{2}=0,\quad\dots,\quad u^{p}=0,
\]
which are $p$ independent equations.

Conversely, if the associated system of a form $F$ of degree $p$ can be reduced to $p$ independent equations, we can, by a linear transformation on the variables, make the associated system into the form
\[
u^{1}=0,\quad u^{2}=0,\quad\dots,\quad u^{p}=0,
\]
but then no non-vanishing coefficients of the form can contain an index other than $1,2,\dots,p$, since a non-zero coefficient such as $a_{i_{1}i_{2}\dots i_{p-1} i_{p+1}}$, will contain the variable $u^{p+1}$ in the derivative $\pd^{p-1}F/\pd u^{i_{1}}\pd u^{i_{2}}\dots\pd u^{i_{p-1}}$, contrary to our assumption. The form is therefore $a[u^{1}u^{2}\dots u^{p}]$, which is monomic \footnote{The same reasoning shows that if the associated system is of rank $r$, it is possible, by a change of variables, to find an expression of the form that involves only $r$ variables, and it is obviously impossible to find an expression involving fewer variables, as otherwise the associated system would be of rank less than $r$.}.
\begin{thm*}
  An exterior form of degree $p$ is a monomial if and only if the associated system is of rank $p$.\footnote{The theorem has in effect already been proven for the case $p=2$ by the introduction of the canonical form.}
\end{thm*}

\vspace{12pt}\fsec We now apply this criterion to investigate the necessary and sufficient conditions for the coefficients of an exterior form to be a monomial. We begin with the simple case of quadratic forms.

Suppose we have a quadratic form
\[
F=\frac{1}{2}a_{ij}[u^{i}u^{j}],
\]
which we assume is non-zero. Without loss of generality, suppose $a_{12}\neq 0$. Among the equations of the associated system there are two equations
\begin{align*}
  \frac{\pd F}{\pd u^{1}}&\equiv a_{12}u^{2}+a_{13}u^{3}+\dots+a_{1n}u^{n}=0,\\
  \frac{\pd F}{\pd u^{2}}&\equiv a_{21}u^{1}+a_{23}u^{3}+\dots+a_{2n}u^{n}=0.
\end{align*}

These two equations are independent and give
\begin{align*}
  u^{1}&=\frac{1}{a_{12}}(a_{23}u^{3}+\dots+a_{2n}u^{n}),\\
  u^{2}&=-\frac{1}{a_{12}}(a_{13}u^{3}+\dots+a_{1n}u^{n}).
\end{align*}

Let us find the coefficient of $u^{k}$ in $\pd F/\pd u^{i}$, taking into account the values of $u^{1},u^{2}$ that we just obtained. We have
\[
\frac{\pd F}{\pd u^{i}}=\left(a_{ik}+\frac{a_{i1}}{a_{12}}a_{2k}-\frac{a_{i2}}{a_{12}}a_{1k}\right)u^{k}=\frac{1}{a_{12}}(a_{12}a_{ik}-a_{1i}a_{2k}+a_{1k}a_{2i})u^{k}.
\]

The right hand side must vanish if we want the associated system to be of rank $2$, and hence the \emph{necessary and sufficient} conditions, for the case $a_{12}\neq 0$, is
\begin{equation}
  \label{eq:12}
  a_{12}a_{ik}-a_{1i}a_{2k}+a_{1k}a_{2i}=0.
\end{equation}

This relation has been proved supposing $a_{12}\neq 0$. However it remains true even when $a_{12}=0$. Indeed, the indices $1,2,i,k$ here all play the same role, since any permutation of the four indices leaves the relation unaltered. The relation is true whenever any of the coefficients $a_{12},a_{2i},a_{2k},a_{1i},a_{1k},a_{ik}$ is non-zero, and it is \emph{a fortiori} true when all of them are zero. We can therefore replace the indices $1,2$ with unspecified indices and the relation \eqref{eq:12} will continue to hold.

\begin{thm*}
  If a quadratic exterior form is monomic, then  among the coefficients there are the relations
  \begin{equation}
    \label{eq:13}
    a_{ij}a_{kh}-a_{ik}a_{jh}+a_{ih}a_{jk}=0\qquad(i,j,k,h=1,2,\dots,n).
  \end{equation}
\end{thm*}

\emph{Conversely}, if these relations hold, the form is monomial, since if we suppose $a_{12}\neq 0$ the relations \eqref{eq:12}, which appear in the relations \eqref{eq:13}, will hold, and we know that under this condition the form is monomic.

\vspace{12pt}\fsec \label{fsec:10} The same method will apply to a form of general degree. To make things concrete, take now $p=5$ and the coefficient $a_{12345}$  non-vanishing. The associated system of the form contains the $5$ independent equations
\begin{align*}
  \frac{\pd^{4}F}{\pd u^{2}\pd u^{3}\pd u^{4}\pd u^{5}}&\equiv a_{12345}u^{1}+a_{2345k}u^{k}=0,\\
  \frac{\pd^{4}F}{\pd u^{1}\pd u^{3}\pd u^{4}\pd u^{5}}&\equiv -a_{12345}u^{2}+a_{1345k}u^{k}=0,\\
  \frac{\pd^{4}F}{\pd u^{1}\pd u^{2}\pd u^{4}\pd u^{5}}&\equiv a_{12345}u^{3}+a_{1245k}u^{k}=0,\\
  \frac{\pd^{4}F}{\pd u^{1}\pd u^{2}\pd u^{3}\pd u^{5}}&\equiv -a_{12345}u^{4}+a_{1235k}u^{k}=0,\\
  \frac{\pd^{4}F}{\pd u^{1}\pd u^{2}\pd u^{3}\pd u^{4}}&\equiv a_{12345}u^{5}+a_{1234k}u^{k}=0.
\end{align*}

For the form to be monomic it is necessary and sufficient that all the equations of the associated system follow from these five equations, which is to say if we replace $u^{1},u^{2},u^{3},u^{4},u^{5}$ by their values determined by these five equations, the other equations are identically satisfied. Consider for example the equation
\[
\frac{\pd ^{4} F}{\pd u^{i_{1}}\pd u^{i_{2}}\pd u^{i_{3}}\pd u^{i_{4}}}=0;
\]
the coefficients of $u^{k}$ on the left hand side, when $u^{1},u^{2},u^{3},u^{4},u^{5}$ have been replaced by their values and  multiplied by $a_{12345}$, will be equal to
\begin{align*}
a_{12345}a_{i_{1}i_{2}i_{3}i_{4}k}-a_{i_{1}i_{2}i_{3}i_{4}1}a_{2345k}&+a_{i_{1}i_{2}i_{3}i_{4}2}a_{1345k}-a_{i_{1}i_{2}i_{3}i_{4}3}a_{1245k}\\
&+a_{i_{1}i_{2}i_{3}i_{4}4}a_{1235k}-a_{i_{1}i_{2}i_{3}i_{4}5}a_{1234k}.
\end{align*}

We see that this expression contain $10$ indices, which can be divided into two groups, the indices $i_{1},i_{2},i_{3},i_{4}$ being the first group and the indices $1,2,3,4,5,k$ the second group, and within each group all the indices play the same role. The expression is antisymmetric with respect to the first group and also  with respect to the second group. The expression obtained vanishes when the form is monomial, supposing that $a_{12345}\neq 0$. It also vanishes when $a_{12345}=0$ but the coefficients $a_{1234k}, a_{1235k},a_{1245k},a_{1345k},a_{2345k}$ are not all zero, because if one of them is non-zero, for example the first, given that the indices $1,2,3,4,5,k$ play the same role we will find the same expression starting from the hypothesis $a_{1234k}\neq0$. On the other hand, obviously  if the $5$ coefficients $a_{12345},a_{1234k},a_{1235k},a_{1245k},a_{1345k},a_{2345k}$ are all zero, the expression still vanishes. We hence have the following theorem:
\begin{thm*}
  A form of degree $5$ is monomic if and only if the $C^{4}_{n}C^{6}_{n}$ quantities
  \begin{align}
    \label{eq:14}
    H_{i_{1}i_{2}i_{3}i_{4},j_{1}j_{2}j_{3}j_{4}j_{5}j_{6}}&\equiv
a_{i_{1}i_{2}i_{3}i_{4}j_{1}}a_{j_{2}j_{3}j_{4}j_{5}j_{6}}
-a_{i_{1}i_{2}i_{3}i_{4}j_{2}}a_{j_{1}j_{3}j_{4}j_{5}j_{6}}\\
&+a_{i_{1}i_{2}i_{3}i_{4}j_{3}}a_{j_{1}j_{2}j_{4}j_{5}j_{6}}
-a_{i_{1}i_{2}i_{3}i_{4}j_{4}}a_{j_{1}j_{2}j_{3}j_{5}j_{6}}\notag\\
&+a_{i_{1}i_{2}i_{3}i_{4}j_{5}}a_{j_{1}j_{2}j_{3}j_{4}j_{6}}
-a_{i_{1}i_{2}i_{3}i_{4}j_{6}}a_{j_{1}j_{2}j_{3}j_{4}j_{5}}\notag
  \end{align}
are all zero.
\end{thm*}

We have already shown that the condition is necessary. It is also sufficient, since if it holds and the coefficient $a_{12345}$ does not vanish, among the equations $\eqref{eq:14}$ we can pick out 
\[
H_{i_{1}i_{2}i_{3}i_{4},12345k}=0
\]
which, as we have already seen, is necessary and sufficient under the assumption $a_{12345}\neq 0$ for the form to be monomic.

Observe that these conditions all reduce to quadratic relations between the coefficients of the given form.

If the form is cubic, we will find the relations
\[
H_{ij,khlm}\equiv a_{ijk}a_{hlm}-a_{ijh}a_{klm}+a_{ijl}a_{khm}-a_{ijm}a_{khl}=0
\]
which by naive counting are $(n(n-1)/2)(n(n-1)(n-2)(n-3)/24)$ in number, but in fact contain fewer equations since if $k=i,h=j$, the expression $H_{ij,ijhm}$ vanishes identically. If $k=i$, the expression has only three terms instead of four:
\[
H_{ij,ihlm}=-a_{ijh}a_{ilm}+a_{ijl}a_{ihm}-a_{ijm}a_{ihl}.
\]

\section{Systems of exterior equations}
\label{sec:syst-exter-equat}

\fsec Consider a system of equations obtained by setting to zero one or more exterior forms of $n$ variables $u^{1},u^{2},\dots,u^{n}$. Each of these equations has a certain degree, but these degrees need not be the same. We can think of $u^{1},u^{2},\dots,u^{n}$ as the Cartesian coordinates of a point in a $n$ dimensional space. We say that a flat manifold passing through the origin of the space (\emph{we do not consider other cases}) satisfies the given system, or constitute a \emph{solution} of the system, if the equations in the system are implied by the defining equations of the manifold, which are linear and homogeneous with respect to the variables. For example, if the flat manifold is of dimension $p$ (a $p$-plane), it is defined by $n-p$ independent linear relations of the coordinates. On this flat manifold we can choose $n-p$ coordinates, for example the last $n-p$ ones, to depend on the other $p$ coordinates $u^{1},u^{2},\dots,u^{p}$, and then we can replace $u^{p+1},\dots,u^{n}$ in the system of equations by their expressions in terms of $u^{1},u^{2},\dots,u^{p}$. We thus obtain a set of exterior forms in $u^{1},u^{2},\dots,u^{p}$ which must vanish identically. More generally we may express $u^{1},u^{2},\dots,u^{n}$ in terms of linear forms of $p$ variables $t^{1},t^{2},\dots,t^{p}$ and we then obtain a set of exterior forms in $t^{1},t^{2},\dots,t^{p}$ that must vanish identically.

We make the following observation:
\begin{thm*}
  All exterior equations of degree $p$ are automatically satisfied by flat manifolds of less than $p$ dimensions.
\end{thm*}

This follows from the fact that an exterior form of degree $p$ in $q<p$ variables vanishes identically.

\emph{To show that a $p$-plane is a solution of a system of exterior equations, it is therefore unnecessary to consider the equations in the system that are of degrees higher  than $p$.}

\vspace{12pt}\fsec To determine if a given $p$-plane is a solution of a given system of exterior equations, we can make progress by considering the so-called \emph{Pl\"ucker coordinates}\index{Plucker coordinate@Pl\"ucker coordinates} or  \emph{Grassmannians}\index{Grassmannian|see{Pl\"ucker coordinates}} of a $p$-plane (passing through the origin). Such a $p$-plane is completely determined if we specify $p$ independent vectors from the origin, the components of which we denote by $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$ respectively. From these we can form the matrix of $p$ rows and $n$ columns
\[
\begin{pmatrix}
  \xi_{1}{}^{1}&\xi_{1}{}^{2}&\dots&\xi_{1}{}^{n}\\
  \xi_{2}{}^{1}&\xi_{2}{}^{2}&\dots&\xi_{2}{}^{n}\\
  \hdotsfor[1.5]{4}\\
  \xi_{p}{}^{1}&\xi_{p}{}^{2}&\dots&\xi_{p}{}^{n}\\
\end{pmatrix},
\]
and we denote by $u^{i_{1}i_{2}\dots i_{p}}$ the determinant formed by the $p$ rows and the columns of the range $i_{1},i_{2},\dots,i_{p}$ of the matrix. The quantities $u^{i_{1}i_{2}\dots i_{p}}$ are antisymmetric with respect to the $p$ indices. If we replace these $p$ vectors by $p$ other independent vectors taken from the same $p$-plane of components $\overline\xi_{1}{}^{i},\overline\xi_{2}{}^{i},\dots,\overline\xi_{p}{}^{i}$, the components $\overline\xi_{1}{}^{i},\overline\xi_{2}{}^{i},\dots,\overline\xi_{p}{}^{i}$ can be calculated from the components $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$ (for a given $i$) by a linear transformation, this being true for all $i$. Then we see that the new determinant $\overline u^{i_{1}i_{2}\dots i_{p}}$ is obtained by multiplying the old determinant of the corresponding indices by the same factor, \emph{i.e.}, the determinant of the transformation that changes $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$ into $\overline\xi_{1}{}^{i},\overline\xi_{2}{}^{i},\dots,\overline\xi_{p}{}^{i}$. The coordinates $u^{i_{1}i_{2}\dots i_{p}}$ of a $p$-plane are therefore determined up to a non-zero factor: these are  the \emph{Pl\"ucker coordinates} of the $p$-plane; they are homogeneous and they are overabundant if $p>1$.

Conversely, knowledge about the Pl\"ucker coordinates of a $p$-plane completely determines the $p$-plane, because the equations of a $p$-plane can be obtained by expressing the vector from the origin, of components $u^{i}$,  as a linear combination of the vectors $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$, {i.e.}, the determinants of the minors of degree $p+1$ of the matrix
\[
\begin{pmatrix}
  u^{1}&u^{2}&\dots&u^{n}\\
  \xi_{1}{}^{1}&\xi_{1}{}^{2}&\dots&\xi_{1}{}^{n}\\
  \xi_{2}{}^{1}&\xi_{2}{}^{2}&\dots&\xi_{2}{}^{n}\\
  \hdotsfor[1.5]{4}\\
  \xi_{p}{}^{1}&\xi_{p}{}^{2}&\dots&\xi_{p}{}^{n}\\
\end{pmatrix}
\]
all vanish. Now each of these determinants is linear in $u^{1},u^{2},\dots,u^{n}$ and the coefficients  are just the determinants $u^{i_{1}i_{2}\dots i_{p}}$.

\vspace{12pt}\fsec This leads us to investigate, first, if a $p$-plane with Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ annihilates a given exterior form of degree $p$
\[
F=\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}].
\]

Introduce in the $p$-plane the coordinates $v^{1},v^{2},\dots,v^{p}$, which are obtained by solving the equations
\[
u^{i}=v^{k}\xi_{k}{}^{i}\qquad (i=1,2,\dots,n),
\]
the summation index $k$ taking the values $1,2,\dots,p$. We have
\[
[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}]=\xi^{i_{1}}_{k_{1}}\xi^{i_{2}}_{k_{2}}\dots\xi^{i_{p}}_{k_{p}}[v^{k_{1}}v^{k_{2}}\dots v^{k_{p}}];
\]
on the right hand side we only have to consider the non-zero monomials, i.e., the terms where all the indices $k_{1},k_{2},\dots,k_{p}$ are distinct. The monomial $[v^{k_{1}}v^{k_{2}}\dots v^{k_{p}}]$ is none other than the monomial $[v^{1}v^{2}\dots v^{p}]$ up to the sign $+$ or $-$ according to whether the permutation of the $p$ indices $k_{1},k_{2},\dots,k_{p}$ is even or odd. The coefficients of $[v^{1}v^{2}\dots v^{p}]$ are then easily seen to be the determinant
\[
\begin{vmatrix}
  \xi_{1}^{i_{1}}&\xi_{1}^{i_{2}}&\dots&\xi_{1}^{i_{p}}\\
  \xi_{2}^{i_{1}}&\xi_{2}^{i_{2}}&\dots&\xi_{2}^{i_{p}}\\
  \hdotsfor[1.5]{4}\\
  \xi_{p}^{i_{1}}&\xi_{p}^{i_{2}}&\dots&\xi_{p}^{i_{p}}\\
\end{vmatrix}
=u^{i_{1}i_{2}\dots i_{p}}.
\]

It therefore follows that
\[
F=\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}u^{i_{1}i_{2}\dots i_{p}}[v^{1}v^{2}\dots v^{p}].
\]

The condition for a given $p$-plane to be a solution of the exterior equation $F=0$ that we are seeking is hence that $F$ vanishes when we replace the exterior product $[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}]$ by the Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ of the $p$-plane.
\begin{thm*}
  A $p$-plane of Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ is a solution of the equation
\[
F\equiv\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}]=0,
\]
if and only if \footnote{Observe that it suffices to substitute the monomial $[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}]$ by the associated linear alternating $p$-form of the $p$ variables  $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$.}
\[
a_{i_{1}i_{2}\dots i_{p}}u^{i_{1}i_{2}\dots i_{p}}=0.
\]
\end{thm*}

\vspace{12pt}\fsec We will now find the conditions for a $p$-plane given in terms of its Pl\"ucker coordinates to annihilate an exterior form $\Phi$ of degree $q<p$.

We begin with some geometrical remarks. If a $p$-plane annihilates the form $\Phi$, then all $q$-planes contained in the $p$-plane ($q<p$) also annihilate the form $\Phi$, since the form $\Phi$ already vanishes on the equations determining the $p$-plane, and will \emph{a fortiori} vanish when we \emph{in addition} take into account more equations which, together with the first set of equations, define the $q$-plane. Conversely suppose that the form $\Phi$ is annihilated by all the $q$-planes contained in a given $p$-plane. If we take into account the equations of the $p$-plane, in which for example the coordinates $u^{p+1},\dots,u^{n}$ have already been solved, the form $\Phi$ becomes a form $\Psi$ of $p$ variables $u^{1},u^{2},\dots,u^{p}$. To say that the form vanishes in all the $q$-planes contained in the given $p$-plane is the same as saying that it vanishes when we reduce the $p$ variables $u^{1},u^{2},\dots,u^{p}$ with any $p-q$ independent linear relations. Now if we take the linear relations
\[
u^{q+1}=u^{q+2}=\dots=u^{p}=0,
\]
there will remain only a single term in $\Psi$, which is $[u^{1}u^{2}\dots u^{q}]$. The coefficient of the monomial $[u^{1}u^{2}\dots u^{p}]$ in $\Psi$ is therefore zero, and the same holds for all the other coefficients.
\begin{thm*}
  A $p$-plane  annihilates an exterior form $\Phi$ of degree $q<p$ if and only if all the $q$-planes contained in the given $p$-plane annihilate the form.
\end{thm*}

Analytically we can proceed in the following manner. If the form $\Phi$ vanishes on the given $p$-plane, all the forms of degree $p$
\[
[\Phi u^{\alpha_{1}}u^{\alpha_{2}}\dots u^{\alpha_{p-q}}]\qquad(\alpha_{1},\alpha_{2},\dots,\alpha_{p-q}=1,2,\dots,n)
\]
vanish \emph{a fortiori} on the $p$-plane. This condition is necessary and sufficient. Indeed, suppose that by using the equations of the $p$-plane in which for example the variables $u^{p+1},\dots,u^{n}$ have been solved, then the form $\Phi$ becomes a certain form $\Psi$ in $u^{1},u^{2},\dots,u^{p}$. If each of the forms $[\Psi u^{\alpha_{1}}u^{\alpha_{2}}\dots u^{\alpha_{p-q}}]$ of degree $p$ obtained by taking $u^{\alpha_{1}},u^{\alpha_{2}},\dots,u^{\alpha_{p-q}}$ to be $p-q$ functions of the variables $u^{1},u^{2},\dots,u^{p}$ is zero, then each of the coefficients of the form $\Psi$ is zero and the $p$-plane annihilate the form $\Phi$.
\begin{thm*}
  A given $p$-plane  annihilates an exterior form $\Phi$ of degree $q<p$
\[
\Phi=\frac{1}{q!}b_{i_{1}i_{2}\dots i_{q}}[u^{i_{1}}u^{i_{2}}\dots u^{i_{q}}],
\]
if and only if the Pl\"ucker components of the $p$-plane satisfy the $C_{n}^{p-q}$ linear equations
\[
b_{i_{1}i_{2}\dots i_{q}}u^{i_{1}i_{2}\dots i_{q}\alpha_{1}\alpha_{2}\dots \alpha_{p-q}}=0\qquad (\alpha_{1},\alpha_{2},\dots,\alpha_{p-q}=1,2,\dots,n).
\]
\end{thm*}

For example, if a tri-plane is a solution of the equations
\begin{align*}
  a_{i}u^{i}&=0,\\
  a_{ij}[u^{i}u^{j}]&=0,\\
  a_{ijk}[u^{i}u^{j}u^{k}]&=0,\\
\end{align*}
then its Pl\"ucker components satisfy $({n^{2}+n+2})/{2}$ linear equations
\begin{align*}
  a_{i}u^{i\alpha\beta}&=0&&(\alpha,\beta=1,2,\dots,n),\\
  a_{ij}u^{ij\alpha}&=0&&(\alpha=1,2,\dots,n),\\
  a_{ijk}u^{ijk}&=0.&
\end{align*}

\vspace{12pt}\fsec \emph{Conditions for the antisymmetric quantities $u^{i_{1}i_{2}\dots i_{p}}$ to be the Pl\"ucker coordinates of a $p$-plane.} Suppose a system of quantities $u^{i_{1}i_{2}\dots i_{p}}$ antisymmetric with respect to the $p$ indices $i_{1},i_{2},\dots,i_{p}$ taking values of the integers $1,2,\dots,n$ are \emph{a priori} given. In general these numbers will not be the Pl\"ucker coordinates of any $p$-plane.
\begin{thm*}
   A system of quantities $u^{i_{1}i_{2}\dots i_{p}}$ antisymmetric with respect to the $p$ indices is the Pl\"ucker coordinates of a $p$-plane if and only if the exterior form of degree $p$
\[
F=\frac{1}{p!}u^{i_{1}i_{2}\dots i_{p}}[z_{i_{1}}z_{i_{2}}\dots z_{i_{p}}]
\]
of $n$ variables $z_{1},z_{2},\dots, z_{n}$ is a monomial.
\end{thm*}
Indeed, if $u^{i_{1}i_{2}\dots i_{p}}$ are the Pl\"ucker coordinates of a $p$-plane determined by the $p$ independent vectors $\xi_{i}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$, we have
\[
F=[f_{1}f_{2}\dots f_{p}],
\]
by putting
\[
f_{1}=\xi_{1}{}^{i}z_{i},\quad,f_{2}=\xi_{2}{}^{i}z_{i},\quad\dots,\quad f_{p}=\xi_{p}{}^{i}z_{i}.
\]

Conversely if $F$ can be put into the form just stated, with $f_{1},f_{2},\dots,f_{p}$ being $p$ arbitrary linear forms, then the coefficients $u^{i_{1}i_{2}\dots i_{p}}$ are the Pl\"ucker coordinates of the $p$-plane determined by the vectors $\xi_{1}{}^{i},\dots,\xi_{p}{}^{i}$. \footnote{The linear relations $u^{i_{1}i_{2}\dots i_{p-1}k}z_{k}=0$ give the necessary and sufficient conditions to be satisfied by the quantities $z_{k}$ for the hyperplane $z_{1}u^{1}+z_{2}u^{2}+\dots+z_{n}u^{n}=0$ to contain the $p$-plane under consideration.}\qed

We can now apply the conditions we have found in \textsection
\textbf{10}. In particular, \emph{a system of quantities $u^{ijk}$ antisymmetric with respect to the three indices $i,j,k$ is the coordinates of a tri-plane if and only if}
\begin{gather*}
H^{ij,khlm}=u^{ijk}u^{hlm}-u^{ijh}u^{klm}+u^{ijl}u^{khm}-u^{ijm}u^{khl}=0\\
(i,j,k,h,l,m=1,2,\dots,n).
\end{gather*}

In previous discussions we have obtained  \emph{linear} equations to be satisfied by the Pl\"ucker coordinates of a $p$-plane for such a $p$-plane to constitute a solution of a given system of exterior equations. For completeness, we must add to these linear equations the \emph{quadratic} equations that must hold for the coordinates to be a $p$-plane.

\section{Algebraically equivalent systems of exterior equations}
\label{sec:algebr-equiv-syst}

\fsec \emph{Ring of exterior forms.} The \emph{ring} determined by $h$ homogeneous exterior forms $F_{1},F_{2},\dots,F_{h}$ of $n$ variables is the set of exterior forms
\[
\Phi=[F_{1}\varphi^{1}]+[F_{2}\varphi^{2}]+\dots+[F_{h}\varphi^{h}],
\]
where the $\varphi^{i}$ are homogeneous exterior forms subject to the  condition that  the terms $[F_{i}\varphi^{i}]$ on the right hand side have the sum of degrees of the two factors of each term, positive or zero, the same for all  terms.

It is obvious that all forms $\Phi$ in the ring determined by $h$ given forms $F_{1}$, $F_{2}$, \dots , $F_{h}$ vanish for all solutions of the system
\[
F_{1}=0,\qquad F_{2}=0,\qquad\dots\qquad F_{h}=0.
\]

However, the converse is not always true. We will show that it is true for a fairly general case, and give an example where it is false.

\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape1}
\newtheorem*{thm17}{\hspace{15pt}\textbf{17.} Theorem}
\begin{thm17}
  An exterior form $\Phi$  vanishes on all solutions of a system of independent linear equations
  \begin{equation}
    \label{eq:15}
    F_{1}=0,\qquad F_{2}=0,\qquad\dots\qquad F_{h}=0,
  \end{equation}
if and only if $\Phi$ belongs to the ring determined by the forms $F_{i}$. Moreover,  $\Phi$  belongs to the ring if and only if the form $[F_{1}F_{2}\dots F_{h}\Phi]$ vanishes.
\end{thm17}

We have already seen that if $\Phi$ belongs to the ring under consideration, the form $\Phi$ vanishes on all the solutions of the linear equations \eqref{eq:15}. This condition is necessary: by a change of variables the forms $F_{i}$ can be reduced to the variables
\[
u^{1},u^{2},\dots,u^{h},
\]
and to say that the form $\Phi$ vanishes when we set the variables $u^{1},u^{2},\dots,u^{h}$ to zero is the same as saying that all monomials of $\Phi$ contain at least one of the variables $u^{1},u^{2},\dots,u^{h}$. Denote by $\varphi^{1}$ the coefficient of $u^{1}$ in the terms of $\Phi$ which contain the variable $u^{1}$ after we pass $u^{1}$ to the first factor of each of these terms, and $\varphi^{2}$ the form obtained in the analogous manner with respect to the variable $u^{2}$ for the form $\Phi-[u^{1}\varphi^{1}]$, {etc.},  we obviously have
\begin{equation}
  \label{eq:16}  
  \Phi=[u^{1}\varphi^{1}]+[u^{2}\varphi^{2}]+\dots+[u^{h}\varphi^{h}].
\end{equation}
The first part of the theorem is thus proved.

Clearly the form
\[
[F_{1}F_{2}\dots F_{h}\Phi]
\]
vanishes identically, since on replacing $\Phi$ by our expresson \eqref{eq:16}, each term in the product contains two identical factors \emph{of degree $1$} and hence vanishes.

Conversely if the form $[F_{1}F_{2}\dots F_{h}\Phi]$ vanishes and we again by a suitable change of variables put the forms into $F_{i}=u^{i}$ $(i=1,2,\dots,h)$, then for each monomial in the expression of $\Phi$ its factors cannot be all distinct from $u^{1},u^{2},\dots,u^{h}$, and consequently $\Phi$ vanishes when $u^{1},u^{2},\dots,u^{h}$ are zero.\qed

\vspace{12pt}\fsec Now consider the following example. Suppose we have the \emph{non-linear} forms
\[
F_{1}=[u^{1}u^{3}],\qquad F_{2}=[u^{1}u^{4}],\qquad F_{3}=[u^{1}u^{2}]-[u^{3}u^{4}].
\]
For all the bi-planes of coordinates $u^{ij}$ annihilating these three forms, if we take into consideration the quadratic relation
\[
u^{13}u^{24}-u^{14}u^{23}-u^{12}u^{34}=0,
\]
then the coordinate $u^{12}$ must be zero, \emph{i.e.}, the following equation holds
\[
\Phi\equiv[u^{1}u^{2}]=0.
\]

The same holds for all $p$-planes annihilating the same $F_{1},F_{2},F_{3}$, as all bi-planes contained in the $p$-plane have to annihilate the form $\Phi$, and hence the form $\Phi$ vanishes on this $p$-plane.

\emph{However, the form $\Phi$, which vanishes on all solutions of the equations $F_{1}=F_{2}=F_{3}=0$, does not belong to the ring determined by the forms $F_{1},F_{2},F_{3}$.}

\begin{dfn*}
  A system of exterior equations is  \emph{complete} \index{complete exterior system} if all the forms that vanish on the solution of the system belong to the ring of the forms defining the equations of the system.
\end{dfn*}

\vspace{6pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape1}
\newtheorem*{def19}{\hspace{15pt}\textbf{19.} Definition}
\begin{def19}
  Two systems of exterior equations are  algebraically equivalent if the forms determining the equations of either of the systems belong to the ring of the forms determining the equations of the other system.
\end{def19}

It is clear that two algebraically equivalent systems admit the same solutions. If a system is complete, all other systems admitting the same solutions are algebraically equivalent to it, but the converse may not be true.

It is easy to construct a more general system algebraically equivalent to a given system. Consider for example the system of degree three
\begin{equation}
  \label{eq:17}
  \left\{
    \begin{aligned}
      F_{1}&\equiv A_{1i}u^{i}=0,\\
      F_{2}&\equiv A_{2i}u^{i}=0,\\
      \Phi&\equiv \frac{1}{2}B_{ij}[u^{i}u^{j}]=0,\\
      \Psi&\equiv \frac{1}{6}C_{ijk}[u^{i}u^{j}u^{k}]=0.
    \end{aligned}
  \right.
\end{equation}

It suffices to construct the equations
\begin{equation}
  \label{eq:17'}
  \tag{17$^{\prime}$}
  \left\{
    \begin{aligned}
      \overline{F}_{1}&\equiv aF_{1}+b F_{2}=0,\\
      \overline{F}_{2}&\equiv a'F_{1}+b'F_{2}=0,\\
      \overline\Phi&\equiv c\Phi+[\varpi^{1}F_{1}]+[\varpi^{2}F_{2}]=0,\\
      \overline\Psi&\equiv h\Psi+[\varpi^{3}\Phi]+[\psi^{1}F_{1}]+[\psi^{2}F_{2}]=0,\\
    \end{aligned}
  \right.
\end{equation}
where $a,b,a',b',c,h$ are constants ($ab'-ba'\neq 0,c\neq 0,h\neq 0$) and $\varpi^{1},\varpi^{2},\varpi^{3}$ are arbitrary linear forms, $\psi^{1},\psi^{2}$ arbitrary quadratic exterior forms.
\section[Associated system of a system of exterior equations]{Associated system of a system\\of exterior equations}
\label{sec:assoc-syst-syst}

\fsec We have already considered (\textsection\textbf{8})  the so-called associated system of an exterior form. We now define the associated system of a system of exterior equations. \index{associated system!of exterior system}

\begin{dfn*}
  A line $\Delta$ through the origin is  distinguished with respect to a system of exterior equations $\Sigma$ if it is a solution of the system and moreover, given any flat manifold $V$ which constitute a solution of the system $\Sigma$, the smallest flat manifold containing $V$ and $\Delta$ is also a solution of $\Sigma$.
\end{dfn*}

We now find the necessary and sufficient conditions for a line of parameters $\xi^{i}$ to be distinguished for a given system of exterior equations.

Consider a system $\Sigma$, which for simplicity we suppose to be of degree three
\begin{equation}
  \label{eq:18}
  \left\{
    \begin{aligned}
      F_{\alpha}&\equiv A_{\alpha i}u^{i}=0&(\alpha&=1,2,\dots,r_{1}),\\
      \Phi_{\alpha}&\equiv A_{\alpha ij}[u^{i}u^{j}]=0&(\alpha&=1,2,\dots,r_{2}),\\
      \Psi_{\alpha}&\equiv A_{\alpha ijk}[u^{i}u^{j}u^{k}]=0&(\alpha&=1,2,\dots,r_{3}).
    \end{aligned}
  \right.
\end{equation}

For a line ($\xi^{i}$) to be distinguished, it must satisfy three kinds of conditions  (\textsection\textbf{13} and \textsection\textbf{14}) :

1. It must be itself a solution of $\Sigma$, that is to say it annihilates the forms $F_{\alpha}$:
\begin{equation}
\label{eq:19}
A_{\alpha i}\xi^{i}=0\qquad (\alpha=1,2,\dots,r_{1});
\end{equation}

2. Given any line $(u^{i})$ which is a solution of $\Sigma$, the bi-plane formed by the line $(u^{i})$ and the line $(\xi^{i})$ must annihilate the forms $\Phi_{\alpha}$, which is to say we have
\begin{equation}
\label{eq:20}
A_{\alpha ij}u^{i}\xi^{j}=0\qquad(\alpha=1,2,\dots,r_{2}),
\end{equation}
whenever the components $u^{i}$ satisfy the $r_{1}$ relations
\begin{equation}
\label{eq:21}
A_{\beta i}u^{i}=0\qquad (\beta=1,2,\dots,r_{1});
\end{equation} 

3. Given a bi-plane of coordinates $u^{ij}$ which is a solution of $\Sigma$, the tri-plane determined by the bi-plane and the line $(\xi^{i})$ must annihilate the forms $\Psi_{\alpha}$; the coordinates $u^{ijk}$ of the tri-plane are
\[
u^{ijk}=u^{ij}\xi^{k}-u^{ik}\xi^{j}+u^{jk}\xi^{i};
\]
therefore we must have
\begin{equation}
\label{eq:22}
  A_{\alpha ijk}u^{ij}\xi^{k}=0\qquad (\alpha=1,2,\dots,r_{3}),
\end{equation}
whenever the coordinates $u^{ij}$ of the given bi-plane satisfy the $nr_{1}+r_{2}$ relations
\begin{equation}
\label{eq:23}
  \left\{
    \begin{aligned}
      A_{\beta i}u^{i1}&=A_{\beta i}u^{i2}=\dots =A_{\beta i}u^{i n}=0,&(\beta&=1,2,\dots,r_{1}),\\
      A_{\gamma ij}u^{ij}&=0,&(\gamma&=1,2,\dots,r_{2}).
    \end{aligned}
  \right.
\end{equation}

These conditions are necessary and sufficient. Indeed, let $V$ be a $p$-plane which is a solution of the system $\Sigma$ and $W$ the $(p+1)$-plane determined by $V$ and the line $\xi^{i}$. For the $(p+1)$-plane to satisfy the system $\Sigma$, it is necessary and sufficient that all tri-planes contained in $W$ are solutions of $\Sigma$. This is the case if the tri-plane belongs to $V$. On the other hand, if it is formed by the line $(\xi^{i})$ and a bi-plane contained in $V$, the bi-plane is therefore a solution of $\Sigma$, and the conditions under which the tri-plane is a solution of $\Sigma$ are precisely those of equations \eqref{eq:19}, \eqref{eq:20} and \eqref{eq:22}, taking into consideration \eqref{eq:21} and \eqref{eq:23}.

\begin{concl*}
  A line $(\xi^{i})$ is distinguished with respect to the system $\Sigma$, if and only if the $\xi^{i}$ satisfy the equations \eqref{eq:19}, \eqref{eq:20} and \eqref{eq:22}, the coefficients $u^{i}$ which appear in the equations \eqref{eq:20}  subject to the  condition that they annihilate the forms $F_{\alpha}$, and the coefficients $u^{ij}$ which appear in the equations \eqref{eq:22}  subject to the condition that they satisfy, first, the quadratic equations dictating that they are the coordinates of a bi-plane, and second, the equation \eqref{eq:23} dictating that the bi-plane is a solution of $\Sigma$.
\end{concl*}

It is easy to see how the search for distinguished lines must proceed given a system of any degree.

\vspace{12pt}\fsec \emph{Associated system of a system of exterior equations}. For now, we \emph{do not adjoin} the quadratic equations dictating that the $u^{ij}$ are the coordinates of a bi-plane to the conditions \eqref{eq:23} that must be  satisfied by the quantities contained in equation \eqref{eq:22} . We then have, for $\xi^{h}$, a system of conditions \eqref{eq:19},\eqref{eq:20}, \eqref{eq:22} which may be more\footnote{In the original work, the word here and in the following is ``less'', but apparently ``more'' is more appropriate here, since there are \emph{less} restrictions on the undetermined parameters in the constraints. --- \textsc{Translator}} restrictive than the conditions for distinguished lines. We can show that the new conditions obtained express the fact that the exterior forms in $u^{1},u^{2},\dots,u^{n}$,
\[
\xi^{i}\frac{\pd F_{\alpha}}{\pd u^{i}},\qquad
\xi^{i}\frac{\pd \Phi_{\alpha}}{\pd u^{i}},\qquad
\xi^{i}\frac{\pd \Psi_{\alpha}}{\pd u^{i}}
\]
belong to the ring of the forms $F_{\alpha},\Phi_{\alpha},\Psi_{\alpha}$, {i.e.}, the ring of the system $\Sigma$.

\begin{dfn*}
  The associated system to a system of exterior equations $\Sigma$ is the set of linear equations in $\xi^{1},\xi^{2},\dots,\xi^{n}$  expressing the fact that the exterior forms $\xi^{i}\pd H/\pd u^{i}$, where $H$ are the forms determining the system $\Sigma$, belong to the ring of the system.
\end{dfn*}

The associated system of an exterior form $F$, introduced in \textsection\textbf{8}, can now be defined as the set of linear equations in $\xi^{1},\xi^{2},\dots,\xi^{n}$ which express the fact that the form $\xi^{i}\pd F/\pd u^{i}$, considered as an exterior form in $u^{1},u^{2},\dots,u^{n}$, vanishes identically, \emph{i.e.}, belongs to the ring of $F$.\footnote{As an example of the case where the associated system is more restrictive than the system that dictates the lines are distinguished, let us consider the system $\Sigma$ of equations (\emph{c.f.}~\textsection\textbf{18})
\[
[u^{1}u^{3}]=
[u^{1}u^{4}]=
[u^{1}u^{2}]-[u^{3}u^{4}]=
[u^{1}u^{2}u^{5}]-[u^{3}u^{4}u^{6}]=
0.
\]
Its associated system is
\[
\xi^{1}=
\xi^{2}=
\xi^{3}=
\xi^{4}=
\xi^{5}-
\xi^{6}=
0,
\]
whereas the distinguished lines are determined by the equations
\[
\xi^{1}=
\xi^{2}=
\xi^{3}=
\xi^{4}=
0.
\]
}

\vspace{12pt}\fsec It is easily seen that two algebraically equivalent systems have the same associated system: the associated system is intrinsically linked to $\Sigma$. We now prove the following theorem:
\begin{thm*}
  If the associated system is of rank $p$ and, by a suitable change of variables, reduces to the relations $\xi^{1}=\xi^{2}=\dots=\xi^{p}=0$, then there exists a system algebraically equivalent to $\Sigma$ which contains only the variables $u^{1},u^{2},\dots,u^{p}$.
\end{thm*}

Indeed, consider the line, whose variables $\xi^{i}$ all vanish except $\xi^{p+1}=1$. The forms $\pd F_{\alpha}/\pd u^{p+1}$, which are constants, cannot belong to the ring of $\Sigma$ unless they are zero: the $F_{\alpha}$ does not contain the variables $u^{p+1}$. The forms
\begin{align*}
  \Phi^{*}_{\alpha}&=\Phi_{\alpha}-\left[u^{p+1}\frac{\pd\Phi_{\alpha}}{\pd u^{p+1}}\right],\\
  \Psi^{*}_{\alpha}&=\Psi_{\alpha}-\left[u^{p+1}\frac{\pd\Psi_{\alpha}}{\pd u^{p+1}}\right],
\end{align*}
obviously do not contain the variable $u^{p+1}$ either. But the form $\pd\Phi_{\alpha}/\pd u^{p+1}$ belongs to the ring of $F_{\alpha}$ and the form $\pd\Psi_{\alpha}/\pd u^{p+1}$ to the ring of $F_{\alpha}$ and $\Phi_{\alpha}$, and hence the system
\[
F_{\alpha}=0,\qquad \Phi^{*}_{\alpha}=0,\qquad \Psi^{*}_{\alpha}=0
\]
is algebraically equivalent to $\Sigma$: the equations of the system do not contain $u^{p+1}$. We can then proceed step by step to obtain a system algebraically equivalent to $\Sigma$ and that contains only the variables $u^{1},u^{2},\dots,u^{p}$.\qed

\begin{rmk*}
  The integer $p$ is obviously the minimal number of variables in which a system algebraically equivalent to $\Sigma$ can be written, since if such a system involves only $q<p$ variables, its associated system, which is the same as that of $\Sigma$, can be at most of rank $q$. Moreover, we see that the $p$ variables that appear in a system algebraically equivalent to $\Sigma$ in the minimal case are  well determined when taken as a whole.
\end{rmk*}

\begin{pcase*}
  If the system $\Sigma$ is of order only two, it can be seen easily that the associated system is obtained by adjoining to the equations of degree $1$ in $\Sigma$ the equations of the associated system of $r_{2}$ forms
\[
[F_{1}F_{2}\dots F_{r_{1}}\Phi_{\alpha}]=0\qquad(\alpha=1,2,\dots,r_{2}),
\]
equations which are obtained by setting to zero all the partial derivatives of order $r_{1}+1$ on the left hand side.
\end{pcase*}


\chapter{Exterior differential forms}
\label{cha:exter-diff-forms}

\section{Definition. Exterior differentiation}
\label{sec:defin-exter-diff}

\fsec Consider a space of $n$ dimensions, or a certain open set $\mathcal{D}$ therein. We suppose that this space is Euclidean. Although this assumption is not necessary, it serves  to simplify the language. Denote by $x^{1},x^{2},\dots,x^{n}$ the coordinates of this space.  \emph{Exterior differential forms of degree $p$} \index{exterior form!differential|see{differential form}}\index{differential form} are exterior forms whose \emph{variables} are the differentials $dx^{1},dx^{2},\dots,dx^{n}$ and whose coordinates are functions of the coordinates $x^{i}$ defined in the open set $\mathcal{D}$. A differential form of degree $1$, or a linear form, can be therefore written as
\[
a_{i}(x)dx^{i},
\]
and a quadratic differential form can be written as
\[
\frac{1}{2}a_{ij}(x)[dx^{i}dx^{j}]\qquad(a_{ij}=-a_{ji}),
\]
and so on.

By a convenient abuse of language, we consider a function of $x^{1},x^{2},\dots,x^{n}$ to be a differential form of degree zero.

By a change of coordinates, which acts on the \emph{variables} $dx^{i}$ as a linear transformation of non-zero determinant, all exterior forms of degree $p$ change to other exterior forms of the same degree. We make the natural assumption that the new coordinates can be differentiated with respect to the old ones.

\vspace{12pt}\fsec \emph{Exterior derivative of differential forms}. If a form is of degree zero, \emph{i.e.}, a function $f(x)$, its exterior derivative is by definition the ordinary differential $df$.\index{exterior differentiation}

The exterior derivative of a linear differential form
\[
\omega=a_{i}dx^{i}
\]
is \emph{by convention} the form of degree two
\[
d\omega=[da_{i}dx^{i}];
\]
and in the same manner the exterior derivative of a form of degree two
\[
\omega=\frac{1}{2}a_{ij}[dx^{i}dx^{j}]
\]is
\[
d\omega=\frac{1}{2}[da_{ij}dx^{i}dx^{j}]
\]
and so on.

This definition presupposes that the coefficients of the forms under consideration admit derivatives to first order, but we shall see that there are cases where we can define the exterior derivatives of a form, even if the coefficients are not differentiable.

\theoremstyle{shape1}
\newtheorem*{gdef*}{\hspace{15pt}General definition}

\begin{gdef*}
  If $\omega$ is an exterior differential form of degree $p$\index{differential form}
  \begin{equation}
    \label{eq:2.1}
    \omega=\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}[dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}],
  \end{equation}
then its exterior derivative $d\omega$ is
\begin{equation}
  \label{eq:2.2}
    d\omega=\frac{1}{p!}[da_{i_{1}i_{2}\dots i_{p}}dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}].
\end{equation}
\end{gdef*}

\vspace{12pt}\fsec We must show that the preceding definition is consistent. We  prove the following theorem:

\begin{thm*}
  If an exterior differential form $\omega(x,dx)$ transforms under a change of coordinates into the form $\varpi(y,dy)$, the exterior derivative $d\omega$ of the form $\omega$ transforms, under the same change of variables of the coordinates, into the exterior differential form $d\varpi$ of the form $\varpi$.
\end{thm*}

We will prove several lemmas, important in themselves, before proving the theorem.

\newtheorem*{lem1*}{\hspace{15pt}Lemma I}

\begin{lem1*}
  The exterior derivative of the differential of a function $f(x)$ vanishes.
\end{lem1*}
Indeed, consider the form
\[
\omega=\frac{\pd f}{\pd x^{i}}dx^{i},
\]
its exterior derivative is by definition
\[
d\omega=\left[d\frac{\pd f}{\pd x^{i}}dx^{i}\right]=\frac{\pd^{2} f}{\pd x^{i}\pd x^{j}}[dx^{i}dx^{j}],
\]
and the right hand side is zero because of the symmetry of the coefficients of $[dx^{i}dx^{j}]$ with respect to their indices.

\newtheorem*{lem2*}{\hspace{15pt}Lemma II}
\begin{lem2*}
  The exterior derivative of the exterior product of two forms $\omega,\varpi$ of degrees respectively $p$ and $q$ is\footnote{In the particular case where one of the form is of degree $0$, {i.e.}, a function $a$, we have
\[
d(a\omega)=[da\,\omega]+a\,d\omega,\qquad\text{or}\qquad d(\omega a)= d\omega\, a+(-1)^{p}[\omega\, da].
\]}
\begin{equation}
  \label{eq:2.3}
  d(\omega\varpi)=[d\omega\varpi]+(-1)^{p}[\omega d\varpi].
\end{equation}
\end{lem2*}

Indeed, consider
\[
\omega=\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}[dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}],\qquad
\varpi=\frac{1}{q!}b_{j_{1}j_{2}\dots j_{q}}[dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}].
\]

We have
\[
[\omega\varpi]=\frac{1}{p!}\frac{1}{q!}a_{i_{1}i_{2}\dots i_{p}}b_{j_{1}j_{2}\dots j_{q}}[dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}],
\]
from which
\[
d[\omega\varpi]=\frac{1}{p!}\frac{1}{q!}[d(a_{i_{1}i_{2}\dots i_{p}}b_{j_{1}j_{2}\dots j_{q}})dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}].
\]

We also have
\[
d(ab)=b\,da+a\,db,
\]
from which
\begin{align*}
d[\omega\varpi]&=\frac{1}{p!}\frac{1}{q!}b_{j_{1}j_{2}\dots j_{q}}[da_{i_{1}i_{2}\dots i_{p}}dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}]\\
&+\frac{1}{p!}\frac{1}{q!}a_{i_{1}i_{2}\dots i_{p}}[db_{j_{1}j_{2}\dots j_{q}}dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}].
\end{align*}

The first sum on the right hand side is nothing but the exterior product $[d\omega\,\varpi]$. As for the second sum, it is multiplied by $(-1)^{p}$ when the factor $db_{j_{1}j_{2}\dots j_{q}}$ is passed  behind the $p$-th factor $dx^{i_{p}}$, and up to  sign it is equal to $[\omega\,d\varpi]$. From this we deduce the formula
\[
d[\omega\varpi]=d[\omega\varpi]+(-1)^{p}[\omega\, d\varpi].
\]

This lemma generalises to the produce of any number of factors. For example
\begin{equation}
  \label{eq:2.4}
  d(\omega\varpi\chi)=[d\omega\,\varpi\chi]+(-1)^{p}[\omega \,d\varpi\,\chi]+(-1)^{p+q}[\omega\varpi\, d\chi],
\end{equation}
where $\omega$ is of degree $p$ and $\varpi$ is of degree $q$.

\vspace{12pt}\fsec We now come to the proof of the theorem. Take for example a differential form of degree $2$, and let
\[
\omega=a_{ij}[dx^{i}dx^{j}]
\]
to be the terms in the form. Let us express the variables $x^{i}$ using new variables $y^{i}$. The term under consideration is the exterior product of three factors, the first $a_{ij}$ a form of degree zero, and the second and third linear forms. We therefore have, passing to the variables $y^{i}$ and calling $\varpi$ the form obtained,
\[
d\varpi=[da_{ij}dx^{i}dx^{j}]+a_{ij}[(ddx^{i})dx^{j}]-a_{ij}[dx^{i}(ddx^{j})],
\]
but according to Lemma I the exterior derivatives of $dx^{i}$ and $dx^{j}$ vanish and consequently the form $d\varpi$ is obtained by replacing in the expression $[da_{ij}dx^{i}dx^{j}]$ of $d\omega$ the coordinates $x^{i}$ by functions of $y^{i}$.

\vspace{12pt}\fsec We could, with regards to the linear differential forms
\[
\omega=a_{i}dx^{i},
\]
relate exterior differentiation to the notion of \emph{bilinear covariant}.

Let us introduce a second symbol of differentiation $\delta$. We can regard the symbols $d\delta x^{i}$ as well as $\delta d x^{i}$ as constituting two new series of variables.  We  assume that the variables $\delta dx^{i}$ are identical to  $d\delta x^{i}$: this convention is consistent in the sense that it continues to hold with respect to a change of variables, since by expressing the new variables $y^{i}$ as functions of the old ones $x^{i}$, we have
\[
dy^{i}=\frac{\pd y^{i}}{\pd x^{k}}dx^{k},\qquad \delta dy^{i}=\frac{\pd^{2}y^{i}}{\pd x^{k}\pd x^{h}}\delta x^{h}dx^{k}+\frac{\pd y^{i}}{\pd x^{k}}\delta dx^{k},
\]
and
\[
\delta y^{i}=\frac{\pd y^{i}}{\pd x^{k}}\delta x^{k},\qquad d\delta y^{i}=\frac{\pd^{2}y^{i}}{\pd x^{k}\pd x^{h}}d x^{h}\delta x^{k}+\frac{\pd y^{i}}{\pd x^{k}}d\delta x^{k},
\]
and the comparison of $\delta d y^{i}$ and $d\delta y^{i}$ shows immediately their equality, if we take into consideration the symmetry properties of the second derivatives $\pd ^{2}y^{i}/\pd x^{k}\pd x^{h}$ with respect to the two indices of differentiation.

Denote by $\omega(d)$ and $\omega(\delta)$ the given differential forms according to whether we use the symbol of differentiation $d$ or $\delta$, and consider the expression
\[
d(\omega(\delta))-\delta(\omega(d))=d(a_{i}\delta x^{i})-\delta(a_{i}dx^{i})=da_{i}\delta x^{i}-\delta a_{a}dx^{i}+a_{i}(d\delta x^{i}-\delta dx^{i}).
\]
We obtain an alternating bilinear form in two series of variables $dx^{i}$ and $\delta x^{i}$. To this alternating bilinear form, which can be written
\[
\begin{vmatrix}
  da_{i}&dx^{i}\\
  \delta a_{i}&\delta x^{i}
\end{vmatrix},
\]
is associated the exterior form $[da_{i}dx^{i}]$, which is nothing but the exterior derivative of the form $a_{i}dx^{i}$. The obvious \emph{intrinsic} (covariant) character of the expression $d\omega(\delta)-\delta\omega(d)$ immediately entails the intrinsic character of the form $[da_{i}dx^{i}]$.

Also, to the exterior differential of a quadratic exterior form $\omega$ we can associate  the alternating trilinear form
\[
d_{1}\omega(d_{2},d_{3})-d_{2}\omega(d_{1},d_{3})+d_{3}\omega(d_{1},d_{2}),
\]
which involves three symbols of differentiations that are exchangeable amongst themselves.

\vspace{12pt}\addtocounter{frenchsec}{1}\index{Poincare's theorem@Poincar\'e's theorem}
\theoremstyle{shape1}
\newtheorem*{thm28}{\hspace{15pt}\textbf{28.} Poncar\'e's Theorem}

\begin{thm28}
  The exterior derivative of the exterior derivative of a differential form vanishes.
\end{thm28}

The proof is immediate. It suffices to verify the theorem for a monomic form
\[
\omega=a[dx^{1}dx^{2}\dots dx^{p}].
\]
We have
\[
d\omega=[da\,dx^{1}dx^{2}\dots dx^{p}].
\]
As each factor on the right hand side is an exact differential, the theorem obviously follows from the formula for the exterior differential of products.

Take for example, \emph{as verification}, the form
\begin{equation}
  \label{eq:2.5}
  \omega=P\,dx+Q\,dy+R\,dz,
\end{equation}
we have
\begin{align}
  \label{eq:2.6}
  d\omega&=[dP\,dx]+[dQ\,dy]+[dR\,dz]\\
  &=\left(\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}\right)[dy\,dz]+\left(\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}\right)[dz\,dx]+\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)[dx\,dy].\notag
\end{align}

Exterior differentiating a second time, we obtain the cubic form
\begin{align*}
  &\left[d\left(\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}\right)dy\,dz\right]+
  \left[d\left(\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}\right)dz\,dx\right]+
  \left[d\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)dx\,dy\right]\\
=&\left\{
\frac{\pd}{\pd x}\left(\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}\right)+
\frac{\pd}{\pd y}\left(\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}\right)+
\frac{\pd}{\pd z}\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)\right\}[dx\,dy\,dz]=0.
\end{align*}

Poincar\'e's theorem admits a converse, which will not be very helpful for us. Here is the theorem.
\begin{thm*}
  If a differential form of degree $p$ whose exterior differential vanishes is defined in the open set $\mathcal{D}$ homeomorphic to to the interior of a hypersphere, then there exists a differential form of degree $p-1$ defined in the same open set, whose exterior derivative is the given form.
\end{thm*}

Let us for the moment content ourselves with the verification of the theorem for $p=1$. The form \eqref{eq:2.5} for example have its exterior differential zero since we have, according to \eqref{eq:2.6},
\[
\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}=0,\qquad
\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}=0,\qquad
\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}=0,
\]
which are the necessary and sufficient conditions for the form $P\,dx+Q\,dy+R\,dz$ to be an exact differential form, {i.e.}, the exterior derivative of an exterior form of degree zero, or a function $f(x,y,z)$.

\section{Exterior differentiation and the general Stokes' formula}
\label{sec:exter-diff-gener}

\fsec The classical formulae of Cauchy-Green,  Stokes and  Ostrogradsky point to a remarkable link between the operation of exterior differentiation and  of integral calculus, which consists of converting an integral over the boundary of a $p+$ dimensional domain in the space to an equal integral over the domain. Consider for example the Cauchy-Green formula
\[
\int_{C}P\,dx+Q\,dy=\iint_{A}\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)dx\,dy,
\]
in which the left hand side is a line integral over the close contour $C$ of a plane and  right hand side is a double integral over the area $A$ bounded by the contour. As the exterior differential of $P\,dx+Q\,dy$ is
\[
[dP\,dx]+[dQ\,dy]=\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)[dx\,dy],
\]
we see that Cauchy's formula can be written as
\begin{equation}
  \label{eq:2.7}
  \int_{C}\omega=\iint_{A}d\omega.
\end{equation}

Note however, for this formula to be meaningful, the curve $C$ and the area $A$ must be oriented in a consistent way, and the sign of each of the integral is only determined when the curve or the area over which the integral is taken is oriented. In fact, the rule in this case is as follows. We first orient the area $A$ by agreeing that the area of the parallelogram constructed by two vectors $\vec{e}_{1}, \vec{e}_{2}$ in the order $\vec{e}_{1},\vec{e}_{2}$ is positive, and then the orientation of the curve $C$ is the following: on each point of $C$ we have a vector $\vec{\epsilon}_{1}$ pointing out of the area, and this leads to a vector $\vec{\epsilon}_{2}$ tangent to the boundary $C$ subject to the condition that the system $(\vec{\epsilon}_{1},\vec{\epsilon}_{2})$ is of positive orientation.

Stokes' formula can also be put into the form \eqref{eq:2.7} by taking $\omega$ to be the form \eqref{eq:5} and $d\omega$ the form \eqref{eq:2.6}, the first integral defined over a curve $C$ bounding over a portion of the surface $A$, and the second integral defined over the portion of the surface. There is also a consistent choice of orientation here for the boundary and the area, which is, \emph{mutatis mutandis}, the same  as indicated in the case of Cauchy's formula.

In fact, Ostrogradsky's formula takes the same form \eqref{eq:2.7} by putting
\begin{align*}
  \omega&=P[dy\,dz]+Q[dz\,dx]+R[dx\,dy],\\
  d\omega&=[dP\,dy\,dz]+[dQ\,dz\,dx]+[dR\,dx\,dy]=\left(\frac{\pd P}{\pd x}+\frac{\pd Q}{\pd y}+\frac{\pd R}{\pd z}\right)[dx\,dy\,dz],
\end{align*}
the integral $\int \omega$ is defined over a closed surface $S$ bounding a volume $V$ and the integral $\int d\omega$ is defined over the volume. We orient the volume by giving a triad $\vec{e}_{1},\vec{e}_{2},\vec{e}_{3}$ which can be drawn as a right trihedral, and the volume of the oriented parallelepiped constructed by the three vectors $\vec{e}_{1},\vec{e}_{2},\vec{e}_{3}$ is taken to be positive. To orient the surface $S$, we take at a point $M$ of the surface three vectors $\vec{\epsilon}_{1},\vec{\epsilon}_{2},\vec{\epsilon}_{3}$ forming a right trihedral, the first of which pointing out of the volume $V$ and the two other tangent to the surface $S$. Then the oriented parallelogram constructed from $\vec{\epsilon}_{2}$ and $\vec{\epsilon}_{3}$ is considered positive, which orients the surface.

We can prove that with analogous conventions of orientations, Stokes' formula becomes very general
\[
\int \omega=\int d\omega,
\]
the first integral being defined over a $p$-dimensional boundary of a $p+1$-dimensional domain over which the the second integral is defined.

\vspace{12pt}\fsec If the exterior derivative of a form $\omega$ of degree $p$ vanishes (following de Rham, we call such forms \emph{closed}), the integral $\int\omega$ over the boundary of a domain of $p+1$ dimensions is zero. In cases where the conditions we have implicitly assumed when we calculate the exterior differential does not hold ({i.e.}, the condition that the coefficients are differentiable), it may happen that the integral $\int \omega$ over the boundary of any $p+1$ dimensional domain is still zero.  In this case we say that the form has vanishing exterior derivative. More generally, given a form $\omega$ of degree $p$, if there exists a form $d\omega$ of degree $p+1$ such that the general Stokes' formula \eqref{eq:2.7} holds for any domain of $p+1$ dimensions, we then say that the form $d\omega$ is the exterior derivative of the form $\omega$, even if the coefficients of $\omega$ are \emph{only continuous} functions.

\vspace{12pt}\fsec We can prove Poincar\'e's theorem easily in the case where the coefficients of the exterior derivative $d\omega$ of a form $\omega$ are not differentiable: it suffices to suppose the existence of the exterior derivative. Indeed suppose $\omega$ is of degree $p$, then the general Stokes' formula tells us that the integral $\int d\omega$ over any $p+1$ dimensional domain is equal to the integral $\int \omega$ over the boundary of the domain. Then let $\Sigma$ be a closed manifold of $p+1$ dimensions, and divide it into two parts $\Sigma_{1}$ and $\Sigma_{2}$ by a closed manifold $C$ of $p$-dimensions. The integral $\int d\omega$ over $\Sigma_{1}$ and the integral $\int d\omega$ over $\Sigma_{2}$ are both equal to the integral $\int \omega$ over $C$, but the first integral with respect to a certain orientation of $C$ and the second the opposite orientation. The total integral $\int d\omega$ over $\Sigma$ is then zero: the form $d\omega$ is closed, which proves Poincar\'e's theorem.

\vspace{12pt}\fsec We can also prove the converse of Poincar\'e's theorem set out in \textsection
\textbf{22}. We will limit ourselves to the case $p=2$, which suffices to show how the proof can be done in the general case.\footnote{This proof and exposition is due to \textsc{Henri Cartan}, {c.f.}, for the particular case considered here, an article by \textsc{A.~Sz\"ucs}, \emph{Sur la variation des int\'egrales triples et le th\'eor\`eme de Stokes} (Acta Szeged, 3, 1927, pp.~81--95).}

Suppose $n=3$ and we place in the space a sphere $\Sigma$ with centre $O$.  Consider then the closed form
\begin{equation}
  \label{eq:2.8}
  \omega=P[dy\,dz]+Q[dz\,dx]+R[dx\,dy]
\end{equation}
whose integral over any closed surface inside the sphere $\Sigma$ have the value zero. Let $(C)$ be a closed curve lying inside the sphere. We are going to calculate the integral $\int\omega$ over the surface $S$ of the cone with the apex $O$ and the base curve $(C)$. We assume that the coordinates $x,y,z$ of a point under consideration of $(C)$ are expressed as a function of a curvilinear abscissa traced in the direction determined by the orientation of the curve. Every point $M$ on the surface $S$ have coordinates of the form
\[
X=tx,\qquad Y=ty,\qquad Z=tz,\qquad(0\le t\le 1),
\]
$x,y,z$ denoting the coordinates of the the point where the generating line $OM$ meets $(C)$. We then have
\[
\omega(X,Y,Z;dX,dY,dZ)=P(X,Y,Z)[(t\,dy+y\,dt)(t\,dz+z\,dt)]+\dots
\]
but $x,y,z$ are functions of the single parameter $s$, and hence the monomial $[dy\,dz]$ vanishes and we have
\[
[(t\,dy+y\,dt)(t\,dz+z\,dt)]=t[dt(y\,dz-z\,dy)],
\]
and consequently
\begin{align}
  \label{eq:2.9}
  \iint_{S}\omega&=\iint_{S}t\left[P(X,Y,Z)\left(y\frac{dz}{ds}-z\frac{dy}{ds}\right)+Q(X,Y,Z)\left(z\frac{dx}{ds}-x\frac{dz}{ds}\right)\right.\\
&+\left.R(X,Y,Z)\left(x\frac{dy}{ds}-y\frac{dx}{ds}\right)\right]dt\,ds\notag
\end{align}

The orientation of $S$ consistent with that of $(C)$ consists of regarding as positive the area of a small parallelogram constructed with two vectors originating from the point $(x,y,z)$ of $(C)$, the first $\vec{\epsilon}_{1}$ along the generating lines point out of $S$, {i.e.}, increasing $t$, the second $\vec{\epsilon}_{2}$ along the tangent to $(C)$, {i.e.}, increasing $s$. If we regard $t$ and $s$ as the rectangular coordinates of a plane, this amounts to taking the rotation that takes the $t$-axis to the $s$-axis as positive. We then have
\[
\iint_{S}H\,dt\,ds=\int_{0}^{l}ds\int_{0}^{1}H\,dt,
\]
where $l$ is the length of $(C)$, and $H$ is the coefficient of $dt\,ds$ on the right hand side of the equation \eqref{eq:2.9}. By putting

\begin{equation}
  \label{eq:2.10}
  \left\{
    \begin{aligned}
      A(x,y,z)&=\int_{0}^{1}t\,P(X,Y,Z)\,dt,\\
      B(x,y,z)&=\int_{0}^{1}t\,Q(X,Y,Z)\,dt,\\
      C(x,y,z)&=\int_{0}^{1}t\,R(X,Y,Z)\,dt,\\
    \end{aligned}
  \right.
\end{equation}
we have
\[
\iint_{S}\omega=\int_{(C)}A(y\,dz-z\,dy)+B(z\,dx-x\,dz)+C(x\,dy-y\,dx).
\]

By putting
\begin{equation}
  \label{eq:2.11}
  \varpi=A(y\,dz-z\,dy)+B(z\,dx-x\,dz)+C(x\,dy-y\,dx)
\end{equation}
we then obtain the relation
\[
\iint_{S}\omega=\int_{(C)}\varpi.
\]

The form $\omega$ is closed, and the integral $\iint\omega$ over any surface bounded by the curve $(C)$ (and lying inside of the sphere $\Sigma$) is the same as $\iint_{S}d\varpi$. Consequently \emph{we have constructed a form $\varpi$ of degree one which has its exterior derivative the form $\omega$ that was assumed to be closed.}

\vspace{12pt}\fsec We can verify that \emph{in the case where the coefficients $P,Q,R$ of the form $\omega$ are differentiable}, the exterior derivative of the form $\varpi$ is equal to $\omega$. Indeed, the form $\omega$ being closed, we have the relation
\begin{equation}
  \label{eq:2.12}
  \frac{\pd P}{\pd x}+\frac{\pd Q}{\pd y}+\frac{\pd R}{\pd z}=0.
\end{equation}

On the other hand we have
\[
d\varpi=\left\{ 2A+x\frac{\pd A}{\pd x}+y\frac{\pd A}{\pd y}+z\frac{\pd A}{\pd z}-x\left(\frac{\pd A}{\pd x}+\frac{\pd B}{\pd y}+\frac{\pd C}{\pd z}\right) \right\}[dy\,dz]+\dots
\]

We immediately see that $\pd A/\pd x+\pd B/\pd y+\pd C/\pd z$ is zero as a consequence of \eqref{eq:2.12}. On the other hand a simple calculation gives
\begin{align*}
  2A+x\frac{\pd A}{\pd x}+y\frac{\pd B}{\pd y}+z\frac{\pd C}{\pd z}&=\int_{0}^{1}\left[2t\,P(X,Y,Z)+t\left(X\frac{\pd P}{\pd X}+Y\frac{\pd P}{\pd Y}+Z\frac{\pd P}{\pd Z}\right)\right]dt\\
  &=\int_{0}^{1}\frac{\pd}{\pd t}[t^{2}P(X,Y,Z)]dt=P(x,y,z),
\end{align*}
which gives
\[
d\varpi=\omega.
\]

\vspace{12pt}\addtocounter{frenchsec}{1}

\theoremstyle{shape0}
\newtheorem*{rmk34}{\hspace{15pt}\textbf{34.} Remark I}
\newtheorem*{rmkii}{\hspace{15pt}Remark II}
\begin{rmk34}
  Sometimes the integral $\int\omega$, where $\omega$ is a closed differential form of degree $p$, does not vanish when it is taken over certain closed manifolds of $p$ dimensions: this happens if there does not exist a $p+1$ dimensional domain in which this manifold is a boundary of or if the domain contains points where the coefficients of the form becomes infinite. This is so for the integral
\[
\iint\frac{x\,dy\,dz+y\,dz\,dx+z\,dx\,dy}{(x^{2}+y^{2}+z^{2})^{3/2}},
\]
which is zero if it is taken over a closed surface that does not contain the origin in its interior, but not otherwise, in which case its value becomes the value of the solid angle when the surface is viewed from the origin ($4\pi$ for the sphere). In all cases the value of the integral does not vary when the closed surface is continuously deformed without passing through the origin.  
\end{rmk34}
\begin{rmkii}
  Finally we can prove that whenever we can define the exterior derivative of a form $\omega$ with the procedure in \textsection\textbf{24}, the general theorems stated and proved in the case where we used the primitive definition by  derivation of coefficients,  continue to be true in a certain sense. For example, the formula
\[
d[\omega\varpi]=[d\omega\varpi]+(-1)^{p}[\omega d\varpi]
\]
is true whenever $\omega$ and $\varpi$ admit generalised exterior derivatives.\footnote{Regarding this subject, see a memoir of \textsc{P.~Gillis}, \emph{Sur les formes diff\'erentielles et la formule de Stokes} (M\'em.~Acad.~Blgique, 20, 1943).}
\end{rmkii}

\chapter{Exterior differential systems. Characteristic system}
\label{cha:exter-diff-syst}

\section[Generalities. Completely integrable systems]{Generalities.\\Completely integrable systems}
\label{sec:gener-compl-integr}

\fsec The differential systems that we are going to study are obtained by setting to zero a certain number of functions of $n$ variables $x^{1},x^{2},\dots,x^{n}$, which we always consider as the coordinates of  points in a $n$ dimensional space, or an open set $\mathcal{D}$ therein, and a certain number of exterior differential forms defined in the open set, which can have any degree.  In the general case we are obliged  to assume that  the functions that we introduce are  \emph{analytic}. We  further assume  that they are analytic on the real domain\footnote{For more details on the analytic functions of real variables, see \textsc{G.~Valiron}, \emph{Sur les fonctions analytiques d'une variable r\'eelle} (Nouvelles Annales, 1, 1922, pp.~321--329).}. The problems that we will be concerned with and the theorems we will prove will always be local in character. Clearly we can still change variables, but in  problems where the variables are assumed to be analytic, the new variables are necessarily analytic functions of the old ones.

\vspace{12pt}\fsec The differential systems of linear equations have been extensively studied. Let
\begin{equation}
  \label{eq:3.1}
  \theta_{\alpha}\equiv A_{\alpha i}dx^{i}=0\qquad(\alpha=1,2,\dots,r)
\end{equation}
be the equations of such a system. We suppose that the linear forms $\theta_{\alpha}$ are linearly independent when we give the variables $x^{i}$ in the coefficients $A_{\alpha i}$ generic values. In general, we say that the point $(x^{i})$ in the space is generic if the rank of the matrix of the coefficients $A_{\alpha i}$  at the point is equal to $r$.

An \emph{integral manifold} of  system \eqref{eq:3.1} is  the manifold defined by a certain number of relations between the variables such that the relations between the variables and those that  can be deduced from them by differentiation identically imply the  vanishing of the forms $\theta_{\alpha}$. This naturally presupposes that the left hand side of the equations defining the manifolds are differentiable.

\emph{In particular}, consider integral manifolds of $(n-r)$ dimensions.  Let us find whether there exists an integral manifold in the space passing through a \emph{given generic} point $M_{0}$ with coordinates $(x^{i})_{0}$. Suppose at the point the determinant of degree $r$ constructed with the $r$ rows and the last $r$ columns of the matrix $A_{\alpha i}$ is non-zero.  Near this point the equations \eqref{eq:3.1} can be solved as the differentials of  $r$ coordinates, which we will denote for convenience as $z^{1},z^{2},\dots,z^{r}$. We see that the integral manifold, if it exists, can be defined by specifying  $z^{1},z^{2},\dots,z^{r}$ in terms of functions of $x^{1},x^{2},\dots,x^{n-r}$ in a suitable manner.

\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape1}
\newtheorem*{thm37}{\hspace{15pt}\textbf{37.} Theorem I}
\begin{thm37}
  If there exists an integral manifold passing through a given generic point, we can obtain it by integrating a system of ordinary differential equations and the integral manifold is unique.
\end{thm37}

Regard $x^{1},x^{2},\dots,x^{h}$ $(h=n-r)$ as the coordinates of points in an Euclidean space of $h$ dimensions and denote by $O$ the point of coordinates $(x^{1})_{0},(x^{2})_{0},\dots,(x^{h})_{0}$ in the space. Inside the hypersphere $\Sigma$ of centre $O$ and radius $R$ in this space we have the radii originating from $O$, each determined by the parameters $a^{1},a^{2},\dots,a^{h}$ of a unit vector based at the centre. For any integral manifold found, the $z^{\alpha}$ $(\alpha=1,2,\dots,r)$ are the functions of the coordinates of points interior to $\Sigma$, and the coordinates can be written
\[
x^{1}=a^{1}t,\quad x^{2}=a^{2}t,\quad\dots\quad x^{h}=a^{h}t,\qquad(0\le t\le R).
\] 

When we move along on the ray, the unknown functions $z^{\alpha}$ satisfy the equations obtained by replacing, in the forms $\theta_{\beta}$, the $x^{i}$ by $a^{i}t$ and the $dx^{i}$ by $a^{i}dt$. We have therefore a system of ordinary differential equations
\begin{equation}
  \label{eq:3.2}
  \frac{d z^{\alpha}}{dt}=\varphi^{\alpha}(a^{1},a^{2},\dots,a^{h},t)\qquad(\alpha=1,2,\dots,r),
\end{equation}
and we can integrate these with the initial conditions
\[
z^{\alpha}=(x^{n-r+\alpha})_{0}\qquad\text{for}\qquad x^{i}=(x^{i})_{0}.
\]

For each ray we are sure that the integration can be carried out for a certain interval $(0,t_{0})$, $t_{0}$ being a continuous function of $a^{1},a^{2},\dots,a^{h}$. Hence $t_{0}$ has a lower bound, and it is this lower bound that we take the value of $R$ to be.

Therefore we see that if there exists an integral manifold of $n-r$ dimensions passing through the point $M_{0}$, then it is given, in the interior of the hypersphere $\Sigma$, by integration of a system of ordinary differential equations and it is unique.\qed

\vspace{12pt}\fsec \emph{Completely integrable systems of differential equations}. \emph{The system \eqref{eq:3.1} is completely integrable if for every generic point of the space and a sufficiently small neighbourhood around it, there passes an integral manifold of dimension $n-r$.}

We have seen, in the previous section, how we can find the integral manifold in the case where it exists.

To find the complete integrability condition of the system \eqref{eq:3.1}, we make the following remark, \emph{which plays a fundamental role in the general theory of differential systems}, and which can be no simpler, namely that \emph{every manifold that annihilates an exterior differential form also annihilates the form resulting from it by exterior differentiation}. All integral manifolds of the system \eqref{eq:3.1} therefore must annihilate the $r$ forms $d\theta_{\alpha}$. If we calculate these forms we can, instead of expressing them as quadratic forms in $dx^{1},dx^{2},\dots,dx^{h},dz^{1},dz^{2},\dots,dz^{r}$, express them as a quadratic forms of the following $r$ forms, \emph{which are independent in a neighbourhood of $M_{0}$},
\[
dx^{1},dx^{2},\dots,dx^{h},\theta_{1},\theta_{2},\dots,\theta_{r}.
\]
We therefore write
\begin{equation}
  \label{eq:3.3}
  d\theta_{\alpha}=\frac{1}{2}C_{\alpha ij}[dx^{i}dx^{j}]+D^{\lambda}_{\alpha i}[dx^{i}\theta_{\lambda}]+\frac{1}{2}E_{\alpha}^{\lambda\mu}[\theta_{\lambda}\theta_{\mu}],
\end{equation}
in the formula the summation indices $i$ and $j$ varies from $1$ to $h$, and the summation indices $\lambda$ and $\mu$ from $1$ to $r$. All integral manifolds passing through $M_{0}$, which annihilate the form $\theta_{\alpha}$, will annihilate the form $\tfrac{1}{2}C_{\alpha ij}dx^{i}dx^{j}$ and therefore, on all points of this manifold lying in a neighbourhood of $M_{0}$, the coefficients $C_{\alpha ij}$ are zero. As these must be zero regardless of the initial values for the functions $z^{\alpha}$ for $x^{i}=(x^{i})_{0}$, we can conclude that the functions $C_{\alpha ij}$ must be zero in a sufficiently small neighbourhood of $M_{0}$, and we have the

\begin{thm*}
   The system \eqref{eq:3.1} is completely integrable if in the neighbourhood in every generic point of the space, the forms $d\theta_{\alpha}$, which are exterior derivatives of the forms $\theta_{\alpha}$, belong to the ring of the forms $\theta_{\alpha}$. \rm{We can express the condition by writing it as a congruence}
\[
d\theta_{\alpha}\equiv 0\pmod{\theta_{1},\theta_{2},\dots,\theta_{r}}.
\]
\end{thm*}

This can again be expressed in a more precise manner, as we have already seen, by using the existence of linear forms $\varpi^{i}{}_{\alpha}$ regular in a neighbourhood of a generic point under consideration in the space to write
\begin{equation}
  \label{eq:3.4}
  d\theta_{\alpha}=[\theta_{1}\varpi^{1}_{\alpha}]+[\theta_{2}\varpi^{2}_{\alpha}]+\dots+[\theta_{r}\varpi^{r}_{\alpha}].
\end{equation}

\vspace{12pt}\fsec We now prove the converse. Let us go back to the manifold of $n-r$ dimensions defined by the differential system \eqref{eq:3.2} and passing through the point $M_{0}$ of coordinates $(x^{i})_{0}$, $(z^{\alpha})_{0}$. If we replace the functions $z^{\alpha}$ by their values as functions of the arguments $a^{i}$, $t$, we have, according to the same manner that we have obtained them,
\[
\theta_{\alpha}=P_{\alpha k}(a,t)da^{k}\qquad(\alpha=1,2,\dots,r),
\]
and also
\[
\varpi^{i}_{\alpha}=Q^{i}_{\alpha}(a,t)dt+Q^{i}_{\alpha k}(a,t)da^{k}\qquad(\alpha=1,2,\dots,r).
\]

The relations \eqref{eq:3.4} will only hold when the two terms containing $dt$ satisfies
\[
\frac{\pd P_{\alpha k}}{\pd t}[dt\,da^{k}]=P_{\lambda k}Q^{\lambda}_{\alpha}[da^{k}dt],
\]
or
\begin{equation}
  \label{eq:3.5}
  \frac{\pd P_{\alpha i}}{\pd t}+Q^{\lambda}_{\alpha}P_{\lambda i}=0.
\end{equation}

For each value of the index $i$, the $r$ functions $P_{\alpha i}(a,t)$, considered as functions of $t$, satisfy a linear and homogeneous system of equations (a system which remains the same for any value of $i$). For $t=0$, the functions $P_{\alpha i}$ are all zero since for $t=0$, the functions $x^{i}$ and $z^{\alpha}$ are fixed and independent of $a^{k}$, and hence when we set $t=0$ in their coefficients, their differentials does not contain the terms in $da^{1},da^{2},\dots,da^{k}$. The initial values of the unknown functions $P_{\alpha i}$ of system \eqref{eq:3.5} are zero, hence these functions are identically zero, and therefore the manifold determined by integration of equations \eqref{eq:3.2} identically annihilate the forms $\theta_{\alpha}$, therefore it is an integral manifold.

We express this result in the following form:
\begin{thm*}
  A system of $r$ linear differential equations is completely integrable if and only if at all generic points of the space, the exterior derivatives $d\theta_{\alpha}$ of the forms $\theta_{\alpha}$ defining the equations of the system belong to the ring of these forms.
\end{thm*}


\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape0}
\newtheorem*{rmk40}{\hspace{15pt}\textbf{40.} Remark I}
\begin{rmk40}
  The complete integrability condition requires only the existence of an integral manifold of $n-r$ dimensions passing through  \emph{generic} points of the space, and it is only in a neighbourhood of a generic point that the forms $d\theta_{\alpha}$ must belong to the ring of the forms $\theta_{\alpha}$. It can fail to be so in the neighbourhood of a non-generic point. For example, the equation
\[
\theta\equiv x\,dy-y\,dx=0
\]
is completely integrable, as any other ordinary differential equation. However we cannot find any linear form $\varpi=A\,dx+B\,dy$, regular and continuous in a neighbourhood of every given point in the plane and satisfy
\[
d\theta=[\theta\varpi]
\]
in other words,
\[
2[dx\,dy]=-(Ax+By)[dx\,dy].
\]
Indeed, for $x=y=0$, it is impossible to satisfy
\[
2=-(Ax+By).
\]
\begin{rmkii}
  We can put the complete integrability condition into a form which does not require us to specially check  for generic points. Indeed the equations \eqref{eq:3.4} give the relations
  \begin{equation}
    \label{eq:3.6}
    [\theta_{1}\theta_{2}\dots\theta_{r}d\theta_{\alpha}]=0\qquad(\alpha=1,2,\dots,r),
  \end{equation}
and conversely in the neighbourhood of any generic point where the linear forms $\theta_{1},\theta_{2},\dots,\theta_{r}$ are independent, these relations imply the existence of the forms $\varpi^{\alpha}_{\beta}$ satisfying the relations \eqref{eq:3.4}. As all non-generic points can be regarded as the limit of a sequence of generic points, the relations \eqref{eq:3.6} hold for all generic points and continue to hold for all non-generic points.

\emph{The relations \eqref{eq:3.6} therefore gives the necessary and sufficient conditions for complete integrability of system \eqref{eq:3.1}}, and is of a more convenient form than the original criterion  we  found.
\end{rmkii}

\newtheorem*{rmkiii}{\hspace{15pt}Remark III}

\begin{rmkiii}
  If the system is completely integrable, there exists $r$ independent functions $\varphi_{i}(x^{1},x^{2},\dots,x^{n})$ defined in a neighbourhood of a generic point in the space and they are constants on any integral manifold (\emph{first integrals} of the system), so that the given system is equivalent to the system $d\varphi_{1}=d\varphi_{2}=\dots=d\varphi_{r}=0$. The converse is obvious.
\end{rmkiii}
\end{rmk40}

\newtheorem*{rmkiv}{\hspace{15pt}Remark IV}
\begin{rmkiv}
  Everything that has been said does not require the analyticity of the coefficients $A_{\alpha i}$ of the given equations and requires only the existence of first order partial derivatives of the coefficients\footnote{The existence of the partial derivatives are necessary  to ensure the existence of the forms $d\theta_{\alpha}$.}, the reason being that we have reduced the task of finding  integral manifolds to the integration of ordinary differential equations.
\end{rmkiv}

\section{Closed differential systems. Characteristic system}
\label{sec:clos-diff-syst}

\fsec Let us return to differential equations obtained by setting to zero a certain number of exterior differential forms, some of which can be of degree zero, {i.e.,} functions of some variables. A solution of such a system can be regarded as analytically representing  a manifold in the space of $n$ dimensions, which we will call \emph{integral manifold}. The integral manifold is defined by a certain number of relations between the variables such that by differentiating these relations and the linear relations between the differentials $dx^{1},dx^{2},\dots,dx^{n}$, we can identically deduce the  vanishing of the differential forms that define the given system. If this system involves any algebraic relation between the variables, these relations will necessarily appear in the relations of any integral manifold.

It is clear that any integral manifold is a solution of the given system adjoined by all equations obtained by exterior differentiation of the equations in the system, since if a differential form vanishes, its exterior derivative also vanishes. The new differential system obtained  obviously cannot be extended further with the same procedure, according to Poincar\'e's theorem.

\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape1}
\newtheorem*{dfn42}{\hspace{15pt}\textbf{42.} Definition}
\begin{dfn42}
  A differential system is closed with respect to the operation of exterior differentiation, or simply closed, if the exterior derivatives of the forms defining the equations belong to the ring of these forms.
\end{dfn42}

It is clear that if we enlarge a system by adjoining to it the equations obtained by exterior differentiation, we obtain a closed system, since exterior differentiating the new system gives us only zero or equations already in the system. We say that the system thus enlarged is the \emph{closure} of the given system.

It is easily seen that if wo differential systems are algebraically equivalent, the closure of them are also algebraically equivalent.

Following the considerations in \textsection\textbf{41},  the  closure of a given system admit the same solutions as the original system, regardless of the dimension of integral manifolds under consideration. We have the
\newtheorem*{prin*}{\hspace{15pt}Principle}
\begin{prin*}
  The search for  solutions of a differential system can always be reduced to the search for  solutions of a closed differential system.
\end{prin*}

\vspace{12pt}\fsec\emph{Characteristic system of a differential system} We have seen in chapter 1 that a system of exterior equations can always be expressed, by means of change of variables and by replacing the given system by an algebraically equivalent system, as a system with a minimum number of variables: the number is well defined and is given by the rank of the associated system, and the new variables are linear combinations of the old variables and, when set to zero, give the associated system.

When we are concerned with a system of exterior differential equations, we can also ask if we can apply a change of variables and replace the system by an algebraically equivalent system in such a way that the new system contains in its coefficients and differentials a minimum number of variables. We will see that this is possible and the solution of the problem is provided by the consideration of the \emph{characteristic system}.

\begin{dfn*}
  The characteristic system of a differential system is the associated system of the closure of the given system.
\end{dfn*}

We are going to prove the following theorem:

\vspace{12pt}\addtocounter{frenchsec}{1}
\newtheorem*{thm44}{\hspace{15pt}\textbf{44.} Theorem}

\begin{thm44}
  The characteristic system of a differential system $\Sigma$ is completely integrable. Moreover, if $y^{1},y^{2},\dots,y^{p}$ constitute a system of independent first integrals, there then exists a system algebraically equivalent to $\Sigma$, constructed by the differentials $dy^{1},dy^{2},\dots,dy^{p}$, the coefficients being functions of $y^{1},y^{2},\dots,y^{p}$.
\end{thm44}

For simplicity and without loss of generality, we suppose that the system $\Sigma$ does not contain algebraic equations in $x^{1},x^{2},\dots,x^{n}$. It suffices to prove the theorem in the case where the system $\Sigma$ is closed.  Consider the a system of degree $3$ defined by the following equations,
\begin{equation}
  \label{eq:3.7}
  \left\{
    \begin{aligned}
      \theta_{\alpha}&\equiv A_{\alpha i}dx^{i}=0&(\alpha&=1,2,\dots,r_{1}),\\
      \varphi_{\alpha}&\equiv \frac{1}{2}A_{\alpha ij}[dx^{i}dx^{j}]=0&(\alpha&=1,2,\dots,r_{2}),\\
      \psi_{\alpha}&\equiv \frac{1}{6}A_{\alpha ijk}[dx^{i}dx^{j}dx^{k}]=0&(\alpha&=1,2,\dots,r_{3}).
    \end{aligned}
  \right.
\end{equation}

If the rank of the characteristic system is equal to $n$, the theorem is trivial. If the rank is an integer $p<n$, in the case of $p<n-1$, let us adjoin to the equations of the characteristic system of another $n-1-p$ linear equations independent among themselves and with respect to the original equations. We then obtain a system of ordinary differential equations which we suppose, in order not to clutter notation, that the variables $x^{1},x^{2},\dots,x^{n-1}$ are first integrals.

First, we know that we can find a system algebraically equivalent to $\Sigma$ which does not contain the differential $dx^{n}$ (\textsection\textbf{22}).

Suppose that this has already been done, that the left hand side of equations \eqref{eq:3.7} does not contain $dx^{n}$.  Notice that the derivative with respect to $x^{n}$ of any left hand side of these equations, $\varphi$ for example, is nothing but the derivative of the form $d\varphi_{\alpha}$ with respect to $dx^{n}$, and  it must also belong to the ring of the system. We therefore have the congruences
\begin{equation}
  \label{eq:3.8}
  \left\{
    \begin{aligned}
      \frac{\pd \theta_{\alpha}}{\pd x^{n}}&\equiv 0&&\pmod{\theta_{1},\theta_{2},\dots,\theta_{r_{1}}},\\
      \frac{\pd \varphi_{\alpha}}{\pd x^{n}}&\equiv 0&&\pmod{\varphi_{1},\dots,\varphi_{r_{2}},\theta_{1},\theta_{2},\dots,\theta_{r_{1}}},\\
      \frac{\pd \psi_{\alpha}}{\pd x^{n}}&\equiv 0&&\pmod{\psi_{1},\dots,\psi_{r_{3}},\varphi_{1},\dots,\varphi_{r_{2}},\theta_{1},\theta_{2},\dots,\theta_{r_{1}}}.
    \end{aligned}
  \right.
\end{equation}

The first congruences of \eqref{eq:3.8} can be written
\begin{equation}
  \label{eq:3.9}
  \frac{\pd \theta_{\alpha}}{\pd x^{n}}=H_{\alpha}^{\beta}\theta_{\beta}.
\end{equation}

Consider the system of ordinary differential equations
\begin{equation}
  \label{eq:3.10}
  \frac{dz_{\alpha}}{dx^{n}}=H^{\beta}_{\alpha}z_{\beta},
\end{equation}
where the coefficients $H_{\alpha}^{\beta}$ are functions of $x^{1},x^{2},\dots,x^{n}$. Let $\overline z^{(1)}_{\alpha},\overline z^{(2)}_{\alpha},\dots,\overline z^{(r_{1})}_{\alpha}$ be a system of $r_{1}$ independent solutions of the system. There will exist linear forms \emph{independent} of $x^{n}$, which we will denote by $\overline\theta_{1},\overline\theta_{2},\dots,\overline\theta_{r_{1}}$, such that we have
\[
\theta_{\alpha}=\overline\theta_{1}\overline z^{(1)}_{\alpha}+\overline\theta_{2}\overline z^{(2)}_{\alpha}+\dots+\overline\theta_{r_{1}}\overline z^{(r_{1})}_{\alpha},
\]
but then the system of equations $\theta_{\alpha}=0$ is equivalent to the system of equations $\overline\theta_{\alpha}=0$ whose left hand side does not involve either $x^{n}$ or $dx^{n}$. Consequently, we can suppose, by replacing $\Sigma$ by an algebraically equivalent system, the the forms $\theta_{\alpha}$ contains neither $x^{n}$ nor $dx^{n}$.

Consider now the forms $\varphi_{\alpha}$. We have
\begin{equation}
  \label{eq:3.11}
  \frac{\pd \varphi_{\alpha}}{\pd x^{n}}=K^{\beta}_{\alpha}\varphi_{\beta}+[\varpi_{\alpha}^{\gamma}\theta_{\gamma}],
\end{equation}
the $\varpi_{\alpha}^{\gamma}$ being linear forms not involving $dx^{n}$ and the $\theta_{\gamma}$ depends on neither $x^{n}$ nor $dx^{n}$.

Consider this time the system of differential equations
\begin{equation}
  \label{eq:3.12}
  \frac{du_{\alpha}}{dx^{n}}=K_{\alpha}^{\beta}u_{\beta}\qquad(\alpha=1,2,\dots,r_{2})
\end{equation}
and a system of $r_{2}$ independent solutions $\overline u_{\alpha}^{(i)}$ $(i=1,2,\dots,r_{2})$. Set
\[
\varphi_{\alpha}=\varphi_{\beta}^{*}\overline u_{\alpha}^{(\beta)},
\]
where the $\varphi^{*}_{\beta}$ are new quadratic exterior forms. The system \eqref{eq:3.11} take the form
\[
\frac{\pd \varphi_{\alpha}^{*}}{\pd x^{n}}=[(\varpi_{\alpha}^{\beta})^{*}\theta_{\beta}].
\]
If we denote by $\chi_{\alpha}^{\beta}$ the primitive functions of $(\varpi_{\alpha}^{\beta})^{*}$, considered as functions of $x^{n}$,  the form
\[
\varphi_{\alpha}^{*}-[\chi_{\alpha}^{\beta}\theta_{\beta}]
\]
depends on neither $x^{n}$ nor $dx^{n}$. Now the system $\theta_{\alpha}=\varphi_{\alpha}^{*}=0$ is algebraically equivalent to the system $\theta_{\alpha}=\varphi_{\alpha}=0$. We can therefore suppose, by replacing $\Sigma$ by an algebraically equivalent system, that the left hand side of first and second degree equations of \eqref{eq:3.7} depends on neither $x^{n}$ nor $dx^{n}$.

We continue in the same manner for the left hand sides of  third degree equations and step by step we complete the proof of the existence of a system algebraically equivalent to the given system which does not contain $x^{n}$ or $dx^{n}$.

If $p<n-1$, we apply the same procedure on this system $\Sigma$ to obtain an algebraically equivalent system which involves only $n-2$ variables. Continue further in the same way we will eventually arrive at a system which contains only $p$ variables and their differentials.

The theorem is hence proven, since if the $p$ variables are $y^{1},y^{2},\dots,y^{p}$, the characteristic system is
\[
dy^{1}=0,\qquad dy^{2}=0,\qquad \dots\qquad dy^{p}=0,
\]
and this system is completely integrable and its most general integral manifold is obtained by setting $y^{1},y^{2},\dots,y^{p}$ to arbitrary constants.

On the other hand, it is evident that we can never find a system algebraically equivalent to the given system which involves less than $p$ variables and their differentials\footnote{The application of the theorem in \textsection\textbf{38} to prove that the characteristic system is completely integrable is a simple  exercise in calculus if the given system is linear. We leave this calculation for later.}.\qed

The \emph{class} of an exterior differential system is the rank of its characteristic system.


\vspace{12pt}\fsec \textsc{Definition.}  {A} \emph{characteristic manifold} {is a $n-p$ dimensional manifold that is a solution of the characteristic system. The following property is evident.}

\begin{thm*}
  Given any integral manifold $V$ of a system $\Sigma$, the manifold obtained by enlarging with the characteristic manifold of each point of $V$ passing through the point is also an integral manifold.
\end{thm*}

In particular, this leads to the 
\begin{thm*}
  If the integral manifold $V$ of a system $\Sigma$ is not contained in any integral manifold of larger dimensions, then it is generated by  characteristic manifolds \footnote{ This theorem would fail if the rank of the characteristic system is less than its usual value  at every point of the integral manifold $V$. We would then be dealing with a singular integral manifold. An example is provided by the singular solutions of an first order partial differential equation.}.
\end{thm*}

Indeed, if it were not so, the characteristic manifolds at  different points of $V$ would together form an integral manifold with more dimensions that $V$.

\vspace{12pt}\fsec If the system $\Sigma$ is not complete (\textsection\textbf{18}) and are able to complete it, we would pass from the characteristic system of $\Sigma$ to a new characteristic system whose rank may drop. If the rank remains the same, the characteristic system does not change.

Take as an example the system
\begin{equation}
  \label{eq:3.13}
  [dx^{1}dx^{3}]=[dx^{1}dx^{4}]=[dx^{3}dx^{4}]-x^{5}[dx^{1}dx^{3}]=0,
\end{equation}
which is closed by the new equation
\begin{equation}
  \label{eq:3.14}
  [dx^{1}dx^{2}dx^{5}]=0.
\end{equation}
The characteristic system is
\[
dx^{1}=dx^{2}=dx^{3}=dx^{4}=dx^{5}=0.
\]

The system \eqref{eq:3.13} is not complete. A complete system admitting the same solutions as the system \eqref{eq:3.13} is provided by the equations
\begin{equation}
  \label{eq:3.15}
  [dx^{1}dx^{3}]=[dx^{1}dx^{4}]=[dx^{1}dx^{2}]=[dx^{3}dx^{4}]=0.
\end{equation}

The characteristic system is
\[
dx^{1}=dx^{2}=dx^{3}=dx^{4}=0.
\]

\section{Applications to the Pfaffian problem}
\label{sec:appl-pfaff-probl}

\fsec Now we apply the preceding results to the case of a single linear differential equation (Pfaffian equation)
\begin{equation}
  \label{eq:3.16'}
  \theta\equiv A_{i}dx^{i}=0.
\end{equation}
The characteristic system is formed by the equation \eqref{eq:3.16'} together with the associated system of the exterior derivative $d\theta$ after replacing any differential $dx^{i}$ by its value determined by \eqref{eq:3.16'}. The associated system being of even rank, it gives the

\begin{thm*}
  For every linear differential equation, the class is an odd number.
\end{thm*}

If the class is equal to $1$, then the characteristic system must be the equation \eqref{eq:3.16'}, which is therefore completely integrable. If $Z$ is a first integral, the given equation is equivalent to $dZ=0$.

In the general case, let the class be $2p+1$. Let $X_{1}$ be a  first integral of the characteristic system. If we constrain the $n$ variables by the equation $X^{1}=C^{1}$ where $C^{1}$ is an arbitrary constant, we remove at least one equation from the characteristic system, but as the class is always odd, we have removed at least two equations.  Let $X^{2}$ be a first integral of the new characteristic system, $X^{2}$ being a function of $x^{i}$ and $C^{1}$, or just of $x^{i}$ (if we replace $C^{1}$ by $X^{1}$). By constraining the variables with the two relations
\[
X^{1}=C^{1},\qquad X^{2}=C^{2},
\]
{i.e.}, constraining the differentials $dx^{i}$ by the two relations
\[
dX^{1}=0,\qquad dX^{2}=0,
\]
the class of the system is again reduced by at least two. Continuing step by step,  there will be $p$ independent first integrals $X^{1},X^{2},\dots,X^{p}$ such that if we constrain the variables by the $p$ relations
\[
X^{1}=C^{1},\qquad X^{2}=C^{2},\qquad\dots\qquad X^{p}=C^{p},
\]
the equation $\theta=0$ becomes completely integrable, and hence reducible to the form $dZ=0$.

This reduction is only valid because we have assumed that the functions $X^{i}$ are constants. If we no longer assume it, then the equation $\theta=0$ is reducible to the form
\[
dZ-Y_{1}dX^{1}-Y_{2}dX^{2}-\dots-Y_{p}dX^{p}=0,
\]
the coefficients $Y_{i}$ being $p$ suitably chosen functions of the given variables. The functions $X^{i}$, $Y_{i}$ and $Z$ now constitute a system of $2p+1$ independent functions, otherwise the equation \eqref{eq:3.16'} can be expressed by  less than $2p+1$ variables, which would imply that the class is less than $2p+1$.

\begin{thm*}
  Every Pfaffian form of class $2p+1$ is reducible to the canonical form
\[
dZ-Y_{1}dX^{1}-Y_{2}dX^{2}-\dots-Y_{p}dX^{p}=0.
\]
\end{thm*}


\vspace{12pt}\fsec  \textsc{Remark.} {We immediately see that if the equation \eqref{eq:3.16'} is of class $2p+1$, the integer $p$ is the largest integer such that the form of degree $2p+1$}
\[
[\theta(d\theta)^{p}]
\]
{does not  vanish identically. For example, the equation}
\[
\theta\equiv P\,dx+Q\,dy+R\,dz=0
\]
{is in  general  of class $3$, unless we have}
\begin{align*}
  [\theta d\theta]&\equiv[(P\,dx+Q\,dy+R\,dz)(dP\,dx+dQ\,dy+dR\,dz)]\\
&=\left\{P\left(\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}\right)+Q\left(\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}\right)+R\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)\right\}[dx\,dy\,dz]=0.
\end{align*}

{The form $[\theta(d\theta)^{p}]$ is, up to a  factor,  the monomial}\[
[dX^{1}dX^{2}\dots dX^{p}dY_{1}dY_{2}\dots dY_{p}dZ].
\]


\vspace{12pt}\fsec \emph{First order partial differential equations.} We can relate to the Pfaffian problem the problem of integrating a first order partial differential equation
\begin{equation}
  \label{eq:3.16}
  F\left(x^{1},x^{2},\dots,x^{n},z,\frac{\pd z}{\pd x^{1}},\frac{\pd z}{\pd x^{2}},\dots,\frac{\pd z}{\pd x^{n}}\right)=0.
\end{equation}

Integrating this equation, in the sense of problem of S.~Lie, is in effect  searching for the integral manifolds of $n$ dimensions of the differential system
\begin{equation}
  \label{eq:3.17}
  \left\{
    \begin{aligned}
      F(x^{1},x^{2},\dots,x^{n},z,p_{1},p_{2},\dots,p_{n})&=0,\\
      dz-p_{1}dx^{1}-p_{2}dx^{2}-\dots-p_{n}dx^{n}&=0,
    \end{aligned}
  \right.
\end{equation}
where we regard $x^{1},x^{2},\dots,x^{n},z,p_{1},p_{2},\dots,p_{n}$ as $2n+1$ independent variables [coordinates of a \emph{contact element} constructed by a point $(x^{i},z)$ of a $n+1$ dimensional space and a hypersurface passing through the point].

The system \eqref{eq:3.17} is closed by adjoining the equations
\begin{equation}
  \label{eq:3.18}
  \left\{
    \begin{aligned}
      \left(\frac{\pd F}{\pd x^{i}}+p_{i}\frac{\pd F}{\pd z}\right) dx^{i}+\frac{\pd F}{\pd p_{i}}dp_{i}&=0,\\
      [dx^{i}dp_{i}]&=0.
    \end{aligned}
  \right.
\end{equation}

The second equation of \eqref{eq:3.17}, where we suppose that the $2n+1$ variables $x_{i},z,p^{i}$ are related by the relation $F=0$, is a Pfaffian equation, and we can practically regard it as of $2n$ variables and the class is then at most to $2n-1$. We are going to verify that it is equal to $2n-1$ by forming the characteristic system of \eqref{eq:3.17}.

The characteristic system is formed by the equations \eqref{eq:3.17}, the first equation of \eqref{eq:3.18} and the equations
\[
u^{i}dp_{i}-v_{i}dx^{i}=0,
\]
where we suppose the coefficients $u^{i}$, $v_{i}$ are related by the relation
\[
\left(\frac{\pd F}{\pd x^{i}}+p_{i}\frac{\pd F}{\pd z}\right)u^{i}+\frac{\pd F}{\pd p_{i}}v_{i}=0,
\]
which gives immediately
\begin{equation}
  \label{eq:3.19}
  \frac{dx^{1}}{\frac{\pd F}{\pd p_{1}}}=
  \frac{dx^{2}}{\frac{\pd F}{\pd p_{2}}}=
  \dots=
  \frac{dx^{n}}{\frac{\pd F}{\pd p_{n}}}=
  \frac{-dp_{1}}{\frac{\pd F}{\pd x^{1}}+p_{1}\frac{\pd F}{\pd z}}=
  \dots=
  \frac{-dp_{n}}{\frac{\pd F}{\pd x^{n}}+p_{n}\frac{\pd F}{\pd z}}=
  \frac{dz}{p_{i}\frac{\pd F}{\pd p_{i}}}.
\end{equation}

Note moreover that the second equation of \eqref{eq:3.17} and the first equation of \eqref{eq:3.18} are consequences of the equations \eqref{eq:3.19}.

\vspace{12pt}\fsec In $2n+1$ dimensional space, or rather in the $2n$ dimensional manifold defined by the equation $F=0$, the \emph{characteristics} are the lines (characteristic bands) obtained by integrating the equations \eqref{eq:3.19}. To integrate  these equations is to put the Pfaffian equation
\[
dz-p_{i}dx^{i}=0,
\]
where the variables $x^{i},p_{i},z$ are related by the relation $F=0$,  into its canonical form
\begin{equation}
  \label{eq:3.20}
  dZ-Y_{1}dX^{1}-\dots-Y_{n-1}dX^{n-1}=0.
\end{equation}

Once the reduction is done, we can no longer search the $n-1$ dimensional solutions of equation \eqref{eq:3.20} in the $2n-1$ dimensional space of $X^{i},Y_{i}$ and $Z$.

Instead, we obtain them in the following way.

First suppose that there are no relations between $X^{1},X^{2},\dots,X^{n-1}$. We then have
\begin{equation}
  \label{eq:3.21}
  Z=f(X^{1},X^{2},\dots,X^{n-1}),\quad P_{1}=\frac{\pd f}{\pd X^{1}},\quad P_{2}=\frac{\pd F}{\pd X^{2}},\ \dots\ P_{n-1}=\frac{\pd f}{\pd X^{n-1}}.
\end{equation}

Now suppose that there are $p<n-1$ relations between $X^{1},X^{2},\dots,X^{n-1}$ which we suppose can be solved for $X^{n-p},X^{n-p+1},\dots,X^{n-1}$, giving
\begin{equation}
  \label{eq:3.22}
  X^{n-i}=f^{i}(X^{1},X^{2},\dots,X^{n-p-1})\qquad (i=1,2,\dots,p);
\end{equation}
$Z$ being also a function of the $n-p-1$ quantities $X^{1},X^{2},\dots,X^{n-p-1}$:
\begin{equation}
  \label{eq:3.23}
  Z=f(X^{1},X^{2},\dots,X^{n-p-1}),
\end{equation}
and it then gives
\begin{equation}
  \label{eq:3.24}
  \frac{\pd f}{\pd X^{i}}-\left(Y_{i}+Y_{n-k}\frac{\pd f^{k}}{\pd X^{i}}\right)=0\qquad(i=1,2,\dots,n-p-1).
\end{equation}
The $n$ equations \eqref{eq:3.22}, \eqref{eq:3.23}, \eqref{eq:3.24} define a new class of solutions.

Finally, if the $n-1$ functions $X^{i}$ are constants, then the same is true for $Z$ and we have the solution
\begin{equation}
  \label{eq:3.25}
  X^{1}=a^{1},\qquad X^{2}=a^{2},\qquad\dots\qquad X^{n-1}=a^{n-1},\qquad Z=b.
\end{equation}

We observe that the equation \eqref{eq:3.20} does not admit integral manifolds of more than $n$ dimensions.

We can regard the solution \eqref{eq:3.25} as defining a \emph{complete integral} of the given equation of partial derivatives. We effectively obtain this integral by eliminating $p_{1},p_{2},\dots,p_{n}$ from the $n$ equations \eqref{eq:3.25} and the equation $F=0$. This complete integral being known, we can, by differentiating them, deduce the general integral \footnote{See, for the general theory of first order partial differential equations, \textsc{E.~Goursat}, \emph{Le\c{c}ons sur l'int\'egration des \'equations aux d\'eriv\'ees partielles du premier ordre}, second edition (Paris, J.~Hermann, 1921).}.

\chapter{Integral elements, characters, genre. Existence theorems}
\label{cha:integr-elem-char}

\section{Integral elements of a differential system}
\label{sec:integr-elem-diff}

\fsec In this chapter we will discuss certain existence theorems concerning  integral manifolds of closed exterior differential systems (we have seen that we can always reduce other systems to closed systems). In certain cases these existence theorems will solve the so-called \emph{Cauchy problem}, which will in turn clarify the statements of the theorems. From this chapter on we are obliged to assume, as we have already remarked, that the functions that enter the equations of the given systems are \emph{analytic}, whereas in the preceding chapters,  the existence of continuous partial derivatives up to low order ($1$ or $2$) is sufficient.

The theory that we are going to develop was first created by \textsc{E.~Cartan} in order to study linear differential equations which corresponds to closed differential systens containing second order equations at most. The theory has then been substantially extended by \textsc{E.~K\"ahler} to systems of any degree.




\vspace{12pt}\fsec The exterior differential systems that we will consider are of the form
\begin{equation}
  \label{eq:4.1}
  \left\{
    \begin{aligned}
      f_{\alpha}(x^{1},x^{2},\dots,x^{n})&=0,&(\alpha&=1,2,\dots,r_{0}),\\
      \theta_{\alpha}\equiv A_{\alpha i}dx^{i}&=0,&(\alpha&=1,2,\dots,r_{1}),\\
      \varphi_{\alpha}\equiv\frac{1}{2}A_{\alpha ij}[dx^{i}dx^{j}]&=0,&(\alpha&=1,2,\dots,r_{2}),\\
      \psi_{\alpha}\equiv\frac{1}{6}A_{\alpha ijk}[dx^{i}dx^{j}dx^{k}]&=0,&(\alpha&=1,2,\dots,r_{3}),\\
      &\hdots
    \end{aligned}
  \right.
\end{equation}

The system being closed, the linear equations $df_{\alpha}=0$ must appear among the equations \eqref{eq:4.1}, or rather the form $df_{\alpha}$ must belong the ring of the forms $\theta_{\alpha}$. Similarly, the exterior derivative $d\theta_{\alpha}$ must belong to the ring of the forms $\theta_{\alpha}$ and $\varphi_{\alpha}$ and so on.


\vspace{12pt}\fsec A key observation can be made about the equations $f_{\alpha}=0$ that appear in the system \eqref{eq:4.1}. They define an analytic manifold $\mathcal{V}$ of a certain dimension $\rho$ in the $n$ dimensional space. We suppose that, on a general point of $\mathcal{V}$, the rank of the matrix of the partial derivatives $\pd f_{\alpha}/\pd x^{i}$ is equal to $\rho$ (we remark in passing here that $\rho$ might be less than the number $r_{0}$ of equations $f_{\alpha}=0$. For example, certain algebraic manifolds of $n-\rho$ dimensions are defined by more than $\rho$ algebraic equations, if we do not want to miss any points of the manifold). The condition that we want to impose on the equations $f_{\alpha}=0$ may not be achieved, if for example we have the single equation
\[
f\equiv[(x^{1})^{2}+(x^{2})^{2}+\dots+(x^{n})^{2}-1]^{2}=0.
\]

In such cases the arguments we are going to pursue will fail.

If the preceding condition is satisfied and if we place a general point $(x^{i})_{0}$ in the manifold $\mathcal{V}$, the equations $df_{\alpha}=0$ appear among the equations $\theta_{\alpha}=0$, in which there are $\rho$ linearly independent equations, for example those that correspond to the lines defined by the matrix of $\pd f_{\alpha}/\pd x^{i}$ of the determinant for $x^{i}=(x^{i})_{0}$. Any manifold that satisfies these $\rho$ equations $df_{x}=0$ and that contain the point $(x^{i})_{0}$ of $\mathcal{V}$ is completely contained in $\mathcal{V}$, at least in a neighbourhood of the point.


\vspace{12pt}\fsec \emph{Integral plane elements} We call a $p$ dimensional plane element the set formed by a point $(x^{1},x^{2},\dots,x^{n})$ and a $p$-plane passing through the point. The point is said to be the origin of the element. A $p$ dimensional plane element of a given origin may be defined by a system of $n-p$ independent linear relations of $dx^{1},dx^{2},\dots,dx^{n}$, regarded as the coordinates in a Cartesian reference frame which has the point $(x^{i})$ as its origin. We can also define it by $p$ linearly independent vectors based on the point $(x^{i})$, or by its Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ (\textsection\textbf{12}).

A $p$ dimensional plane element is said to be \emph{integral} if it satisfies the following two conditions:

1. Its origin belongs to the manifold $\mathcal{V}$ (we can say the the point is an \emph{integral point});

2. The exterior forms which define the equations of the system are annihilated by the plane element considered.

It is clear that if a manifold is integral, each of its points is an integral point and their tangent plane elements are integral. The converse is obvious.

\emph{It is natural to investigate the integral elements as a preliminary study of integral manifolds.}

\vspace{12pt}\fsec \emph{Determining if a $p$ dimensional plane element is integral.} This is the problem that we have already solved in chapter 1 (\textsection\textbf{12}--\textbf{14}). We now briefly review the results obtained.

A linear element of origin $(x^{i})$ and of parameters $u^{i}$ is integral if its origin is an integral point and the linear element $(u^{i})$ annihilates the forms $\theta_{\alpha}$ of degree $1$ defining the system \eqref{eq:4.1}:
\[
A_{\alpha i}u^{i}=0.
\]

A plane element of two dimensions is integral if its origin is an integral point and its Pl\"ucker coordinates $u^{ij}$ annihilate the exterior quadratic forms $[\theta_{\alpha}dx^{1}],\dots,[\theta_{\alpha}dx^{n}]$ and $\varphi^{\alpha}$, or alternatively if the Pl\"ucker coordinates $u^{ij}$ annihilate all quadratic exterior forms in the ring of the system \eqref{eq:4.1} \footnote{Recall that the coordinates $u^{ij}$ annihilate an exterior quadratic form $[H_{ij}dx^{i}dx^{j}]$ if we have $H_{ij}u^{ij}=0$. The ring of system \eqref{eq:4.1} is the ring determined by the forms $\theta_{\alpha},\varphi_{\alpha},\psi_{\alpha},\dots$ which define the equations in \eqref{eq:4.1}.}.

More generally \emph{a $p$ dimensional plane element is integral if its origin is an integral point and its Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ annihilate all forms of degree $p$ which belong to the ring of the given system.}


\vspace{12pt}\fsec \emph{Regular integral point, linear ordinary integral element}. Let $(x^{i})$ be a generic point of $\mathcal{V}$. The linear integral elements having the point as the origin are defined by the condition that their parameters $u^{i}$ satisfy the equations
\begin{equation}
  \label{eq:4.2}
  A_{\alpha i}u^{i}=0\qquad(\alpha=1,2,\dots,r_{1}).
\end{equation}

Let $s_{0}$ be the number of these equations that are linearly independent, \emph{i.e.}, the rank of the matrix of $A_{\alpha i}$, when the point $(x^{i})$ is \emph{generic}. \emph{The point $(x^{i})$ is said to be regular if  the number of independent equations of \eqref{eq:4.2} at this point is not less than $s_{0}$. A linear integral element is said to be ordinary if its origin is a regular point of $\mathcal{V}$.}

The integer $s_{0}$ is the zeroth order \emph{character}  of the system \footnote{If we solve the equations $f_\alpha=0$ with respect to a certain number of variables $x^{i}$ so as to make the equations \eqref{eq:4.1} contain only $n-\rho$ variables and their differentials, the character $s_{0}$ is naturally also reduced as a consequence.}.

Every regular point is necessarily a generic point of the manifold $\mathcal{V}$, but the converse may not be true. As the condition for a point to fail to be regular results in additional equations for the coordinates of the point, every sufficiently small neighbourhood of a regular point in the manifold $\mathcal{V}$ contains only regular points. We can also say that if a point of $\mathcal{V}$ is not regular, in any neighbourhood of this point in $\mathcal{V}$ there are infinitely many regular points, since all non-regular points are limits of sets infinite sequences of regular points.

Observe also that the number of equations which define a linear integral element with a given origin can never exceed $s_{0}$. Finally, for any point of the space, \emph{integral or not}, sufficiently close to a regular point, the rank of equations \eqref{eq:4.2} is at least equal to $s_{0}$ and may be greater.

\vspace{12pt}\fsec \emph{Regular linear integral element, ordinary two-dimensional integral element.} Let $(E_{1})$ be an ordinary integral element of parameters $u^{i}$. To know more about the two dimensional integral elements that contain $(E_{1})$,  we  form the \emph{polar element} of $(E_{1})$: this is the locus of linear elements $(dx^{i}
)$ such that the plane element determined by $(u^{i})$ and $(dx^{i})$ are integral. The conditions which the parameters $dx^{i}$ have to satisfy are
\begin{align}
  \label{eq:4.3}
  A_{\alpha i}dx^{i}&=0&(\alpha&=1,2,\dots,r_{1}),\\
  \label{eq:4.4}
  A_{\alpha ij}u^{i}dx^{j}&=0&(\alpha&=1,2,\dots,r_{2}),
\end{align}
these equations constitute what we call the \emph{polar system} of $(E_{1})$.

Let $s_{0}+s_{1}$ be the rank of the polar system of a \emph{generic} ordinary integral element $(E_{1})$. This value signifies that the $r_{1}$ equations \eqref{eq:4.3}  and $r_{2}$ equations \eqref{eq:4.4} can be reduced to $s_{0}+s_{1}$ independent relations if we take into account that the $u^{i}$ satisfy the equations
\[
A_{\alpha i}u^{i}=0.
\]

\emph{The ordinary integral element $(E_{1})$ is regular if the rank of its polar system does not decrease from its normal value $s_{0}+s_{1}$. A two dimensional integral element  is  ordinary if it contains at least a regular linear integral element.}

The integer $s_{1}$ is  the first order \emph{character} of the given differential system.

Observe that if $s_{0}+s_{1}$ is greater or equal to $n-1$, the polar element of a regular integral element $(E_{1}) $ is reduced to the element itself. In this case there does not exist any ordinary integral element of two dimensions.

As in the previous section, we  remark that every sufficiently small neighbourhood of a regular linear integral element  in the manifold contains only regular elements.

\vspace{12pt}\fsec \emph{Generalisations.} Suppose $s_{0}+s_{1}<n-1$. Let $(E_{2})$ be a two dimensional ordinary integral element, defined for example by two linear integral elements $(u^{i})$ and $(v^{i})$. The \emph{polar system} of $(E_{2})$ is formed by the equations expressing that the linear integral element $(dx^{i})$ determines with $(E_{2})$ a three dimensional integral element. The equations of the polar system are
\begin{equation}
  \label{eq:4.5}
  \left\{
    \begin{aligned}
      A_{\alpha i}dx^{i}&=0&(\alpha&=1,2,\dots,r_{1}),\\
      A_{\alpha ij}u^{i}dx^{j}&=0,\quad A_{\alpha ij}v^{i}dx^{j}=0&(\alpha&=1,2,\dots,r_{2}),\\
      A_{\alpha ijk}u^{i}u^{j}dx^{k}&=0&(\alpha&=1,2,\dots,r_{3}),\\
    \end{aligned}
  \right.
\end{equation}

Let $s_{0}+s_{1}+s_{2}$ be the rank of this system for a \emph{generic} ordinary integral element.

The ordinary integral element $(E_{2})$ is  \emph{regular} if the rank of its polar system does not fall below $s_{0}+s_{1}+s_{2}$, and a three dimensional integral element is  \emph{ordinary} if it contain at least one two dimensional regular integral element.

The integer $s_{2}$ is the \emph{character} of order $2$ of the given differential system.

If $s_{0}+s_{1}+s_{2}$ is greater or equal to $n-2$, the polar element of $(E_{2})$ is two dimensional and there does not exist any ordinary three dimensional integral element.

These definitions can be generalised easily by proceeding step by step to higher dimensions.

Ultimately, a $p$ dimensional integral element $(E_{p})$ is ordinary if it contains at least a regular integral element $(E_{p-1})$, which in turn contains at least a regular integral element $(E_{p-2})$ and so on, until we reach the regular linear element $(E_{1})$ whose origin is a regular point.

There does  exists a $p$ dimensional ordinary integral element only if we have
\[
s_{0}+s_{1}+\dots+s_{p-1}<n-p+1.
\]

\vspace{12pt}\fsec \emph{Genre of a closed differential system}. As we increase the dimension, we will eventually come to a certain dimension, say $h+1$, such that there does not exist any ordinary integral element. The integer $h$ is  the genre of the differential system. It is the smallest integer such that
\[
s_{0}+s_{1}+\dots+s_{h}=n-h.
\]

There exists $h$ dimensional regular integral elements, but there does not exist any $h+1$ dimensional ordinary integral element.

The integers $s_{0},s_{1},s_{2},\dots,s_{h}$ are the characters of the differential system.

Recall once more that in the manifold of integral elements of $p\le h$ dimensions, any sufficiently small neighbourhood of a regular integral element contains only regular integral elements. An integral element $(E_{p})$ may be defined analytically by the coordinates $x^{1},x^{2},\dots,x^{n}$ of its origin and its Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ subject to the condition that they satisfy a system of quadratic relations that we have derived in chapter 1. A neighbourhood of an element $(E_{p})$ may be defined by the condition that the coordinates $x^{i},u^{i_{1}i_{2}\dots i_{p}}$ of any element $(E_{p})$ in this neighbourhood does not deviate too much from the coordinates of the element $(E_{p})$.

\section{Two existence theorems}
\label{sec:two-theor-exist}

\fsec Given a differential system $\Sigma$ of genre $h$, we will prove the existence of integral manifold of dimensions $p\le h$. This does not exclude the possibility of existence of integral manifolds of more than $h$ dimensions, and nor does it mean that the $p$ dimensional manifolds whose existence  we will prove exhaust all integral manifolds of $p$ dimensions. We will prove the theorem using the Cauchy-Kowalewski theorem, which we will state later.


\vspace{12pt}\fsec Consider now a closed differential system $\Sigma$ of genre $h$ and let $p\le h$. We have
\[
s_{0}+s_{1}+\dots+s_{p-1}\le n-p.
\]
Consider a $p$ dimensional integral element $(E_{p})_{0}$ which is \emph{ordinary}. We can suppose that its equations involve no linear relations in $dx^{1},dx^{2},\dots,dx^{p}$. Let us now change  notation and denote by $z^{\lambda}$ $(\lambda=1,2,\dots,n-p=\nu)$ the variables $x^{p+1},\dots,x^{n}$. The element $(E_{p})_{0}$ is defined by the equations of the form
\begin{equation}
  \label{eq:4.6}
  dz^{\lambda}=a_{i}{}^{\lambda}dx^{i}\qquad(\lambda=1,2,\dots,\nu),
\end{equation}
the index of summation $i$ varies from $1$ to $p$.

There exists a chain of regular integral elements $(E_{p-1})_{0},(E_{p-2})_{0},\dots,(E_{1})_{0}$ in which each element is contained in the previous element and in $(E_{p})_{0}$, and their origin is a regular point of $\mathcal{V}$. To simplify the exposition, we suppose that the first coordinates $x^{i}$ of the point is zero, and the other are $z^{\lambda}=a^{\lambda}$. Finally we can suppose, by means of a linear transformation with constant coefficient on the $p$ coordinates $x^{i}$, that the equations of the linear elements $(E_{p-1})_{0},(E_{p-2})_{0},\dots$ are obtained by successively adjoining to the equations \eqref{eq:4.5} the equations
\[
dx^{p}=0,\qquad dx^{p-1}=0,\qquad\dots\qquad dx^{2}=0.
\]
The parameters of $(E_{1})_{0}$ are then
\[
1,\quad 0\quad\dots\quad 0,\quad a^{1}_{1},\quad a^{2}_{1},\quad\dots\quad a^{\nu}_{1}.
\]

Such conventions being made, for every $p$ dimensional integral manifold $V_{p}$ tangent to the element $(E_{p})_{0}$, its tangent plane elements in a neighbourhood of the origin $(x^{i}=0,z^{\lambda}=a^{\lambda})$ of $(E_{p})_{0}$ will be ordinary integral elements. The integral manifold can be defined by $\nu$ equations
\begin{equation}
  \label{eq:4.7}
  z^{\lambda}=\varphi^{\lambda}(x^{1},x^{2},\dots,x^{p})\qquad (\lambda=1,2,\dots,\nu),
\end{equation}
the $\varphi^{\lambda}$ being holomorphic functions of $x^{i}$ in a neighbourhood of the values $x^{i}=0$, and taking on $x^{i}=0$ the values $z^{\lambda}=a^{\lambda}$.

We are going to prove the following theorem (generalised Cauchy theorem), the significance of which will be explained later.
\begin{thm*}
  There exists at least one analytic integral manifold that is tangent to a $p$ dimensional ordinary integral element $(E_{p})_{0}$, such that $(E_{p})_{0}$ contains a $p-1$ dimensional integral manifold $V_{p-1}$ tangent to the $(p-1)$ dimensional  regular integral element $(E_{p-1})_{0}$.
\end{thm*}


\vspace{12pt}\fsec \emph{The Cauchy-Kowalewski theorem.} For the proof, it suffices to make use of a classic theorem, which we state in the following form \footnote{For the general formulation, see \textsc{E.~Goursat}, \emph{Cours d'Analyse math\'ematique}, 2nd edition, volume 2, pp.~652--637 (Paris, Gauthier-Villars, 1911).}.

\emph{Given a system of $q$ first order partial differential equations of $q$ unknown functions $z^{\lambda}$ in $p$ independent variables $x^{i}$, whose left hand sides are $\pd z^{\lambda}/\pd x^{p}$ and whose right hand sides are  functions of $x^{i},z^{\lambda},\pd z^{\lambda}/\pd x^{1},\dots,\pd z^{\lambda}/\pd x^{p-1}$, holomorphic in a neighbourhood of the values $x^{i}=0,z^{\lambda}=a^{\lambda},\pd z^{\lambda}/\pd x^{i}=a^{\lambda}_{i}$, then the system admits one and only one analytic solution where the unknown functions are holomorphic functions of $x^{1},x^{2},\dots,x^{p}$ in a neighbourhood of $x^{i}=0$, which reduces on $x^{p}=0$ to some given holomorphic functions $z^{\lambda}=\chi^{\lambda}(x^{1},x^{2},\dots,x^{p-1})$, which in turn take on $x^{1}=x^{2}=\dots=x^{p-1}=0$ the values $a^{\lambda}$ and whose derivatives $\pd z^{\lambda}/\pd x^{i}$ take the values $a^{\lambda}_{i}$.}


\vspace{12pt}\fsec We now prove the following theorem,  a particular case of the theorem stated in \textsection\textbf{61}.

\newtheorem*{thmexs1}{\hspace{15pt}First existence theorem}
\begin{thmexs1}
  Consider a closed differential system which satisfies
\[
s_{0}+s_{1}+\dots+s_{p-1}=n-p.
\]
Let $(E_{p})_{0}$ be a $p$ dimensional ordinary integral element and $V_{p-1}$ a $p-1$ dimensional integral manifold tangent to the regular integral plane element $(E_{p-1})_{0}$ contained in $(E_{p})_{0}$. Then there exists one and only one $p$ dimensional integral manifold containing $V_{p-1}$ and this manifold is tangent to the element $(E_{p})_{0}$.
\end{thmexs1}

We can immediately see that the last part of the statement is evident, since on the regular integral element $(E_{p-1})_{0}$, due to the value $n-p$ of the sum $s_{0}+s_{1}+\dots+s_{p-1}$, there passes only one $p$ dimensional integral element, which is therefore $(E_{p})_{0}$.

We will do our proof for the case $p=3$, which suffices to give the idea of the proof in the general case.


\vspace{12pt}\fsec \emph{Preliminary remarks to the proof.} We suppose, as we did in \textsection\textbf{61}, that the element $(E_{3})_{0}$ has its origin at the coordinates $x^{i}=0,z^{\lambda}=a^{\lambda}$, $(\lambda=1,2,\dots,n-3=\nu)$ and it is defined by the equations
\begin{equation}
  \label{eq:4.6}
  dz^{\lambda}=a^{\lambda}_{a}dx^{1}+a^{\lambda}_{2}dx^{2}+a^{\lambda}_{3}dx^{3}\qquad (\lambda=1,2,\dots,\nu).
\end{equation}

The regular integral element $(E_{2})_{0}$ is obtained by adjoining to the equations \eqref{eq:4.6} the equation $dx^{3}=0$, and the regular element $(E_{1})_{0}$ is obtained by adjoining further the equation $dx^{2}=0$.

Let
\[
x^{3}=0,\qquad z^{\lambda}=\Phi^{\lambda}(x^{1},x^{2})\qquad(\lambda=1,2,\dots,\nu)
\]
be the equations of the two dimensional integral manifold $V_{2}$. For $x^{1}=x^{2}=0$, the functions $\Phi^{\lambda}$ and their partial derivatives $\pd \Phi^{\lambda}/\pd x^{1}$, $\pd \Phi^{\lambda}/\pd x^{2}$ take respectively the values $a^{\lambda},a_{1}{}^{\lambda},a_{2}{}^{\lambda}$.

Now let
\[
z^{\lambda}=F^{\lambda}(x^{1},x^{2},x^{3})
\]
be the equations of the unknown three dimensional manifold $V_{3}$. For $x^{3}=0$, the functions $F^{\lambda}$ reduce to the given functions $\Phi^{\lambda}$, and for $x^{1}=x^{2}=x^{3}=0$, we must have $\pd F^{\lambda}/\pd x^{3}=a_{3}{}^{\lambda}$.

The equations that the functions $F^{\lambda}$ must satisfy are, according to the equations \eqref{eq:4.1} of the system $\Sigma$,
\begin{equation}
  \label{eq:4.8}
  \left\{
    \begin{aligned}
      f_{\alpha}(x,z)&=0\qquad(\alpha=1,2,\dots,r_{0}),\\
      H_{\alpha i}&\equiv A_{\alpha i}+A_{\alpha \lambda}\frac{\pd z^{\lambda}}{\pd x^{i}}=0\qquad(i=1,2,3;\alpha=1,2,\dots,r_{1}),\\
      H_{\alpha ij}&\equiv A_{\alpha ij}+A_{\alpha i\lambda}\frac{\pd z^{\lambda}}{\pd x^{j}}-A_{\alpha j\lambda}\frac{\pd z^{\lambda}}{\pd x^{i}}+A_{\alpha \lambda \mu}\frac{\pd z^{\lambda}}{\pd x^{i}}\frac{\pd z^{\mu}}{\pd x^{j}}=0\\
      &\qquad\qquad\qquad\qquad\qquad\qquad\qquad(i,j=1,2,3;\alpha=1,2,\dots,r_{2}),\\
H_{\alpha 123}&\equiv A_{\alpha 123}+A_{\alpha ij\lambda}\frac{\pd z^{\lambda}}{\pd x^{k}}+A_{\alpha i \lambda\mu}\frac{\pd z^{\lambda}}{\pd x^{j}}\frac{\pd z^{\mu}}{\pd x^{k}}+A_{\alpha\lambda\mu\nu}\frac{\pd z^{\lambda}}{\pd x^{1}}\frac{\pd z^{\mu}}{\pd x^{2}}\frac{\pd z^{\nu}}{\pd x^{3}}=0.
    \end{aligned}
  \right.
\end{equation}

In the expression of $H_{\alpha 123}$, the indices $i,j,k$  appearing in the second and third terms are respectively the three even permutations $123,231,312$ of the indices $1,2,3$.

In a neighbourhood of the origin $M_{0}$ of $(E_{3})_{0}$ we  only need to enforce $\rho$ of the equations $f_{\alpha}(x,z)=0$ so that all of the rest are automatically satisfied, where the matrix of the coefficients of $dz^{\lambda}$ in  $\rho$ of the differentials $df_{\alpha}$ are of rank $\rho$. We suppose that this holds for the first $\rho$ columns.

The equations \eqref{eq:4.8} can be divided into three groups
\begin{align}
\label{eq:4.a}&f_{\alpha}(x,z)=0&(\alpha=1,2,\dots,\rho);\tag{A}\\
\label{eq:4.b}&H_{\alpha1}=0,\quad H_{\alpha 2}=0,\quad H_{\alpha 12}=0;\tag{B}\\
\label{eq:4.c}&H_{\alpha 3}=0,\quad H_{\alpha 13}=0,\quad H_{\alpha23}=0,\quad H_{\alpha 123}=0.\tag{C}
\end{align}

The manifold $V_{2}$ satisfies the equations \eqref{eq:4.a} and \eqref{eq:4.b}. The manifold $V_{3}$ in addition satisfies the equations \eqref{eq:4.c}.

\vspace{12pt}\fsec If among the coefficients of the equations \eqref{eq:4.c} we give the arguments $x$, $z^{\lambda}$, $\pd z^{\lambda}/\pd x^{1}$, $\pd z^{\lambda}/\pd x^{2}$ respectively the values $0,a^{\lambda},a^{\lambda}_{1},a^{\lambda}_{2}$, we obtain a system of linear equations in $\pd z^{\lambda}/\pd x^{3}$, and it is easy to see that the system \eqref{eq:4.c} becomes the polar system of $(E_{2})_{0}$ by replacing $dx^{1}$ and $dx^{2}$ by $0$, replacing $dx^{3}$ by $1$, and replacing $dz^{\lambda}$ by $\pd z^{\lambda}/\pd x^{3}$. The equations of this polar system contain by hypothesis $s_{0}+s_{1}+s_{2}$ independent relations in $dx^{i}$ and $dz^{\lambda}$, but these equations cannot contain any linear equations between $dx^{1}, dx^{2}, dx^{3}$, otherwise these relations will appear among those that define the element $(E_{3})_{0}$, which we assumed is not the case. The equations \eqref{eq:4.c}, regarded as linear equations in $\pd z^{\lambda}/\pd x^{3}$, therefore contain $s_{0}+s_{1}+s_{2}$ independent relations when, among the coefficients of the equations, we replace respectively $x^{i},z^{\lambda}, \pd z^{\lambda}/\pd x^{i}, \pd z^{\lambda}/\pd x^{2}$ by $0,a^{\lambda},a_{1}^{\lambda},a_{2}^{\lambda}$. Among these equations, we will make a choice of $s_{0}+s_{1}+s_{2}$ independent equations, which will be called \emph{principal}.

If in the principal equations, we give the arguments $x^{i}, z^{\lambda}, \pd z^{\lambda}/\pd x^{1}, \pd z^{\lambda}/\pd x^{2}$ the values sufficiently close to $0, a^{\lambda}, a_{1}^{\lambda}, a_{2}^{\lambda}$, they will not cease to be linearly independent. There are three possible cases:
\vspace{12pt}

1. \emph{The values given to the arguments define a two dimensional integral element.} The element is necessarily regular if the values of the arguments do not differ too much from the values $0, a^{\lambda}, a_{1}^{\lambda}, a_{2}^{\lambda}$. In this case the non-principal equations of \eqref{eq:4.c} are consequences of the principal equations of \eqref{eq:4.c}.
\vspace{12pt}

2. \emph{The values given to the arguments $x^{i}, z^{\lambda}$ define an integral point} but the values given to the other arguments do not define a two dimensional integral element. The non-principal equations of \eqref{eq:4.c} are  consequences of the principal equations of \eqref{eq:4.c} and the equations \eqref{eq:4.b}. More precisely, the non-principal equations of $H_{\alpha 3}=0$ depending on neither $\pd z^\lambda/\pd x^{1}$ nor $\pd z^{\lambda}/\pd x^{2}$ are consequences of the principal equations of $H_{\alpha 3}=0$. The non-principal equations of $H_{\alpha 13}$ depending on neither $\pd z^\lambda/\pd x^{1}$ nor $\pd z^{\lambda}/\pd x^{2}$ are consequences of the principal equations of $H_{\alpha 13}=0$ and $H_{\alpha 3}=0$, and also the equations $H_{\alpha 1}=0$ that express that $\pd z^{\lambda}/\pd x^{1}$ define a linear integral element. Finally the non-principal equations of $H_{\alpha 23}=0, H_{\alpha 123}=0$ are consequences of the principal equations of \eqref{eq:4.c} and the equations \eqref{eq:4.b}. These results may be expressed in the following formulae, where $\alpha'$ is the non-principal index of equation \eqref{eq:4.c}, and $\alpha$ is the principal index of equation \eqref{eq:4.c} or any index of the equation \eqref{eq:4.b}:
\begin{equation}
  \label{eq:4.9}
  \left\{
    \begin{aligned}
      H_{\alpha'3}&=\{H_{\alpha 3}\},\\
      H_{\alpha'13}&=\{H_{\alpha 3}, H_{\alpha 13}, H_{\alpha 1}\},\\
      H_{\alpha'23}&=\{H_{\alpha 3}, H_{\alpha 13}, H_{\alpha 23}, H_{\alpha 123}, H_{\alpha 1}, H_{\alpha 2}, H_{\alpha 12}\},\\
      H_{\alpha'123}&=\{H_{\alpha 3}, H_{\alpha 13}, H_{\alpha 23}, H_{\alpha 123}, H_{\alpha 1}, H_{\alpha 2}, H_{\alpha 12}\},
    \end{aligned}
  \right.
\end{equation}
the curly braces represent  linear combinations of the expressions it encloses, the coefficients of which are holomorphic functions of $x^{i}, z^{\lambda}, \pd z^{\lambda}/\pd x^{1}, \pd z^{\lambda}/\pd x^{2}$ in a neighbourhood of the values $0, a^{\lambda}, a^{\lambda}_{1}, a^{\lambda}_{2}$ of the argument.
\vspace{12pt}

3. \emph{The values given to the arguments $x^{i}, z^{\lambda}$ does not define an integral point}. The non-principal equations \eqref{eq:4.c} are then consequences of principal equations of \eqref{eq:4.c}, the equations \eqref{eq:4.b} and the equations \eqref{eq:4.a}.
\vspace{12pt}

We will have two further remarks. The first is that we can suppose that the equations $df_{\alpha}=0$ appear among the equations $\theta_{\alpha}=0$, and the equations $d\theta_{\alpha}=0$ appear among the equations $\varphi_{\alpha}=0$ and the equations $d\varphi_{\alpha}=0$ appear among the equations $\psi_{\alpha}=0$. The second is that we can suppose that, among the principal equations \eqref{eq:4.c} of the form $H_{\alpha 3}=0$ we have the first $\rho$  equations coming from $df_{\alpha}=0$, that is to say,
\begin{equation}
  \label{eq:4.10}
  \frac{\pd f_{\alpha}}{\pd x^{3}}+\frac{\pd f_{\alpha}}{\pd z^{\lambda}}\frac{\pd z^{\lambda}}{\pd x^{3}}=0\qquad (\alpha=1,2,\dots,\rho).
\end{equation}


\vspace{12pt}\fsec \emph{Proof of the first existence theorem}. Consider the $s_{0}+s_{1}+s_{2}$ principal equations of \eqref{eq:4.c}. As the number of unknown functions $z^{\lambda}$ is $s_{0}+s_{1}+s_{2}=n-3$, these give the derivatives $\pd z^{\lambda}/\pd x^{3}$ in terms of the other derivatives $\pd z^{\lambda}/\pd x^{1}$, $\pd z^{\lambda}/\pd x^{2}$. They constitute a Cauchy-Kowalewski system. Hence they admit one and only one holomorphic solution
\begin{equation}
  \label{eq:4.11'}
  z^{\lambda}=F^{\lambda}(x^{1},x^{2},x^{3})
\end{equation}
where $F^{\lambda}$ reduces to the given functions $\Phi^{\lambda}(x^{1},x^{2})$ when we set $x^{3}=0$. We are now going to prove that the manifold $V_{3}$ defined by the equations \eqref{eq:4.11'} is integrable.

\emph{Firstly}, among the equations of the Cauchy-Kowalewski system there are $\rho$ equations \eqref{eq:4.10}, which express the fact that on the manifold $V_{3}$ the functions $f_{\alpha}(x,z)$ are independent of $x^{3}$, or, for $x^{3}=0$, they vanish since the manifold $V_{2}$ is integrable. Hence these equations vanish identically. \emph{The manifold $V_{3}$ therefore satisfies the equations \eqref{eq:4.a}}.

\emph{Secondly}, the manifold $V_{3}$ satisfies the equations \eqref{eq:4.a}, all the points of which are integral points and hence, according to \textsection\textbf{65}.2, all the expressions $H_{\alpha 3}$, even the non-principal ones, are identically zero on the manifold $V_{3}$. As for the expressions $H_{\alpha 13}, H_{\alpha 23}, H_{\alpha 123}$, the principal ones are zero according to the same conditions, and the non-principal ones satisfy according to \eqref{eq:4.9} equations of the form
\begin{equation}
  \label{eq:4.11}
  \left\{
    \begin{aligned}
      H_{\alpha'13}&=\{H_{\alpha 1}\},\\
      H_{\alpha'23}&=\{H_{\alpha 1},H_{\alpha 2},H_{\alpha 12}\},\\
      H_{\alpha'123}&=\{H_{\alpha 1},H_{\alpha 2},H_{\alpha 12}\}.
    \end{aligned}
  \right.
\end{equation}
\emph{It therefore suffices that the manifold $V_{3}$ satisfies the equations \eqref{eq:4.b} for it to be integrable}.

\emph{Thirdly}, the quantities $H_{\alpha 1}, H_{\alpha 2}, H_{\alpha 12}$ vanish for $x^{3}=0$. Take a form $\theta^{\alpha}$, on the manifold $V_{3}$ we have
\[
\theta_{\alpha}=H_{\alpha 1}dx^{1}+H_{\alpha 2}dx^{2},
\]
from which
\begin{equation}
  \label{eq:4.12}
  d\theta_{\alpha}=-\frac{\pd H_{\alpha 1}}{\pd x^{3}}[dx^{1}dx^{3}]-\frac{\pd H_{\alpha 2}}{\pd x^{3}}[dx^{2}dx^{3}]+\left[\frac{\pd H_{\alpha 2}}{\pd x^{1}}-\frac{\pd H_{\alpha 1}}{\pd x^{2}}\right][dx^{1}dx^{2}].
\end{equation}

As the equation $d\theta_{\alpha}=0$ is part of the equations $\varphi_{\alpha}=0$, the coefficient $\pd H_{\alpha 1}/\pd x^{3}$ is a linear combination with holomorphic coefficients of expressions $H_{\alpha 13}$ and hence of the expressions $H_{\alpha 1}$ according to \eqref{eq:4.11}. The $r_{1}$ quantities $H_{\alpha 1}$ therefore satisfy a system of linear differential equations with holomorphic coefficients, with independent variable is $x^{3}$. As the functions $H_{\alpha 1}$ vanish for $x^{3}=0$, they vanish identically. \emph{The manifold $V_{3}$ therefore satisfies the equations $H_{\alpha 1}=0$ and hence the equations $H_{\alpha 13}=0$.}

\emph{Fourthly}, the expression $\pd H_{\alpha 2}/\pd x^{3}$ is, according to \eqref{eq:4.12}, a linear combination with holomorphic coefficients of $H_{\alpha 23}$, that is to say of $H_{\alpha 2}$ and $H_{\alpha 12}$ according to \eqref{eq:4.11}.

On the other hand, as $\varphi_{\alpha}=H_{\alpha 12}[dx^{1}dx^{2}]+H_{\alpha 23}[dx^{2}dx^{3}]$, we have
\[
d\varphi_{\alpha}=\left(\frac{\pd H_{\alpha 12}}{\pd x^{3}}+\frac{\pd H_{\alpha 23}}{\pd x^{1}}\right)[dx^{1}dx^{2}dx^{3}],
\]
and as $d\varphi_{\alpha}$ is a linear combination of $\psi_{\alpha}$, the expression $\pd H_{\alpha 12}/\pd x^{3}+\pd H_{\alpha 23}/\pd x^{1}$ is a linear combination of $H_{\alpha 123}$, that is to say, according to \eqref{eq:4.11}, of $H_{\alpha 2}$ and $H_{\alpha 12}$. Finally, as $\pd H_{\alpha 23}/\pd x^{1}$ is a linear combination of $H_{\alpha 2}$, $H_{\alpha 12}$, $\pd H_{\alpha 2}/\pd x^{1}$, $\pd H_{\alpha 12}/\pd x^{1}$ according to \eqref{eq:4.11}, we see that the $\pd H_{\alpha 12}/\pd x^{3}$ are linear combinations with holomorphic coefficients of the functions $H_{\alpha 2}$ and $H_{\alpha 12}$ and of their partial derivatives with respect to $x^{1}$. They therefore satisfy a Cauchy-Kowalewski system and, as they vanish for $x^{3}=0$, they are identically zero, and hence according to \eqref{eq:4.11}, $H_{\alpha 23}$ and $H_{\alpha 123}$ are zero as well.

The manifold $V_{3}$ therefore satisfies all the equations \eqref{eq:4.a}, \eqref{eq:4.b} and \eqref{eq:4.c}.\qed


\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape1}
\newtheorem*{thm67}{\hspace{15pt}\textbf{67.} Second existence theorem}
\begin{thm67}
  Given a closed differential system $\Sigma$ for which we have
\[
s_{0}+s_{1}+\dots+s_{p-1}<n-p,
\]
and let $(E_{p})_{0}$ be a $n$ dimensional integral element and $V_{p-1}$ a $p-1$ dimensional integral manifold tangent to the regular integral element $(E_{p-1})$ contained in $(E_{p})_{0}$. Then there exists infinitely many integral manifolds of $p$ dimensions containing $V_{p-1}$ and tangent to the element $(E_{p})_{0}$. Each of these integral manifold is determined uniquely if we choose arbitrarily $n-p-s_{0}-s_{1}-\dots-s_{p-1}$ unknown functions such that they reduce to the functions $\Phi^{\lambda}(x^{1},x^{2})$ for $x^{3}=0$.
\end{thm67}

The proof is easy and can be reduced to that of the first theorem. Indeed, let us take the hypothesis $p=3$ and consider the principal equations of \eqref{eq:4.c}. They are solvable with respect to the $s_{0}+s_{1}+s_{2}$ derivatives $\pd z^{\lambda}/\pd x^{3}$. On the right hand side there are $n-3-s_{0}-s_{1}-s_{2}$ derivatives $\pd z^{\lambda}/\pd x^{3}$. Let us assign to the $n-3-s_{0}-s_{1}-s_{2}$ functions $z^{\lambda}$ the values corresponding to holomorphic functions of $x^{1},x^{2},x^{3}$ in a neighbourhood of $x^{i}=0$ subject to the  condition that they reduce to the functions $\Phi^{\lambda}(x^{1},x^{2})=0$ of the same index for $x^{3}=0$. We then have a Cauchy-Kowalewski system with unique solution corresponding the the given initial conditions $z^{\lambda}=\Phi^{\lambda}(x^{1},x^{2})$.

\vspace{12pt}\fsec The integral manifolds whose existence has been established by the two existence theorems are the \emph{ordinary} integral manifolds. The set of ordinary integral manifolds constitute  the so-called \emph{general solution} of the given differential system. The integral manifolds that are not ordinary do not have ordinary tangent integral elements at any point. The \emph{regular integral manifolds} are those that admit regular tangent integral elements.

We can calculate the \emph{degree of freedom of  ordinary integral manifolds $V_{p}$} admitting a given $p$ dimensional ordinary tangent integral element.

Continuing to use the preceding notations, the section $V_{1}$ of $V_{p}$ of the flat manifold $x^{p}=x^{p-1}=\dots=x^{2}=0$ depend on $n-p-s_{0}$ arbitrary functions of $x^{1}$ subject only to the condition that for $x^{1}=0$ their derivatives take the given values $a_{1}^{\lambda}$. The integral manifold $V_{1}$ being chosen, the section of $V_{p}$ of the plane manifold $x^{p}=x^{p-1}=\dots=x^{3}=0$ depend on $n-p-s_{0}-s_{1}$ arbitrary functions of $x^{1}$, $x^{2}$, subject only to the condition that it reduces for $x^{2}=0$ to the known functions of $x^{1}$. And so on. The manifold $V_{p}$ depend on $n-p-s_{0}-s_{1}-\dots-s_{p-1}$ arbitrary functions of $x^{1},x^{2},\dots,x^{p}$, subject only to the condition that it reduces for $x^{p}=0$ to the given functions of $x^{1},x^{2},\dots,x^{p-1}$.

Introduce, for symmetry reason, an integer $\sigma_{p}$ (which is not a character) by the relation
\[
s_{0}+s_{1}+\dots+s_{p-1}+\sigma_{p}=n-p.
\]

We can say that, \emph{roughly}, the $p$ dimensional ordinary integral manifold tangent to $(E_{p})_{0}$ depend on
\[\begin{array}{rcl}
  s_{1}+s_{2}+\dots+s_{p-1}+\sigma_{p}&\text{arbitrary functions of}& x^{1},\\
  s_{2}+\dots+s_{p-1}+\sigma_{p}&\quad\prime\prime\qquad\qquad\prime\prime\qquad\prime \prime & x^{1},x^{2},\\
  \dots&\dots&\dots\\
  s_{p-1}+\sigma_{p}&\quad\prime\prime\qquad\qquad\prime\prime\qquad\prime \prime& x^{1},x^{2},\dots,x^{p-1},\\
  \sigma_{p}&\quad\prime\prime\qquad\qquad\prime\prime\qquad\prime \prime& x^{1},x^{2},\dots,x^{p-1},x^{p}.\\
\end{array}\]

\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape0}
\newtheorem*{rmk69}{\hspace{15pt}\textbf{69.} Remark}
\begin{rmk69}
  It is inappropriate to given an interpretation too absolute to the above statement, which only specifies the number of arbitrary functions we can give to arrive at the most general $p$ dimensional integral manifold by successive applications of the Cauchy-Kowalewski theorem. In fact, the only one of these integers that have an absolute meaning is the number of arbitrary functions of maximal numbers of variables ($\sigma_{p}$ if $\sigma_{p}\neq 0$, $s_{p-1}$ if $\sigma_{p}=0, s_{p-1}\neq 0$, \emph{etc}.). Without wanting to justify this assertion, which  makes sense  only for \emph{analytic} differential systems and their \emph{analytic} integral manifolds, let us content ourselves by studying a simple example to see why  we should proceed cautiously in such cases.

Consider the equation
\[
\frac{\pd^{2}z}{\pd y^{2}}=\frac{\pd z}{\pd x}.
\]
From our point of view, the general solution of this equation depend on two arbitrary functions of one variable, for example the holomorphic functions of $x$ which reduces to $z$ and $\pd z/\pd y$ for $x=0$. But we could also be tempted to say that the general solution depend on only one arbitrary function, namely the function $\Phi(y)$ which reduces to $z$ for $x=0$. Unfortunately, for example, if we write $\Phi(y)$ as a holomorphic function of $y$ in a neighbourhood of $y=0$, the equation may not admit \emph{any} holomorphic solutions in a neighbourhood of the point $x=0, y=0$: it suffices to take $\Phi(y)=1/(1-y)$, which makes $z$ into a series
\[
z=\frac{1}{1-y}+\frac{2}{1}\frac{x}{(1-y)^{3}}+\frac{4!}{2!}\frac{x^{2}}{(1-y)^{5}}+\dots+\frac{(2n)!}{n!}\frac{x^{n}}{(1-y)^{2n+1}}+\cdots
\]
And this entire series in $x$ converges only for $x=0$. Such negative results occur when the function $\Phi(y)$ is not entire, as well as in certain cases where the function $\Phi(y)$ is entire.
\end{rmk69}



\vspace{12pt}\fsec \emph{Degree of freedom of ordinary integral elements $E_{p}$ having a given regular integral point as the origin}. If we make the same assumption as above for the regular integral elements $(E_{p-1}),\dots,(E_{1})_{0}$ contained in the ordinary element $(E_{p})_{0}$, the ordinary elements $(E_{p})$ near $(E_{p})_{0}$ of the same origin as $(E_{p})_{0}$ are obtained in one and only one way by means of $p$ linear integral elements, which, the $i$th of the first $p$ components is $dx^{k}$ set to zero with the exception of $dx^{i}=1$, and the rest $\nu$ components are $dz^{\lambda}=t_{i}{}^{\lambda}$. The components $t_{1}^{\lambda}$ of the first linear element are required to satisfy  $s_{0}$ equations. The components $t_{2}^{\lambda}$ of the second linear element which, together with the first linear element form an integral element $(E_{2})$, are required to satisfy $s_{0}+s_{1}$ equations of the polar system of $(E_{1})$. The components $t_{3}^{\lambda}$ of the third linear element which, together with $(E_{2})$ form an integral element $(E_{3})$, are required to satisfy $s_{0}+s_{1}+s_{2}$ equations of the polar system of $(E_{2})$. And we can go on in this way. All these equations are independent and we have in total the number of equations
\begin{align*}
s_{0}+(s_{0}+s_{1})+(s_{0}+s_{1}+s_{2})+\dots+(s_{0}+s_{1}+\dots+s_{p-1})\qquad\\
=ps_{0}+(p-1)s_{1}+\dots+s_{p-1}.
\end{align*}

By introducing the number $\sigma_{p}$, we see that the sought-after number of arbitrary parameters is equal to
\begin{align*}
 &\quad\  p(n-p)-[ps_{0}+(p-1)s_{1}+\dots+2s_{p-2}+s_{p-1}]\\
&=p(s_{0}+s_{1}+\dots+s_{p-1}+\sigma_{p})-[ps_{0}+(p-1)s_{1}+\dots+s_{p-1}]\\
&=s_{1}+2s_{2}+3s_{3}+\dots+(p-1)s_{p-1}+s\sigma_{p}.
\end{align*}

\begin{thm*}
  In a closed system of genre equal or greater than $p$, the $p$ dimensional ordinary integral manifolds having given a regular integral point as its origin depend on $s_{1}+2s_{2}+\dots+(p-1)s_{p-1}+p\sigma_{p}$ arbitrary parameters.
\end{thm*}


\vspace{12pt}\fsec \emph{Particular case.} If all the integers except those of ranks $q<p$ are zero and the integer $\sigma_{p}$ is zero, we have the following important theorem:

\begin{thm*}
  If $s_{q}=s_{q+1}=\dots=s_{p-1}=\sigma_{p}=0$, there is one and only one ordinary integral manifold $V_{p}$ containing any $q-1$ dimensional regular integral manifold.
\end{thm*}
The theorem applies for example to a completely integrable system of $n-p$ equations in terms of total differentials of $n$ variables. For then the closed system contain only the equations $\theta_{\alpha}=0$ and we have
\[
s_{0}=n-p,\qquad s_{1}=s_{2}=\dots=s_{p}=0.
\]
Then effectively there is one and only one integral manifold $V_{p}$ for all regular points of the space.

In the general case, given the integral manifold $V_{q-1}$, the method indicated above results in successively integrating $p-q$ Cauchy-Kowalewski systems. But we can achieve the same result by integrating only once: it suffices by supposing that $V_{q-1}$ is contained in the manifold 
\[
x^{q}=x^{q+1}=\dots=x^{p}=0,
\]
to set
\[
x^{q}=a^{q}t,\qquad x^{q+1}=a^{q+1}t,\qquad\dots\qquad x^{p}=a^{p}t,
\]
and regard $a^{q},a^{q+1},\dots,a^{p}$ as arbitrary \emph{parameters} and then integrate the system, where the $z^{p}$ are regarded as unknown functions of $q$ independent variables $x^{1},x^{2},\dots,x^{q-1},t$. We then come to a single Cauchy-Kowalewski system of $n-p$ unknown functions of $q$ independent variables. Once this system is integrated, we replace in the expression obtained the unknown functions $t$ by $1$ and $a^{q},q^{q+1},\dots,a^{p}$ by $x^{q},x^{q+1},\dots,x^{p}$.

The preceding procedure is fundamentally the same as the procedure indicated for integrating completely integrable system.

\begin{rmk*}
  If the system \eqref{eq:4.1} does not contain exterior differentials of degree more than $2$, the numbers $s_{0},s_{1},s_{2},\dots$ cannot increase, and it suffices that the integer $s_{q}$ vanishes for all the following integers to vanish. As for $\sigma_{p}$, which is not a character, it is enough to suppose that we have
\[
s_{0}+s_{1}+\dots+\dots+s_{q-1}=n-p.
\]
\end{rmk*}


\section{General solution and singular solutions. Characteristics}
\label{sec:gener-solut-sing}


\fsec We say that a $p$ dimensional integral manifold is part of the \emph{general solution} of a differential systen, considered as $p$ independent variables, if its $p$ dimensional generic tangent element is an ordinary integral element. The existence, at least local, of the integral manifold is established by the consequence of the Cauchy-Kowalewski theorem.

An integral manifold which is not part of the general solution is said to be \emph{singular}. A solution may be singular either because none of the points of its integral manifold is regular, or none of the integral elements of one dimension, or of two dimensions, \emph{etc.}, or of $p-1$ dimensions, is a regular integral element. Therefore there may be different classes of singular integral manifolds, the degree of singularity decreases when we move from one class to the next.



\vspace{12pt}\fsec We have already introduced the notion of \emph{characteristic} of a differential system, which is the characteristic of the generation of integral manifolds which is contained in no integral manifold of greater dimensions. These characteristics exist only for certain differential system. We will refer to them as the \emph{Cauchy characteristics}.

There are other spaces of characteristics, existing in general for every integral manifold $V_{p}$ of a given dimension $p$ and constitute part of the general solution of the system. These are the $q<p$ dimensional manifolds contained in the integral manifold $V_{p}$ with the property that their $p$ dimensional tangent elements are not regular.  \emph{Their importance is that the Cauchy-Kowalewski theorem fails if we try to determine the $q+1$ dimensional integral manifolds which contain them.} Determining them is related to the preliminary problem of finding the $q$ dimensional integral elements which are not regular. The existence of such elements does not \emph{ipso facto} entails the existence of $q$ dimensional characteristic manifolds, unless $q=1$, and due to compatibility conditions we necessarily have $q>1$.

We will clarify these concepts by some examples taken from classical problems.



\vspace{12pt}\fsec \textsc{Example I.} \emph{Second order partial differential equations}. Consider, using classical notations, the second order partial differential equations
\[
F(x,y,z,p,q)=0
\]
of an unknown function of two independent variables. Following the conceptions of S.~Lie, let us extend this problem to the searching of two dimensional solutions of the closed differential system
\begin{equation}
  \label{eq:4.13}
  \left\{
    \begin{aligned}
      F(x,y,z,p,q)&=0,\\
      F_{x}dx+F_{y}dy+F_{z}dz+F_{p}dp+F_{q}dq&=0,\\
      dz-p\,dx-q\,dy&=0,\\
      [dx\,dp]-[dy\,dq]&=0.
    \end{aligned}
\right.
\end{equation}

The character $s_{0}$ is equal to the rank of the system
\begin{equation}
  \label{eq:4.14}
  \left\{
    \begin{aligned}
      (F_{x}+pF_{z})dx+(F_{y}+qF_{z})dy+F_{p}dp+F_{q}dq&=0,\\
      dz-p\,dx-q\,dy&=0.
    \end{aligned}
  \right.
\end{equation}

The rank is equal to $2$. A point of the integral manifold is singular (\emph{i.e.}, non-regular) if the rank of the system \eqref{eq:4.14} is less than $2$, and this happens if we have
\begin{equation}
  \label{eq:4.15}
  F_{x}+pF_{z}=0,\qquad F_{y}+qF_{z}=0,\qquad F_{p}=0,\qquad F_{q}=0.
\end{equation}

The \emph{singular solutions} are therefore those that satisfy equations \eqref{eq:4.15}.

There are no others. Indeed suppose a generic point of the integral manifold to be regular. The polar element of a linear integral element of components $\delta x,\delta y, \delta z, \delta p, \delta q$ is given by the equations \eqref{eq:4.14} together with the equation
\begin{equation}
  \label{eq:4.16}
  \delta p\,dx+\delta q\,dy-\delta x\,dp-\delta y,dq=0.
\end{equation}

The character $s_{1}$ is therefore equal to $1$. A singular integral manifold can in principle be present  if for all tangent linear elements, the rank of the system \eqref{eq:4.14} and \eqref{eq:4.16} is less by one, and this happens if we have
\begin{equation}
  \label{eq:4.17}
  \frac{\delta x}{F_{p}}=\frac{\delta y}{F_{q}}=\frac{-\delta p}{F_{x}+pF_{z}}=\frac{-\delta q}{F_{y}+q F_{z}}=\frac{\delta z}{p F_{p}+qF_{q}},
\end{equation}
but the relations \eqref{eq:4.17} shows that for a given point of the integral manifold, there exists only a single singular tangent linear element. \emph{Therefore there exists no other singular solutions than those that satisfy equations \eqref{eq:4.15}, if they exist.}

The characteristics of an integral manifold $V_{2}$ are, according to the definition given in \textsection\emph{73}, the lines whose elements satisfy the equation \eqref{eq:4.17}: they are the characteristics that we have already encountered (\textsection\textbf{59}). They depend on their arbitrary set of constants.

\vspace{12pt}\fsec \textsc{Example II.} \emph{Second order partial differential equations}. Every second order partial differential equation of an unknown function $z$ of two independent variables $x,y$ can be represented by the closed differential system
\begin{equation}
  \label{eq:4.18}
  \left\{
    \begin{aligned}
      F(x,y,z,p,q,r,s,t)&=0,\\
      (F_{x}+pF_{z}+rF_{p}+sF_{q})dx+
      (F_{y}+qF_{z}+sF_{p}+tF_{q})dy\\
      +F_{r}dr+F_{s}ds+F_{t}dt&=0,\\
      dz-p\,dx-q\,dy&=0,\\
      dp-r\,dx-s\,dy&=0,\\
      dq-s\,dx-t\,dy&=0,\\
      [dx\,dr]+[dy\,ds]&=0,\\
      [dx\,ds]+[dy\,dt]&=0.
    \end{aligned}
  \right.
\end{equation}

The character $s_{0}$ is equal to $4$, the rank of the linear system in $dx,dy,\dots,dt$ formed by the four equations in \eqref{eq:4.18} that follows the first equation. The rank is reduced by one on the points where
\begin{equation}
  \label{eq:4.19}
  F_{x}+pF_{z}+rF_{p}+sF_{q}=0,\quad F_{y}+qF_{z}+sF_{p}+tF_{q}=0,\quad F_{r}=0,\quad F_{s}=0,\quad F_{t}=0.
\end{equation}

We hence have a first class of singular solutions, these that satisfy equations \eqref{eq:4.19}.

Consider now a regular point. The polar system of an ordinary linear integral element of components $(\delta x,\delta y, \delta z, \delta p, \delta q, \delta r, \delta s, \delta t)$ is given by the four equations \eqref{eq:4.18} that follow the first equations, together with the two equations
\begin{equation}
  \label{eq:4.20}
  \left\{
    \begin{aligned}
    \delta x\,dr+\delta y\,ds-\delta r\,dx-\delta s\,dy&=0,\\
    \delta x\,ds+\delta y\,dt-\delta s\,dx-\delta t\,dy&=0.      
    \end{aligned}
  \right.
\end{equation}

We have $s_{0}+s_{1}=6$, from which $s_{1}=2$, and the rank $6$ of the polar system is reduced by one if the rank of the matrix
\begin{equation}
  \label{eq:4.21}
  \begin{pmatrix}
    F_{x}+pF_{z}+rF_{p}+sF_{q}& F_{y}+qF_{z}+sF_{p}+tF_{q}& F_{r}& F_{s}& F_{t}\\
-\delta r&-\delta s&\delta x&\delta y&0\\
-\delta s&-\delta t& 0&\delta x&\delta y
  \end{pmatrix}
\end{equation}
is equal to $2$. This cannot happen for any  linear element tangent to a two dimensional integral manifold, at least if there exists in this manifold no relations between $x$ and $y$ \footnote{The cases where one such relation exists are of no interest: either there is only one relation between $x$ and $y$, for example $y$ as a function of $x$: then, according to the three equations in \eqref{eq:4.18} which follow the second equation, $z,p,q$ are also functions of $x$ and we have the equations of the manifold of the form
  \begin{align*}
    r+2sy'+ty'^{2}&=z''-qy'',\\
    F(x,y,z,p,q,r,s,t)&=0,
  \end{align*}
  or $x$ and $y$ are constants: then $z,p,q$ are as well and the equation of the manifold becomes $F(x,y,z,p,q,r,s,t)=0$.
}. Indeed, in particular, the hypothesis we made results in the relation
\[
F_{t}\delta x^{2}-F_{s}\delta x\,\delta y+F_{r}\delta y^{2}=0,
\]
from which we have the consequence
\[
F_{r}=F_{s}=F_{t}=0
\]
and, according the the first equation \eqref{eq:4.18}, the relations
\[
F_{x}+pF_{z}+rF_{p}+sF_{q}=0,\qquad F_{y}+q F_{z}+sF_{p}+tF_{q}=0,
\]
and no point of the integral manifold is singular. \emph{Therefore there exists no other singular integral manifold other than those that satisfy equations \eqref{eq:4.19}.}

On an integral manifold which forms part of the general solution, the characteristics are the one dimensional manifolds whose rank of the matrix \eqref{eq:4.21} is equal to $2$. In particular, we have, by moving along such a characteristic, the relation
\begin{equation}
  \label{eq:4.22}
  F_{t}\delta x^{2}-F_{s}\delta x\,\delta y+F_{r}\delta y^{2}=0.
\end{equation}

On every point of the manifold there are two, real if $(F_{s})^{2}-F_{r}F_{t}>0$. We quickly verify that if we move on the integral manifold in a manner which satisfies the relation \eqref{eq:4.22}, the rank of the matrix \eqref{eq:4.21} becomes automatically equal to $2$.

Unlike what happens for first order partial differential equations, the characteristics of second order partial differential equations depend in general on an infinite number of arbitrary parameters (in fact, of arbitrary functions), and those are not the Cauchy characteristics. 

\vspace{12pt}\fsec \textsc{Example III.} \emph{System of two first order partial differential equations of two unknown functions $z_{1}, z_{2}$ of two independent variables $x,y$}.

Such a system can be represented by the closed differential system
\begin{equation}
  \label{eq:4.23}
  \left\{
    \begin{aligned}
      F(x,y,z^{1},z^{2},p_{1},q_{1},p_{2},q_{2})&=0,\\
      \Phi(x,y,z^{1},z^{2},p_{1},q_{1},p_{2},q_{2})&=0,\\
      (F_{x}+p_{1}F_{z^{1}}+p_{2}F_{z^{2}})dx+(F_{y}+q_{1}F_{z^{1}}+q_{2}F_{z^{2}})dy\\
      +F_{p_{1}}dp_{1}+F_{q_{1}}dq_{1}+F_{p_{2}}dp_{2}+F_{q_{2}}dq_{2}&=0,\\
      (\Phi_{x}+p_{1}\Phi_{z^{1}}+p_{2}\Phi_{z^{2}})dx+(\Phi_{y}+q_{1}\Phi_{z^{1}}+q_{2}\Phi_{z^{2}})dy\\
      +\Phi_{p_{1}}dp_{1}+\Phi_{q_{1}}dq_{1}+\Phi_{p_{2}}dp_{2}+\Phi_{q_{2}}dq_{2}&=0,\\
      dz^{1}-p_{1}dx-q_{1}dy&=0,\\
      dz^{2}-p_{2}dx-q_{2}dy&=0,\\
      [dx\,dp_{1}]+[dy\,dq_{1}]&=0,\\
      [dx\,dp_{2}]+[dy\,dq_{2}]&=0.
    \end{aligned}
  \right.
\end{equation}

The character $s_{0}$ is equal to $4$ and it is the rank of the linear system consisting of the four equations in \eqref{eq:4.23} that follow the second equation. The rank decreases only if we have either
\begin{equation}
  \label{eq:4.24}
  \left\{
    \begin{aligned}
      F_{x}+p_{1}F_{z^{1}}+p_{2}F_{z^{2}}&=F_{y}+q_{1}F_{z^{1}}+q_{2}F_{z^{2}}=F_{p_{1}}=F_{q_{1}}=F_{p_{2}}=F_{q_{2}}=0,\\
      \Phi_{x}+p_{1}\Phi_{z^{1}}+p_{2}\Phi_{z^{2}}&=\Phi_{y}+q_{1}\Phi_{z^{1}}+q_{2}\Phi_{z^{2}}=\Phi_{p_{1}}=\Phi_{q_{1}}=\Phi_{p_{2}}=\Phi_{q_{2}}=0,
    \end{aligned}
  \right.
\end{equation}
or
\begin{equation}
  \label{eq:4.25}
  \frac{F_{x}+p_{1}F_{z^{1}}+p_{2}F_{z^{2}}}{\Phi_{x}+p_{1}\Phi_{z^{1}}+p_{2}\Phi_{z^{2}}}=\frac{F_{y}+q_{1}F_{z^{1}}+q_{2}F_{z^{2}}}{\Phi_{y}+q_{1}\Phi_{z^{1}}+q_{2}\Phi_{z^{2}}}=\frac{F_{p_{1}}}{\Phi_{p_{1}}}=\frac{F_{q_{1}}}{\Phi_{q_{1}}}=\frac{F_{p_{2}}}{\Phi_{p_{2}}}=\frac{F_{q_{2}}}{\Phi_{q_{2}}}.
\end{equation}

Therefore there are two possible kinds of singular integral manifolds, according to wither the equations \eqref{eq:4.24} or \eqref{eq:4.25} are satisfied.

Let us now calculate the character $s_{1}$. To have the equations of the polar element of the linear integral elements of components $\delta x,\delta y, \delta z^{1},\delta z^{2},\delta p_{1},\delta q_{1}, \delta p_{2}, \delta q_{2}$, the four linear equations appearing in the system \eqref{eq:4.23} have to be complemented with the equations
\begin{align*}
  \delta x\,dp_{1}+\delta y\,dq_{1}-\delta p_{1}dx-\delta q_{1}dy&=0,\\
  \delta x\,dp_{2}+\delta y\,dq_{2}-\delta p_{2}dx-\delta q_{2}dy&=0.
\end{align*}

We deduce from this that $s_{1}=2$. The linear integral element considered is singular if the six equations defining the polar element of this linear integral element reduce to five equations. Now on a two dimensional integral manifold not having any relations between $x$ and $y$, these equations can contain no linear relations between $dx$ and $dy$ since the polar element considered contains all the linear elements tangent to the manifold. Therefore it is necessary and sufficient that, for the linear element considered to be singular, the equations
\begin{equation}
  \label{eq:4.26}
  \left\{
    \begin{aligned}
      F_{p_{1}}dp_{1}+F_{q_{1}}dq_{1}+F_{p_{2}}dp_{2}+F_{q_{2}}dq_{2}&=0,\\
      \Phi_{p_{1}}dp_{1}+\Phi_{q_{1}}dq_{1}+\Phi_{p_{2}}dp_{2}+\Phi_{q_{2}}dq_{2}&=0,\\
      \delta x\,dp_{1}+\delta y\,dq_{1}&=0,\\
      \delta x\,dp_{2}+\delta y\,dq_{2}&=0,
    \end{aligned}
\right.
\end{equation}
reduce to three equations, which gives immediately
\begin{equation}
  \label{eq:4.27}
  \frac{D(F,\Phi)}{D(q_{1},q_{2})}\delta x^{2}-\left[\frac{D(F,\Phi)}{D(p_{1},q_{2})}-\frac{D(F,\Phi)}{D(p_{2},q_{1})}\right]\delta x\,\delta y+\frac{D(F,\Phi)}{D(p_{1},p_{2})}\delta y^{2}=0.
\end{equation}

We deduce from this result two conclusions:

1. We can have a second class of singular integral manifolds, which satisfy the three equations
\begin{equation}
  \label{eq:4.28}
  \frac{D(F,\Phi)}{D(q_{1},q_{2})}=\frac{D(F,\Phi)}{D(p_{1},q_{2})}-\frac{D(F,\Phi)}{D(p_{2},q_{1})}=\frac{D(F,\Phi)}{D(p_{1},p_{2})}=0.
\end{equation}

2. On an ordinary integral manifold, that is to say one that belongs to the general solution, there exists in general two families of characteristic lines, defined by the differential equation \eqref{eq:4.27}.
\begin{thm*}
  Giving a system of two first order partial differential equations of two unknown functions $z^{1},z^{2}$ in two independent variables $x,y$, there exist three classes of singular solutions, according to whether the equations \eqref{eq:4.24}, \eqref{eq:4.25} or equations \eqref{eq:4.28}. Moreover, each general integral manifold admit two families of characteristic lines defined by the differential equation \eqref{eq:4.27}.
\end{thm*}

\vspace{12pt}\fsec \emph{Second order partial differential equation of one unknown function $z$ of three independent variables $x^{1},x^{2},x^{3}$}. We denote by $p_{i}$ and $p_{ij}=p_{ji}$ the first and second partial derivatives of $z$, the indices indicating the variables with respect to which the derivation is taken. If $F(x^{i},z,p_{i},p_{ij})=0$ is the given equation, we can write
\begin{equation}
  \label{eq:4.29}
  \left\{
    \begin{aligned}
      A_{i}&=\frac{\pd F}{\pd x^{i}}+p_{i}\frac{\pd F}{\pd z}+p_{ik}\frac{\pd F}{\pd p_{k}},\\
      A^{ij}&=m\frac{\pd F}{\pd p_{ij}}\qquad \text{($m=1$ if $i=j$, $m=\frac{1}{2}$ if $i\neq j$).}
    \end{aligned}
\right.
\end{equation}

The given equation can be represented by the differential system formed by the $9$ equations
\begin{equation}
  \label{eq:4.30}
  \left\{
    \begin{aligned}
      F&=0,\\
      A_{i}dx^{i}+A^{ij}dp_{ij}&=0,\\
      dz-p_{i}dx^{i}&=0,\\
      dp_{i}-p_{ik}dx^{k}&=0,&(i&=1,2,3),\\
      [dx^{k}dp_{ik}]&=0,&(i&=1,2,3).
    \end{aligned}
\right.
\end{equation}

The character $s_{0}$ is equal to $5$, the rank of the system formed by the $5$ linear equations in \eqref{eq:4.30} following the equation $F=0$. A non-regular integral point is characterised by the relations
\begin{equation}
  \label{eq:4.31}
  A_{i}=0,\qquad A^{ij}=0.
\end{equation}
We have a first possible class of singular integral manifolds, by finding the integral manifolds satisfying the equations \eqref{eq:4.31}.

The polar system of a regular linear integral element contains $s_{0}+s_{1}=8$ equations, the first $5$ equations define the linear integral elements and the other $3$ equations are
\[
\delta x^{k}dp_{ik}-\delta p_{ik}dx^{k}=0,\qquad (i=1,2,3).
\]

The integral element $(\delta x^{k},\delta z,\dots)$ will be singular if the four equations
\begin{equation}
  \label{eq:4.32}
\left\{\begin{aligned}
  A_{i}dx^{i}+A^{ij}dp_{ij}&=0,\\
  \delta x^{k}dp_{ik}-\delta p_{ik}dx^{k}&=0
\end{aligned}\right.
\end{equation}
reduce to three equations. As on a three dimensional integral manifold for which the independent variables are $x^{1},x^{2},x^{3}$ the equations \eqref{eq:4.32} can contain no linear relations between the $dx^{i}$, a singular linear integral element will be characterised by the property that the four equations
\begin{equation}
  \label{eq:4.33}
  \left\{
    \begin{aligned}
      A^{ij}dp_{ij}&=0,\\
      \delta x^{k}dp_{ij}&=0
    \end{aligned}
\right.
\end{equation}
reduce to three equations. Let $\delta x^{i}=a^{i}$ be such a singular integral element, which we now suppose that it exists. If, in the equations \eqref{eq:4.33}, we replace $dp_{ij}$ by the product $\xi_{i}\xi_{j}$ of two new variables $\xi$, we see that the equation $a^{i}\xi_{i}$ must imply $A^{ij}\xi_{i}\xi_{j}=0$. Then the quadratic form $A^{ij}\xi_{i}\xi_{j}$ must decompose into a product of two linear factors:
\[
A^{ij}\xi_{i}\xi_{j}=a^{k}b^{h}\xi_{k}\xi_{h}
\]
from which
\begin{equation}
  \label{eq:4.34}
  A^{ij}=\frac{1}{2}(a^{i}b^{j}+b^{i}a^{j}).
\end{equation}

Conversely if the quantities $A^{ij}$ are of the form \eqref{eq:4.33}, we verify easily that there is on each point two singular linear integral elements, of components respectively $\delta x^{i}=a^{i}$ and $\delta x^{i}=b^{i}$.

We see that if an integral element does not annihilate all the $A^{ij}$, it is impossible that all its tangent linear elements are singular.

We see on the other hand that if the discriminant of the quadratic form $A^{ij}\xi_{i}\xi_{j}$ is zero, every non-singular integral manifold admit two families of distinct or coinciding characteristic lines, one of them formed by the trajectories of the vector field $a^{i}$, the other formed  by the trajectories of the vector field $b^{i}$. If on the contrary the discriminant of the quadratic form $A^{ij}\xi_{i}\xi_{j}$ is non-zero, there does not exist any characteristic lines.

Finally let us consider the character $s_{2}$. The polar element of a regular two dimensional integral element is given by the $5$ equations defining the linear integral elements, together with the six other equations which can be written, omitting the terms in $dx^{1},dx^{2},dx^{3}$,
\begin{align*}
  \delta_{1}x^{k}dp_{ik}&=0,\\
  \delta_{2}x^{k}dp_{ik}&=0,
\end{align*}
where we denote by $\delta_{1}x^{k}$ and $\delta_{2}x^{k}$ the components of two linear integral elements determining the two dimensional integral element considered. The equations can be written as
\[
\frac{dp_{i1}}{c_{1}}=\frac{dp_{i2}}{c_{2}}=\frac{dp_{i3}}{c_{3}},\qquad(i=1,2,3),
\]
where we denote by 
\[
c_{i}dx^{i}=0
\]
the equation of the plane integral element considered. From this it follows that the $dp_{ij}$ are proportional to the products $c_{i}c_{j}$. The two dimensional integral element will therefore be singular if we have
\begin{equation}
  \label{eq:4.35}
  A^{ij}c_{i}c_{j}=0.
\end{equation}

This equation expresses that in the three dimensional space constituted by the integral manifold under consideration, the singular tangent plane elements based on a point is tangent to the cone of the second class having the point as its apex.

\emph{Therefore there exists two dimensional characteristic manifolds: they are the solutions}, in the manifold formed by the coordinates $x^{1},x^{2},x^{3}$, \emph{of a first order ordinary partial differential equation}. The \emph{bi-characteristics} of J.~Hadamard are the characteristics of this equation. Strictly speaking it is not the characteristics of the differential system \eqref{eq:4.30}, unless the equation \eqref{eq:4.35} decompose into two linear equations, in which case the partial differential equation of the characteristics decompose into two linear equations whose integral surfaces are the surfaces formed  by the characteristic lines of the first family or by the characteristic lines of the second family.

\begin{thm*}
  A second order partial differential equation of one unknown function $z$ of three independent variables admit only singular solutions which annihilate the partial derivatives occurring in the defining equations that are second order derivatives with respect to $z$. The non-singular solutions always contain the  two dimensional characteristic manifolds given by the integration of a first order partial differential equation, the enveloping cone of the tangent planes of a point given by the characteristic manifolds passing through the point being of the second class. If the cone degenerates into two lines $\Delta_{1},\Delta_{2}$, there exists two families of two dimensional characteristic manifolds: the first one constituted by the surfaces formed by the trajectories of the lines $\Delta_{1}$, and the second by the surface formed by the trajectories of the lines $\Delta_{2}$, the trajectories being the characteristic lines of the general integral manifolds of the given equation. If on the contrary the cone is non-degenerate, there does not exist any characteristic lines, that is to say the lines whose tangent elements are singular. The \textsc{bi-characteristics} of J.~Hadamard, that is to say the characteristics of the first order partial differential equation which gives the characteristic surfaces, are strictly speaking not characteristic lines, in the sense that the two dimensional integral manifolds of the system \eqref{eq:4.30} passing through a bi-characteristic are proved by the Cauchy-Kowalewski theorem.
\end{thm*}

\chapter{Differential systems in involution with imposed independent variables}
\label{cha:diff-syst-invol}

\section{Generalities. Systems in involution}
\label{sec:gener-syst-invol}

\fsec In many applications the differential systems that we consider contain the independent variables $x^{1},x^{2},\dots,x^{p}$ given to us and after we put the differential system into the form \eqref{eq:4.1} of \textsection\textbf{52}, we are only interested in the $p$ dimensional integral manifolds and, among these, those that contain no relations between the variables $x^{1},x^{2},\dots,x^{p}$.
\begin{dfn*}
  A differential system $\Sigma$ of $n-p$ unknown functions $z^{\lambda}$ of $p$ variables $x^{i}$ is in involution if its genre is greater or equal to $p$ and  the equations defining the $p$ dimensional generic ordinary integral elements do not introduce any linear relations between $dx^{1},dx^{2},\dots,dx^{p}$.
\end{dfn*}
It is clear that the $p$ dimensional \emph{ordinary} integral manifolds, with the independent variables $x^{1},x^{2},\dots,x^{p}$, can be obtained by applying the existence theorems stated and proved in the preceding chapter. 

\vspace{12pt}\fsec \emph{Systems of partial differential equations}. Every system of exterior differential equations of imposed independent variables can obviously written in the form of a system of partial differential equations of $n-p$ unknown functions in $p$ independent variables. The converse is also true. Indeed, to make things concrete, consider a system with a certain number of relations between  the partial derivatives of the first three orders of $q$ unknown functions $z^{\lambda}$. Denoting by $t_{i}^{\lambda},t_{ij}^{\lambda},t_{ijk}^{\lambda}$ the partial derivatives, the system will be closed by the given conditions between the variables
\[
x^{i},z^{\lambda},t_{i}^{\lambda},t_{ij}^{\lambda},t_{ijk}^{\lambda},\qquad(i,j,k=1,2,\dots,p;\ \lambda=1,2,\dots,q),
\]
to which we add the Pfaffian equations
\begin{align*}
  dz^{\lambda}-t_{i}^{\lambda}dx^{i}&=0,\\
  dt_{i}^{\lambda}-t_{ij}^{\lambda}dx^{j}&=0,\\
  dt_{ij}^{\lambda}-t_{ijk}^{\lambda}dx^{k}&=0.
\end{align*}

We then adjoin the equations which results from the preceding ones by exterior differentiation. The system $\Sigma$ obtained will in fact contain no exterior differentials of degrees more than $2$. We will only have to find the $p$ dimensional integral manifolds of the system, and among these, those that contain no relations between $x^{1},x^{2},\dots,x^{p}$.

We know that, in the theory of first order partial differential equations, S.~Lie has showed that it can be fruitful to relax the last restriction.



\vspace{12pt}\fsec We are going to state immediately the condition for a differential system $\Sigma$ to be in involution.
\begin{thm*}
  For a closed differential system of $n-p$ unknown functions $z^{\lambda}$ in $p$ independent variables $x^{1},x^{2},\dots,x^{p}$ to be in involution, it is necessary and sufficient that the polar systems of  generic integral points and of  $q\le p-1$ dimensional generic integral elements, contain no linear relations between $dx^{1},dx^{2},\dots,dx^{p}$.
\end{thm*}

The condition is obviously necessary. It is sufficient because if it is satisfied, the equations of generic $p$ dimensional ordinary integral elements contain no relations between $dx^{1},dx^{2},\dots,dx^{p}$. The $q<p$ dimensional regular integral elements contained in a $p$ dimensional ordinary integral element then can contain no more than $p-q$ independent relations between $dx^{1},dx^{2},\dots,dx^{p}$.


\section{Reduced characters}
\label{sec:reduced-characters}

\fsec We are going to state the criteria for involution taking into consideration of what we call the \emph{reduced characters}.

First let us determine the $p$ dimensional integral elements (without linear relations between the $dx^{i}$, which we will always suppose in the following) having a generic integral point as the origin. We exclude the case where the existence of such an integral element would imply new relations between the dependent and independent variables, whence the rest of the system will not be in involution. We then consider the family $\mathcal{F}$ of integral elements of $1,2,\dots,p-1$ dimensions that may be contained in a $p$ dimensional integral element.
\begin{dfn*}
  A reduced polar system of an integral point or an integral element is the polar system of the point or the element where  we discard in  its equations the terms in $dx^{1},dx^{2},\dots,dx^{p}$ and keep only the terms in $dz^{1},dz^{2},\dots,dz^{q}$.
\end{dfn*}

We respectively denote by
\[
s'_{0},\quad s'_{0}+s'_{1},\quad s'_{0}+s'_{1}+s'_{2},\quad\dots\quad s'_{0}+s'_{1}+s'_{2}+\dots+s'_{p-1}
\]
the rank of the reduced polar system of a generic integral point, of a one dimensional integral element of the family $\mathcal{F}$, of a two dimensional integral element of the family $\mathcal{F}$ and so on.

\emph{The non-negative integers $s'_{0},s'_{1},\dots,s'_{p-1}$ are  the reduced characters of orders $0,1,\dots,p-1$.} It is clear that for the equations of the different reduced polar systems containing only the variables $dz^{\lambda}$, we have
\[
s'_{0}+s'_{1}+s'_{2}+\dots+s'_{p-1}\le n-p.
\]

We introduce finally the reduced character $s'_{p}$ by the relation
\[
s'_{0}+s'_{1}+s'_{2}+\dots+s'_{p-1}+s'_{p}=n-p.
\]


\vspace{12pt}\fsec \emph{Remark.} It may happen that the $p$ dimensional integral elements of a generic integral point form several distinct continuous families. Each of these families correspond to a set of reduced characters. The problem of knowing if the given differential system is in involution has to be solved for each of these different families, since the $p$ dimensional integral manifolds that we are after are not the same in each different case, as their $p$ dimensional tangent elements vary from one family to the other. Therefore it is possible that the given system is in involution for one of the families of $p$ dimensional integral manifolds and is not in involution for another.


\section{Necessary and sufficient criteria for involution}
\label{sec:necess-suff-crit}

\fsec We now state a criteria for involution.

\theoremstyle{shape1}
\newtheorem*{cnsi}{\hspace{15pt}Necessary and sufficient criteria for involution}
\begin{cnsi}
  Consider a closed differential system of $n-p$ unknown functions in $p$ independent variables. Let $s'_{0},s'_{1},\dots,s'_{p-1}$ be the reduced characters of the system corresponding to the family [or one of the families] of $p$ dimensional integral elements of the system. For the system to be in involution, it is necessary and sufficient that the number of independent equations linking the parameters $t_{i}^{\lambda}$ of a $p$ dimensional generic integral element of the family is equal to 
\[
ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}.
\]
If the system is not in involution, the number of these equations is greater.
\end{cnsi}


\vspace{12pt}\fsec We begin by an important remark. We suppose, by means of a linear change of variables on $x^{1},x^{2},\dots,x^{p}$ with constant coefficients, that the rank of the reduced polar system of a generic linear integral element of the family $\mathcal{F}$ for which $dx^{2}=dx^{3}=\dots=dx^{p}=0$ is equal to its normal value $s'_{0}+s'_{1}$, that the rank of the reduced polar system of the generic two dimensional integral element of the family $\mathcal{F}$ for which $dx^{3}=\dots=dx^{p}=0$ is equal to $s'_{0}+s'_{1}+s'_{2}$ and so on.

If we denote by
\[
dz^{\lambda}=t_{1}^{\lambda}dx^{1}+t_{2}^{\lambda}dx^{2}+\dots+t_{p}^{\lambda}dx^{p}
\]
the equations of a $p$ dimensional generic element of the given family, we see that $t_{1}^{\lambda}$ must satisfy $s'_{0}$ linear independent equations, and these are the $s'_{0}$ reduced polar equations of a generic integral point where we replace $dz^{\lambda}$ by $t_{1}^{\lambda}$. For $t_{1}^{\lambda}$ satisfying these equations, the $t_{2}^{\lambda}$ must satisfy $s'_{0}+s'_{1}$ independent equations, which are the $s'_{0}+s'_{1}$ reduced polar equations of the linear integral elements $(\delta x^{1}=1,\delta x^{2}=\dots=\delta x^{p}=0,\delta z^{\lambda}=t_{1}^{\lambda})$ where we replace $dz^{\lambda}$ by $t_{2}^{\lambda}$. And so on for the rest. This shows that there exists \emph{at least}
\[
ps'_{0}+(p-1)s'_{1}+\dots+2s'_{p-2}+s'_{p-1}
\]
independent equations which the parameters $t_{i}^{\lambda}$ must satisfy. This justifies the last part of the statement of the criteria. We say that the equations, which are well defined whether or not the system is in involution, are the equations that the parameters $t_{i}^{\lambda}$ \emph{normally} satisfy.


\vspace{12pt}\fsec \emph{Proof of the criteria}. We now proceed to prove the main part of the criteria.

Suppose first that the system is in involution. Hence every linear integral element for which $dx^{i}$ do not all vanish belong to the family $\mathcal{F}$ and it only needs to satisfy the $s'_{0}$ equations which do not contain any relations between the $dx^{i}$, and therefore we have $s_{0}=s'_{0}$ \footnote{Recall that the $s_{i}$ are the characters defined in \textsection\textbf{56} and \textsection\textbf{57} and the $s'_{i}$ are the reduced characters.}. The polar system of a generic linear regular integral element is then defined by $s'_{0}+s'_{1}$ linear independent equations containing no relations between the $dx^{i}$, we therefore have $s'_{0}+s'_{1}=s_{0}+s_{1}$, from which $s'_{1}=s_{1}$, and all two dimensional ordinary integral elements that contain only $p-2$ relations between the $dx^{i}$ belong to the family $\mathcal{F}$. We can continue the argument to the end and we have $s'_{h}=s_{h}$ for $h=0,1,2,\dots,p-1$. But we know (\textsection\textbf{70}) that the number of independent equations which the parameters $t_{i}^{\lambda}$ satisfy for a $p$ dimensional ordinary integral element is equal to
\[
ps_{0}+(p-1)s_{1}+\dots+s_{p-1}=ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}.
\]
The condition for involution stated is hence necessary.

\emph{Conversely} suppose the system is not in involution. The parameters $t_{i}^{\lambda}$ satisfy
\[
ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}
\]
normal equations. But they are not the only ones. Indeed if all linear integral elements for which there exists only $p-1$ relations between the $dx^{i}$ belong to the family $\mathcal{F}$, we have $s'_{0}=s_{0}$, and if all two dimensional integral elements for which there are only $p-2$ relations between the $dx^{i}$ belong to the family $\mathcal{F}$, we have $s'_{1}=s_{1}$, but we cannot pursue these hypotheses to the end, otherwise the system will be in involution.  Therefore for example suppose that the three dimensional integral elements for which there exists only $p-3$ relations between the $dx^{i}$ do not all belong to the family $\mathcal{F}$. This means that the $t_{1}^{\lambda},t_{2}^{\lambda}$ and $t_{3}^{\lambda}$ satisfy other equations besides the $3s'_{0}+2s'_{1}+s'_{2}$ normal equations which relate them. Then the number of independent equations  which the parameters $t_{i}^{\lambda}$ satisfy is \emph{greater} than $ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}$.\qed

\begin{rmk*} By introducing  the character $s'_{p}$ as above, we can say that \emph{the necessary and sufficient condition for involution is that the more general $p$ dimensional integral element of the family considered depend on}
\[
s'_{1}+2s'_{2}+\dots+(p-1)s'_{p-1}+ps'_{p}
\]
\emph{independent parameters.}
  
\end{rmk*}



\vspace{12pt}\fsec \textsc{Example I.} Consider the system of two independent variables and three unknown functions, defined by the four equations
\begin{align*}
  [dx^{1}dz^{1}]&=0,\\
  [dx^{2}dz^{1}]&=0,\\
  [dz^{1}dz^{2}]&=0,\\
  [dx^{1}dz^{3}]&=0.
\end{align*}

The two dimensional integral elements form only one irreducible family defined by the equations
\begin{align*}
  dz^{1}&=0,\\
  dz^{2}&=a\,dx^{1}+b\,dx^{2},\\
  dz^{3}&=a'dx^{1}+b'dx^{2},
\end{align*}
with four arbitrary parameters: therefore there exists two relations between the $6$ quantities $t_{1}^{\lambda},t_{2}^{\lambda}$. Here we have $s_{0}=0$. On the other hand we can take for a generic linear element of the family $\mathcal{F}$ the element
\[
\delta x^{1}=\alpha,\quad \delta x^{2}=\beta,\quad \delta z^{1}=0,\quad \delta z^{2}=a\alpha+b\beta,\quad \delta z^{3}=a'\alpha+b'\beta,
\]
its reduced polar system is
\[
\alpha\,dz^{1}=0,\quad \beta\, dz^{1}=0,\quad(a\alpha+b\beta)dz^{1}=0,\quad(a'\alpha+b'\beta)dz^{1}=0,
\]
we therefore have $s'_{1}=1$, from which $s'_{2}=2$ (since $s'_{1}+s'_{2}$ is equal to $3$, the number of unknown functions). However, the number $4$ of independent parameters the two dimensional generic integral elements depend on is less than $s'_{1}+2s'_{2}=5$. The system is not in involution. We will arrive at the same conclusion if we suppress the last equation of the system, which does not contain more than two unknown functions.


\vspace{12pt}\fsec \textsc{Example II.} Consider the following system, of four unknown functions $z^{\lambda}$ of two independent variables $x^{1},x^{2}$,
\begin{align*}
  [dx^{1}dz^{1}]+[dx^{2}dz^{2}]&=0,\\
  [dx^{2}dz^{1}]&=0,\\
  [dx^{2}dz^{3}]&=0,\\
  [dz^{3}dz^{4}]&=0.
\end{align*}

The two dimensional integral elements are given by
\begin{alignat*}{2}
  dz^{1}&=&a\,dx^{2},\\
  dz^{2}&=a\,dx^{1}+&b\,dx^{2},\\
  dz^{3}&=&c\,dx^{2},\\
  c[dx^{3}dz^{4}]&=0.&
\end{alignat*}

Two cases can be distinguished:

1. $c\neq 0$. We then have the irreducible family
\begin{alignat*}{2}
  dz^{1}&=&a\,dx^{2},\\
  dz^{2}&=a\,dx^{1}+&b\,dx^{2},\\
  dz^{3}&=&c\,dx^{2},\\
  dz^{4}&=&h\,dx^{2}
\end{alignat*}
of four parameters $a,b,c,h$. The reduced polar system of a linear integral element $(\delta x^{1}=\alpha,\delta x^{2}=\beta,\delta z^{1}=a\beta,\delta z^{2}=a\alpha+b\beta,\delta z^{3}=c\beta,\delta z^{4}=h\beta)$ is
\begin{align*}
  \alpha\,dz^{1}+\beta\,dz^{2}&=0,\\
  \beta\,dz^{1}&=0,\\
  \beta\,dz^{3}&=0,\\
  c\beta\,dz^{4}-h\beta\,dz^{3}&=0,
\end{align*}
we therefore have $s'_{1}=4,s'_{2}=0$. The number $4$ of independent parameters of the two dimensional integral elements is equal to $s'_{1}+2s'_{2}$, and the system is in involution.

2. $c=0$. We then have the irreducible family
\begin{align*}
  dz^{1}&=a\,dx^{2},\\
  dz^{2}&=a\,dx^{1}+b\,dx^{2},\\
  dz^{3}&=0,\\
  dz^{4}&=h\,dx^{1}+k\,dx^{2}
\end{align*}
with $4$ independent parameters $a,b,h,k$. The reduced polar system of the linear element $(\delta x^{1}=\alpha,\delta x^{2}=\beta,\delta z^{1}=a\beta,\delta z^{2}=a\alpha+b\beta, \delta z^{3}=0,\delta z^{4}=h\alpha+k\beta)$ is
\begin{align*}
  \alpha dz^{1}+\beta dz^{2}&=0,\\
  \beta\,dz^{1}&=0,\\
  \beta\,dz^{3}&=0,\\
  (h\alpha+k\beta)dz^{3}&=0,
\end{align*}
we have $s'_{1}=3,s'_{2}=1$. As $4<s'_{1}+2s'_{2}=5$, the system is not in involution. 

We can remark that if we did not impose the choice of independent variables, we would have a system in involution with a single family of two dimensional ordinary integral elements, by knowing the first family that we have considered.


\vspace{12pt}\fsec \emph{Remark}. We can be tempted to extend the criterion in the case where the $s'_{i}$ are defined by means of reduced polar systems of the generic integral elements of the corresponding dimensions belong to \emph{or not belong to} the family $\mathcal{F}$. But then the criterion could fail in the general case.

We can see this from the example in \textsection\textbf{86}. Indeed, if we take the system
\begin{align*}
  [dx^{1}dz^{1}]&=0,\\
  [dx^{2}dz^{1}]&=0,\\
  [dz^{1}dz^{2}]&=0,\\
  [dz^{1}dz^{3}]&=0,
\end{align*}
and we form the reduced polar system of the linear integral element $(\delta x^{1}=\alpha,\delta x^{2}=\beta,\delta z^{1}=t^{1},\delta z^{2}=t^{2},\delta z^{3}=t^{3})$, we obtain
\[
\alpha\,dz^{1}=0,\quad \beta\,dz^{1}=0,\quad t^{1}dz^{2}-t^{2}dz^{1}=0,\quad t^{1}dz^{3}-t^{3}dz^{1}=0,
\]
whose rank is $s'_{1}=3$. On the other hand the number of equations the two dimensional integral elements must satisfy have been shown to equal to $2$, and the number is \emph{less} than $2s'_{0}+s'_{1}=3$. We hence arrive at a result in contradiction with the last part of the criterion.

If we now take the system not in involution
\begin{align*}
  [dx^{1}dz^{1}]&=0,\\
  [dx^{2}dz^{1}]&=0,\\
  [dz^{1}dz^{2}]&=0,  
\end{align*}
the reduced polar system of the linear integral element $(\delta x^{1}=\alpha,\delta x^{2}=\beta,\delta z^{1}=t^{1},\delta z^{2}=t^{2})$ is
\[
\alpha\, dz^{1}=0,\qquad \beta\, dz^{1}=0,\qquad t^{1}dz^{2}-t^{2}dz^{2}=0,
\]
its rank is $s'_{1}=2$. On the other hand the number of equations the parameters of the two dimensional integral elements must satisfy is again equal to $2$, which is in this case equal to $2s'_{0}+s'_{1}$. Nonetheless the system is not in involution.


\section{A sufficient criterion for involution}
\label{sec:suff-crit-invol}

\fsec We now establish a second criterion for involution that is \emph{only sufficient}, which is often useful for applications.

\newtheorem*{inv2cri}{\hspace{15pt}Second criterion, sufficient for involution.}
\begin{inv2cri}
  Consider a closed differential system of $n-p$ unknown functions in $p$ independent variables, and consider an irreducible family of $p$ dimensional integral elements and the family $\mathcal{F}$ of corresponding integral elements of $q=1,2,\dots,p-1$ dimensions. Denote by $\sigma_{0}=s'_{0}$ the rank of the reduced polar system of a generic point, by $\sigma_{0}+\sigma_{1}$ the rank of the reduced polar system of a generic linear element of the family $\mathcal{F}$ for which $\delta x^{2}=\delta x^{3}=\dots=\delta x^{p}=0$, by $\sigma_{0}+\sigma_{1}+\sigma_{2}$ the rank of the most general two dimensional reduced polar system of the family $\mathcal{F}$ for which $\delta x^{3}=\dots =\delta x^{p}=0$, and so on. The system is in involution if the number of independent equations that the parameters for the family  under consideration of $p$ dimensional integral element must satisfy is equal to $p\sigma_{0}+(p-1)\sigma_{1}+\dots+\sigma_{p-1}$.
\end{inv2cri}

\vspace{12pt}\fsec \emph{Proof}. We know from the previous criterion that the total number of independent equations that the parameters of a $p$ dimensional generic integral element in the family under consideration must satisfy is at least equal to $ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}$. We therefore have
\[
p\sigma_{0}+(p-1)\sigma_{1}+\dots+\sigma_{p-1}\ge ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}.
\]
But we obviously also have the inequalities (where the first is actually an equality)
\begin{align*}
  \sigma_{0}&\le s'_{0},\\
  \sigma_{0}+\sigma_{1}&\le s'_{0}+s'_{1}\\
  \sigma_{0}+\sigma_{1}+\sigma_{2}&\le s'_{0}+s'_{1}+s'_{2},\\
  &\dots\\
  \sigma_{0}+\sigma_{1}+\dots+\sigma_{p-2}+\sigma_{p-1}&\le s'_{0}+s'_{1}+\dots+s'_{p-2}+s'_{p-1},
\end{align*}
which implies, under addition,
\[
p\sigma_{0}+(p-1)\sigma_{1}+\dots+\sigma_{p-1}\le ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}.
\]

It then follows:

1. that the two sides of the first inequality is equal and immediately all the preceding inequalities reduce to the equalities $(\sigma_{i}=s'_{i})$;

2. that the system is in involution.

We can add that the $q$ dimensional integral elements of the family $\mathcal{F}$ for which $\delta x^{q+1}=\delta x^{q+2}=\dots=\delta x^{p}=0$ are \emph{regular}.


\vspace{12pt}\fsec \emph{Complementary remark}. The second criterion continue to be valid if the integers $\sigma_{i}$ are calculated by leaving aside one or more equations of the given differential system. Indeed, this can only diminish the numerical value of the integers $\sigma_{0},\sigma_{0}+\sigma_{1},\sigma_{0}+\sigma_{1}+\sigma_{2}$, etc., and the proof given for the criterion will not cease to be valid. Obviously, if we want to profit from this remark, it is essential to take into consideration \emph{all} of the equations of the system to determine the $p$ dimensional integral elements.

\vspace{12pt}\fsec \textsc{Particular case}. An interesting particular case, which occurs often  in the applications, is that where the $dz^{\lambda}$ occur only in first degree in the exterior differential equations of the given closed system. Indeed, in this case the reduced characters can be calculated \emph{without first knowing the $p$ dimensional integral elements}. In this case the reduced polar system can be formed directly without this prior knowledge.

Take for example
\begin{equation}
  \label{eq:5.1}
  \left\{
  \begin{aligned}
    &\quad\ f_{\alpha}(x,z)=0&(\alpha&=1,2,\dots,r_{0}),\\
    \theta_{\alpha}&\equiv A_{\alpha i}dx^{i}+a_{\alpha\lambda}dz^{\lambda}=0&(\alpha&=1,2,\dots,r_{1}),\\
    \varphi_{\alpha}&\equiv \frac{1}{2}A_{\alpha ij}[dx^{i}dx^{j}]+A_{\alpha i \lambda}[dx^{i}dz^{\lambda}]=0&(\alpha&=1,2,\dots,r_{2}),\\
    \psi_{\alpha}&\equiv \frac{1}{6}A_{\alpha ijh}[dx^{i}dx^{j}dx^{h}]+\frac{1}{2}A_{\alpha ij\lambda}[dx^{i}dx^{j}dz^{\lambda}]=0&(\alpha&=1,2,\dots,r_{3}),\\
    &\dots
  \end{aligned}    
  \right.
\end{equation}
as the equations of the system. The reduced character $s_{0}$ is the rank of the system 
\[
A_{\alpha\lambda}dz^{\lambda}=0,
\]
the sum $s'_{0}+s'_{1}$ is the rank of the system
\begin{align*}
  A_{\alpha\lambda}dz^{\lambda}&=0,\\
  A_{\alpha i \lambda}\delta_{1}x^{i}dz^{\lambda}&=0,
\end{align*}
where the $\delta_{1}x^{i}$ are arbitrary parameters.

The sum $s'_{0}+s'_{1}+s'_{2}$ is the rank of the system
\begin{align*}
  A_{\alpha\lambda}dz^{\lambda}&=0,\\
  A_{\alpha i \lambda}\delta_{1}x^{i}dz^{\lambda}&=0,\\
  A_{\alpha i \lambda}\delta_{2}x^{i}dz^{\lambda}&=0,\\
  A_{\alpha ij\lambda}\delta_{1}x^{i}\delta_{2}x^{j}dz^{\lambda}&=0,
\end{align*}
where the $\delta_{1}x^{i}$ and $\delta_{2}x^{i}$ are arbitrary parameters, and so on.

We immediately observe that in this case there is only one irreducible family of $p$ dimensional integral elements, defined by the relations
\begin{align*}
  A_{\alpha i}+A_{\alpha\lambda}t_{i}^{\lambda}&=0,\\
  &(\alpha=1,2,\dots,r_{1};i=1,2,\dots,p);\\
  A_{\alpha ij}+A_{\alpha i\lambda}t_{j}^{\lambda}-A_{\alpha j\lambda}t_{i}^{\lambda}&=0,\\
  &(\alpha=1,2,\dots,r_{1};i,j=1,2,\dots,p);\\
  A_{\alpha ijh}+A_{\alpha ij\lambda}t_{h}^{\lambda}-A_{\alpha ih\lambda}t_{j}^{\lambda}+A_{\alpha jh\lambda}t_{i}^{\lambda}&=0,\\
  &(\alpha=1,2,\dots,r_{1};i,j,h=1,2,\dots,p);\\
\end{align*}

\section{The case of two independent variables}
\label{sec:case-two-independent}

\fsec If a closed differential system contains only two independent variables, this system contains no exterior differential equations of degree more than $2$. Therefore it is not necessary to concern us with equations resulting from exterior differentiation of second order equations that the system may have, since they are always satisfied by all two dimensional plane elements.

We suppose, for simplicity and without loss of generality, that the system contains no algebraic relations. We write the $s_{0}$ linear independent equations under the form
\[
\theta_{\alpha}=0\qquad(\alpha=1,2,\dots,s_{0}).
\]

Introduce, with the differentials $dx, dy$ of the independent variables, $n-s_{0}-2$ linear differential forms $\varpi^{\lambda}$ $(\lambda=1,2,\dots,n-s_{0}-s)$ independent between themselves and independent with $\theta_{\alpha},dx,dy$. They therefore form, with the $\theta_{\alpha}$, $n-2$ independent forms with respect to differentials of unknown functions. Finally we can take, in place of $dx,dy$, two independent combinations $\omega^{1},\omega^{2}$ of these differentials, which may be convenient in applications.

This put the given system under the form
\begin{equation}
  \label{eq:5.2}
  \left\{
    \begin{aligned}
      \theta_{\alpha}&= 0&(\alpha&=1,2,\dots,s_{0}),\\
      \varphi_{\alpha}&\equiv C_{\alpha}[\omega^{1}\omega^{2}]+A_{\alpha\lambda}[\omega^{1}\varpi^{\lambda}]+B_{\alpha\lambda}[\omega^{2}\varpi^{\lambda}]\\
      &+\frac{1}{2}D_{\alpha\lambda\mu}[\varpi^{\lambda}\varpi^{\mu}]=0&(\alpha&=1,2,\dots,r).
    \end{aligned}
  \right.
\end{equation}

We are concerned only with the case where there exists two dimensional integral elements, which would moreover permit us to assume that the coefficients $C_{\alpha}$ are all zero by replacing $\varpi^{\lambda}$ by $\varpi^{\lambda}$ minus a linear combination of $\omega^{1}$ and $\omega^{2}$. We therefore exclude the systems not in involution.

If we know the general equations
\begin{equation}
  \label{eq:5.3}
  \theta_{\alpha}=0,\qquad\varpi^{\lambda}=t_{1}^{\lambda}\omega^{1}+t_{2}^{\lambda}\omega^{2}
\end{equation}
of two dimensional integral elements, we know the family $\mathcal{F}$ of linear integral elements contained in a two dimensional integral element. After this, the reduced polar system of a linear integral element $(\omega^{i}_{\delta},\varpi^{\lambda}_{\delta})$ of the family $\mathcal{F}$ is formed by the equations $\theta_{\alpha}=0$ and the equations
\begin{equation}
  \label{eq:5.4}
  (A_{\alpha\lambda}\omega^{1}_{\delta}+B_{\alpha\lambda}\omega^{2}_{\delta})+D_{\alpha\lambda\mu}\varpi^{\lambda}_{\delta}\omega^{\mu}=0.
\end{equation}

The matrix of coefficients of the polar system, or \emph{polar matrix}, is none other than the matrix of partial derivatives $\pd \varphi_{\alpha}/\pd \varpi^{\lambda}_{\delta}$ or, when we replace $\omega^{i}_{\delta},\varpi^{\lambda}_{\delta}$ by $\omega^{i},\varpi^{\lambda}$, the matrix of $r$ rows and $\nu=n-s_{0}$ columns
\begin{equation}
  \label{eq:5.5}
  \left(\frac{\pd \varphi_{\alpha}}{\pd \varpi^{\lambda}}\right).
\end{equation}

The reduced character $s'_{1}$, which we now write as $s_{1}$ as long as no confusion will arise, is the rank of the polar matrix. The non-regular, or singular, linear elements are those that annihilate all the determinants formed by $s_{1}$ rows and $s_{1}$ columns of this matrix.

\vspace{12pt}\fsec \emph{Criterion for involution}. We immediately obtain a sufficient criterion for involution by observing that if the reduced character $s_{1}$ is equal to the number $r$ of independent linear  forms $\varphi_{\alpha}$, the condition which must be satisfied by the coefficients $t_{1}^{\lambda},t_{2}^{\lambda}$ of the general equations \eqref{eq:5.2} of two dimensional integral elements reduce to $s_{1}$ conditions
\[
C_{\alpha}+A_{\alpha\lambda}t_{2}^{\lambda}-B_{\alpha\lambda}t_{1}^{\lambda}+D_{\alpha\lambda\mu}t_{1}^{\lambda}t_{2}^{\mu}=0\qquad(\alpha=1,2,\dots,s_{1}).
\]
These conditions are necessarily independent, since the number of independent relations between the $t_{1}^{\lambda}$ and the $t_{2}^{\lambda}$ is \emph{at least} equal to $s_{1}$.

On the other hand there is a case where the sufficient criterion is also necessary, that is if the $\varpi^{\lambda}$ enter linearly  in the forms $\varphi_{\alpha}$. Indeed in this case the equations which must be satisfied by $t_{1}^{\lambda}$ and $t_{2}^{\lambda}$ are
\[
C_{\alpha}+A_{\alpha\lambda}t_{2}^{\lambda}-B_{\alpha\lambda}t_{1}^{\lambda}=0,
\]
and it is clear that there is as many linearly independent equations of this system as there are linearly independent forms $\varphi_{\alpha}$ (taking into consideration of the hypothesis made once and for all that these equations are compatible).

We therefore arrive at the following theorem.
\begin{thm*}
  A sufficient condition for a closed differential system of two independent variables to be in involution is that the reduced character $s_{1}$ is equal to the number of linearly independent quadratic forms $\varphi_{\alpha}$. This condition is also necessary if the forms $\varphi_{\alpha}$ contains only first degree terms of the forms $\varpi^{\lambda}$ (if the coefficients $D_{\alpha\lambda\mu}$ are all zero).
\end{thm*}

\vspace{12pt}\fsec \emph{Remark}. The following example shows that the condition is not always necessary. Consider the closed differential system of three quadratic exterior equations
\[
[\varpi^{2}\varpi^{3}]=0,\qquad[\varpi^{3}\varpi^{1}]=0,\qquad [\varpi^{1}\varpi^{2}]=0.
\]

The two dimensional integral elements are given by the equations
\begin{align*}
  \varpi^{1}&=a_{1}\omega^{1}+b_{1}\omega^{2},\\
  \varpi^{2}&=a_{2}\omega^{1}+b_{2}\omega^{2},\\
  \varpi^{3}&=a_{3}\omega^{1}+b_{3}\omega^{2},
\end{align*}
with
\[
a_{2}b_{3}-a_{3}b_{2}=0,\qquad a_{3}b_{1}-b_{3}a_{1}=0,\qquad a_{1}b_{2}-a_{2}b_{1}=0,
\]
the number of independent parameters where the equations depend on is equal to $4$ ($a_{1},a_{2},a_{3}$ are arbitrary and $b_{1},b_{2},b_{3}$ are proportional). On the other hand the polar matrix is
\[
\begin{pmatrix}
  0&\varpi^{3}&-\varpi^{2}\\
  -\varpi^{3}&0&\varpi^{1}\\
  \varpi^{2}&-\varpi^{1}&0
\end{pmatrix}
\]
whose rank is equal to $2$: $s_{1}=2$ and $s_{2}=1$. We have $s_{1}+2s_{2}=2+2=4$, the number of independent parameters of a generic two dimensional integral element. Nonetheless the number of linearly independent forms $\varpi_{\alpha}$ is equal to $3>s_{1}$.

\vspace{12pt}\fsec \emph{The case where $s_{2}=0$. Characteristics}. In the case where $s_{2}=0$, i.e., $s_{0}=n-s_{1}-2$, the number of the forms $\varpi^{\lambda}$ is equal to $s_{1}$ if the system is in involution. For all non-characteristic one dimensional integral manifold there passes one and only one two dimensional integral manifold. The characteristic lines of a ordinary integral manifold annihilate all the determinants of $s_{1}$ rows and $s_{1}$ columns of the polar matrix. In the case where $s_{1}$ is equal to the number of linearly independent forms $\varphi_{\alpha}$, the polar matrix has exactly $s_{1}$ rows and $s_{1}$ columns, so that the characteristic lines of a given integral manifold are given by a homogeneous equations of degree $s_{1}$ in $\omega^{1},\omega^{2}$, i.e., in $dx,dy$.

Take in particular the case where the coefficients $D_{\alpha\lambda\mu}$ of equations \eqref{eq:5.2} are zero, i.e., the $\varpi^{\lambda}$ enter linearly in the $\varphi_{\alpha}$. In this case the elements of the polar matrix are $A_{\alpha\lambda}\omega^{1}+B_{\alpha\lambda}\omega^{2}$. We can put the quadratic equations $\varphi_{\alpha}=0$ under a remarkable form highlighting the $s_{1}$ families of characteristics, at least when the families are distinct.

Indeed, let $\omega^{2}-m\omega^{1}=0$ be the equation of one of the families. The coefficient $m$ is the root of the equation
\begin{equation}
  \label{eq:5.6}
  |A_{\alpha\lambda}+mB_{\alpha\lambda}|=0,\qquad(\alpha,\lambda=1,2,\dots,s_{1}).
\end{equation}

Let us find a linear combination of the equations $\varphi_{\alpha}$ where the left hand side contains the factor $\omega^{2}-m\omega^{1}$. If $k^{\alpha}\varphi_{\alpha}=0$ is such a combination, that is, when $\lambda=1,2,\dots,s_{1}$ we have
\[
k^{\alpha}(A_{\alpha\lambda}+mB_{\alpha\lambda})=0.
\]
It is possible to find non-zero values of the $k^{\alpha}$ satisfying these $s_{1}$ homogeneous equations since the determinant of the unknown coefficients is zero. A solution of the quadratic exterior equations of the given differential system can therefore be written as the form
\[
[(\omega^{2}-m\omega^{1})\cdot c_{\lambda}\varpi^{\lambda}]=0.
\]

Then, if the $s_{1}$ families of characteristics are distinct and are given by the equations $\omega^{2}-m_{i}\omega^{1}=0$ $(i=1,2,\dots,s_{1})$, the quadratic equations of the system can be put under the form \footnote{We have assumed that the coefficients $C_{\alpha}$ are zero, which can always be realised by adding to $\varpi^{\lambda}$ suitable linear combinations of $\omega^{1},\omega^{2}$.}
\begin{equation}
\label{eq:5.7}
[(\omega^{2}-m_{i}\omega^{1})\cdot c_{i\lambda}\varpi^{\lambda}]=0,\qquad(i=1,2,\dots,s_{1}).
\end{equation}

These equations show a very interesting fact. When we try to determine for an ordinary integral manifold a \emph{characteristic} solution, the problem is in general impossible to solve. This result is evident from equations \eqref{eq:5.7}, since if we have $\omega^{2}=m_{i}\omega^{1}$ along the characteristic curve given, it is necessary that along this curve we also have
\[
c_{i\lambda}\varpi^{\lambda}=0,
\]
as on the unknown two dimensional integral manifold the form $c_{i \lambda}\varpi^{\lambda}$ must be a multiple of $\omega^{2}-m_{i}\omega^{1}$. The question of knowing if this necessary condition is equally sufficient remains unsolved.  If we suppose that it is sufficient, then the problem has infinitely many solutions. This is an area of the theory that is little studied and we know little about.

\section{Systems in involution whose general solution depends only on one function of one variable}
\label{sec:syst-invol-gener}

\fsec We assume, without loss of generality, that the system does not contain algebraic equations. If it is of $p$ independent variables, we denote by $\omega^{1},\omega^{2},\dots,\omega^{p}$ a system of $p$ independent linear combinations of their differentials. Consider respectively
\begin{equation}
  \label{eq:5.8}
  \left\{
    \begin{aligned}
      \theta_{\alpha}&=0&(\alpha&=1,2,\dots,s_{0}),\\
      \varphi_{\alpha}&\equiv A_{\alpha i\lambda}[\omega^{i}\varpi^{\lambda}]=0&(\alpha&=1,2,\dots,r)
    \end{aligned}
  \right.
\end{equation}
the equations of the system which are of first and second degrees. We make the hypothesis that in the forms $\varphi_{\alpha}$ the $\varpi^{\lambda}$ appears only as linear terms. We did not include terms in $[\omega^{i}\omega^{j}]$ because the system, being in involution, admit $p$ dimensional integral elements and so by adding to $\varpi^{\lambda}$ linear combinations of $\omega^{i}$, we can make the coefficients of the products $[\omega^{i}\omega^{j}]$ vanish.

The system may contain equations of degrees higher than $2$, but it is not useful to write them out.

We will indicate a remarkable form of the equations $\varphi_{\alpha}=0$ and from it deduce some relatively important consequences about the characteristics of the given system.

\vspace{12pt}\fsec Let $s_{1}$ be the first order reduced character. The characters of higher degrees are by our hypothesis all zero. The number of  forms $\varpi^{\lambda}$ independent between themselves and with respect to $\theta_{\alpha}$ and $\omega^{i}$ is equal to $s_{1}$. We can then suppose, by means of a suitable linear change of variables of $\omega^{i}$, that the rank of the system
\[
A_{\alpha1\lambda}\varpi^{\lambda}=0
\]
is equal to $s_{1}$ and, by a linear change of variables of the forms $\varphi_{\alpha}$, we have
\begin{equation}
  \label{eq:5.9}
  \left\{
    \begin{aligned}
      A_{\alpha 1 \lambda}\varpi^{\lambda}&\equiv \varpi^{\alpha}&(\alpha&=1,2,\dots,s_{1}),\\
      A_{\beta 1\lambda}\varpi^{\lambda}&=0&(\beta&=s_{1}+1,\dots,r).
    \end{aligned}
  \right.
\end{equation}

This being done, to say that the system is in involution with $s_{2}=s_{3}=\dots=s_{p}=0$ is to say that there exists exactly $(p-1)s_{1}$ relations between the coefficients of the equations
\begin{equation}
  \label{eq:5.10}
  \varpi^{\alpha}=t_{i}^{\alpha}\omega^{i}
\end{equation}
which give a generic $p$ dimensional integral element. These relations are necessarily
\begin{equation}
  \label{eq:5.11}
  t_{i}^{\alpha}=A_{\alpha i \lambda}t_{1}^{\lambda}\qquad(\alpha=1,2,\dots,s_{1};\ i=2,3,\dots,p).
\end{equation}

In particular, it follows from this that \emph{there is no more than $s_{1}$ linearly independent forms $\varphi_{\alpha}$} ($r=s_{1}$), since the consideration of the form $\varphi_{s_{1}+1}$, in which $\omega^{1}$ does not appear, gives
\[
A_{s_{1}+1,i,\lambda}t_{1}^{\lambda}=0\qquad(t=2,3,\dots,p),
\]
which introduces relations between the $t_{1}^{\lambda}$ that cannot be deduced from \eqref{eq:5.11}.

We now form the determinant of the polar matrix of the linear elements $(\omega^{i}{}_{\delta})$. This is a homogeneous form of degree $s_{1}$ in $\omega^{1},\omega^{2},\dots,\omega^{p}$. Suppose, without loss of generality, that for $\omega^{3}=\dots=\omega^{p}=0$ the determinant decompose into a product of $s_{1}$ distinct linear forms in $\omega^{1},\omega^{2}$. According to the argument in \textsection\textbf{96}, we see that we can, by means of a suitable linear change of variables on $\varphi_{\alpha}$ and $\varpi^{\alpha}$, suppose
\[
\varphi_{\alpha}\equiv[(\omega^{1}+m_{\alpha}\omega^{2})\varpi^{\alpha}]+A_{\alpha i \beta}[\omega^{i}\varpi^{\beta}]\qquad(\alpha=1,2,\dots,s_{1};\ i=3,4,\dots,p).
\]

From this we deduce that by expressing that the forms $\varpi^{\alpha}=t_{i}^{\alpha}\omega^{i}$ annihilate the forms $\varphi_{\alpha}$, in particular in the forms the coefficients of $[\omega^{2}\omega^{i}]$ is zero,
\[
m_{\alpha}t_{i}^{\alpha}=A_{\alpha i\beta}t_{2}^{\beta},
\]
from which, taking into considerations of the values \eqref{eq:4.11} of $t_{2}^{\beta}$ and $t_{i}^{\alpha}$,
\[
m_{\alpha}A_{\alpha i\beta}t_{1}^{\beta}=A_{\alpha i \beta}m_{\beta}t_{1}^{\beta},
\]
from which finally
\[
A_{\alpha i \beta}=0\qquad\text{for}\qquad \alpha\neq \beta.
\]

By putting $A_{\alpha i\alpha}=m_{i\alpha}$ and, for symmetry reason, $m_{\alpha}=m_{2\alpha}$, we have finally
\begin{equation}
  \label{eq:5.12}
  \varphi_{\alpha}\equiv[(\omega^{1}+m_{i\alpha}\omega^{i})\varpi^{\alpha}]=0.
\end{equation}

This is the remarkable form in which we can bring the quadratic exterior equations of the given differential system into.

\vspace{12pt}\fsec The characteristic lines of the integral manifolds are therefore those that are, at each of their point, tangent to the $s_{1}$ plane elements of $(p-1)$ dimensions defined by the equations
\[
\omega^{1}+m_{i\alpha}\omega^{i}=0\qquad(\alpha=1,2,\dots,s_{1}).
\]

These $p-1$ dimensional elements enjoy a remarkable property, which is a consequence of the following theorem:
\begin{thm*}
  On a given integral manifold, each of the equations
\[
\omega^{1}+m_{i\alpha}\omega^{i}=0
\]
is completely integrable.
\end{thm*}

Before we proceed with the proof, we will for brevity write
\begin{equation}
  \label{eq:5.13}
  \omega^{1}+m_{i\alpha}\omega^{i}=\overline\omega^{\alpha}\qquad(\alpha=1,2,\dots,s_{1}).
\end{equation}
The $\overline\omega^{\alpha}$ are $s_{1}$ distinct forms in $\omega^{1},\omega^{2},\dots,\omega^{p}$ which naturally cannot be linearly independent.

With this notation in place, we first remark that on an integral manifold, we have the relation of the form
\begin{equation}
  \label{eq:5.14}
  \varpi^{\alpha}=t^{\alpha}\overline\omega^{\alpha},
\end{equation}
which follows from \eqref{eq:5.12}. On the other hand the equation $d\varphi_{\alpha}=0$ is a consequence of the equations of the given differential system, which is closed. The exterior derivative $d\varphi_{\alpha}$ is therefore identically zero when we take into consideration of the equations $\theta_{\beta}=0$ and when we replace $\varpi^{\lambda}$ by $t^{\lambda}\overline\omega^{\lambda}$ (the equations of the system which are of degrees more than $2$ are identically satisfied under the preceding conditions).

\emph{We now fix the index $\alpha$.} We have, by $\varphi_{\alpha}=[\overline\omega^{\alpha}\varpi^{\alpha}]$,
\begin{equation}
  \label{eq:5.15}
  d\varphi_{\alpha}=[d\overline\omega^{\alpha}\varpi^{\alpha}]-[\overline\omega^{\alpha}d\varpi^{\alpha}].
\end{equation}
Suppose we have, taking into consideration of the equations $\theta_{\beta}=0$ \footnote{Naturally the coefficients $a_{ij},a_{i\lambda},\dots$ of the equations \eqref{eq:5.16} vary with the index $\alpha$ remaining fixed.},
\begin{equation}
  \label{eq:5.16}
  \left\{
    \begin{aligned}
      d\overline\omega^{\alpha}&=\frac{1}{2}a_{ij}[\omega^{i}\omega^{j}]+a_{i\lambda}[\omega^{i}\varpi^{\lambda}],\\
      d\varpi^{\alpha}&=\frac{1}{2}c_{ij}[\omega^{i}\omega^{j}]+c_{i\lambda}[\omega^{i}\varpi^{\lambda}]+\frac{1}{2}c_{\lambda\mu}[\varpi^{\lambda}\varpi^{\mu}],
    \end{aligned}
  \right.
\end{equation}
the exterior derivative $d\overline\omega^{\alpha}$ does not contain the terms in $[\varpi^{\lambda}\varpi^{\mu}]$ because the equations $\omega^{1}=\omega^{2}=\dots=\omega^{p}=0$ form a completely integrable system \footnote{The forms $\omega^{i}$ are independent linear combinations of $dx^{1},dx^{2},\dots,dx^{p}$.}.

We have, according to \eqref{eq:5.15} and \eqref{eq:5.16},
\begin{align}
  \label{eq:5.17}
  d\varphi_{\alpha}=\frac{1}{2}a_{ij}[\omega^{i}\omega^{j}\varpi^{\alpha}]&+a_{i\lambda}[\omega^{i}\varpi^{\lambda}\varpi^{\alpha}]-\frac{1}{2}c_{ij}[\omega^{i}\omega^{j}\overline\omega^{\alpha}]\\
  &+c_{i\lambda}[\omega^{i}\overline\omega^{\alpha}\varpi^{\lambda}]-\frac{1}{2}c_{\lambda\mu}[\overline\omega^{\alpha}\varpi^{\lambda}\varpi^{\mu}].\notag
\end{align}
We therefore must have, \emph{regardless of the arbitrary parameters $t^{\lambda}$},
\begin{align*}
  \frac{1}{2}t^{\alpha}a_{ij}[\omega^{i}\omega^{j}\overline\omega^{\alpha}]+t^{\alpha}t^{\lambda}a_{i\lambda}[\omega^{i}\overline\omega^{\lambda}\overline\omega^{\alpha}]&-\frac{1}{2}[\omega^{i}\omega^{j}\overline\omega^{\alpha}]+t^{\lambda}c_{i\lambda}[\omega^{i}\overline\omega^{\alpha}\overline\omega^{\lambda}]\\
&-\frac{1}{2}t^{\lambda}t^{\mu}c_{\lambda\mu}[\overline\omega^{\alpha}\overline\omega^{\lambda}\overline\omega^{\mu}]=0.
\end{align*}

Equating to zero the coefficients of $t^{\alpha}$ and of $t^{\alpha}t^{\beta}$, where we suppose $\beta\neq\alpha$, we obtain
\[
a_{ij}[\omega^{i}\omega^{j}\overline\omega^{\alpha}]=0,\qquad a_{i\beta}[\omega^{i}\overline\omega^{\beta}\overline\omega^{\alpha}]=0,
\]
in the second set of equations there is no summation with respect to $\beta$. These equations express that \emph{on every integral manifold} the form $d\overline\omega^{\alpha}$ vanishes by taking into consideration $\overline\omega^{\alpha}=0$. The last equation is therefore completely integrable.\qed


\vspace{12pt}\fsec The preceding theorem can be stated in the following form:
\begin{thm*}
  Every differential system in involution in $p$ independent variables whose general solution depend only on $s_{1}$ arbitrary functions of one variable admit in general $s_{1}$ families of $p-1$ dimensional characteristic manifolds such that there passes one and only one manifold of each family for a point on the integral manifold. All curves on one of these characteristic manifolds are themselves characteristics.
\end{thm*}

\begin{rmk*}
  If the $s_{1}$ families of linear characteristic elements are not distinct, the form resulting from \eqref{eq:5.12}, the exterior quadratic equations of the differential system, will become less simple. We can show that the polar matrix of $s_{1}$ rows and $s_{1}$ columns can be reduced in a manner that all its elements above the principal diagonal is zero. This means that \emph{the determinant of the polar matrix decomposes into a product of $s_{1}$ linear forms in $\omega^{1},\omega^{2},\dots,\omega^{p}$} \footnote{Once we make the forms $A_{\alpha1\lambda}\varpi^{\lambda}$ into $\varpi^{\alpha}$, we can regard the $s_{1}^{2}$ coefficients $A_{\alpha i\beta}$, where $i$ has a fixed value greater than $1$, as the elements of a matrix $S_{i}$. The involution condition for the system, which are expressed by the fact that the relations
\[
A_{\alpha i\lambda}t^{\lambda}_{j}=A_{\alpha j\lambda}t^{\lambda}_{i}
\]
are consequences of the equations \eqref{eq:5.11}, where the $t_{1}^{\lambda}$ are not constrained by any relation, is simply equivalent to the property of the $p-1$ matrices $S_{2},\dots,S_{p}$ are exchangeable between themselves. From the theory of exchangeable matrices it follows that the characteristic equation of the matrix $u^{2}S_{2}+u^{3}S_{3}+\dots+u^{p}S_{p}$, where the $u^{i}$ are parameters, has all its roots linear in $u^{2},u^{3},\dots,u^{p}$. As the matrix $S_{1}$ is the unit matrix, this means that the determinant of the matrix $u^{1}S_{1}+u^{2}S_{2}+\dots+u^{p}S_{p}$ is the product of the linear forms in $u^{1},u^{2},\dots,u^{p}$. By replacing $u^{i}$ by $\omega^{i}$, we obtain the result of the text.
}.

We will end this chapter with an application of the theory of systems in involution to Pfaffian systems.

\end{rmk*}


\section{A theorem of J.-A. Schouten and van der Kulk}
\label{sec:theorem-j.-a}

\fsec This theorem is concerned with a generic system of linear total derivatives (Pfaffian system). Let
\begin{equation}
  \label{eq:5.18}
  \theta_{\alpha}=0\qquad(\alpha=0,1,2,\dots,q)
\end{equation}
be a system of $q+1$ linearly independent equations in $n$ variables $x^{1},x^{2},\dots,x^{n}$, which contain both dependent and independent variables. \textsc{F.~Engel} has attracted attention by his new numeric invariant of Pfaffian system, which is, in the present case, the largest integer $m$ such that the exterior form of degree $2m+q+1$
\begin{equation}
  \label{eq:5.19}
  [\theta_{0}\theta_{1}\dots\theta_{q}(\lambda_{0}d\theta_{0}+\lambda_{1}d\theta_{1}+\dots+\lambda_{q}d\theta_{q})^{m}]
\end{equation}
does not vanish identically, where $\lambda_{0},\lambda_{1},\dots,\lambda_{q}$ are arbitrary parameters.

The theorem in question can be stated in the following manner:
\begin{thm*}
  If $m$ is the Engel invariant of system \eqref{eq:5.18}, it is possible to find an algebraically equivalent system where the left hand sides are all of class $2m+1$.
\end{thm*}

It is clear that if $2m+q$ is equal or greater than $n$, the form \eqref{eq:5.19} is identically zero. Hence we surely have $2m\le n-q$.

\vspace{12pt}\fsec To put into equations the problem of the determination of an algebraically equivalent system of a given system enjoying the property indicated, suppose, as it is always permissible, that the form
\[
[\theta_{0}\theta_{1}\dots\theta_{q}(d\theta_{0})^{m}]
\]
does not vanish identically, and write
\[
\Theta=\theta_{0}+u^{1}\theta_{1}+u^{2}\theta_{2}+\dots+u^{q}\theta_{q},
\]
where $u^{1},u^{2},\dots,u^{q}$ denote $q$ unknown functions in the variables $x^{1},x^{2},\dots,x^{n}$.

We are going to determine the unknown functions such that the form $\Theta$ is of class $2m+1$. This condition is expressed by the equation of degree $2m+3$
\begin{equation}
  \label{eq:5.20}
  [\Theta(d\Theta)^{m+1}]=0
\end{equation}
to which we should adjoin the equation obtained by exterior differentiation, namely
\begin{equation}
  \label{eq:5.21}
  [(d\Theta)^{m+2}]=0.
\end{equation}

By the hypothesis, if we regard $u^{1},u^{2},\dots,u^{p}$ as constants, the exterior derivative $d\Theta$ is reducible$\pmod{\theta_{0},\theta_{1},\dots,\theta_{q}}$ to
\[
[\omega^{1}\omega^{2}]+[\omega^{3}\omega^{4}]+\dots+[\omega^{2m-1}\omega^{2m}],
\]
where the $\omega^{i}$ are $2m$ linearly independent differential forms constructed with the variables $x^{i}$ and their differentials, the coefficients can naturally depend on $u^{1},u^{2},\dots,u^{q}$.

We then have, by regarding now the $u^{i}$ as variables,
\begin{equation}
  \label{eq:5.22}
  d\Theta\equiv[\omega^{1}\omega^{2}]+\dots+[\omega^{2m-1}\omega^{2m}]+[\theta_{1}\varpi^{1}]+\dots+[\theta_{q}\varpi^{q}]\pmod{\Theta},
\end{equation}
where $\varpi^{i}+du^{i}$ are linear forms in $dx^{1},dx^{2},\dots,dx^{n}$.



\vspace{12pt}\fsec We will first find the $n$ dimensional generic integral elements of the system \eqref{eq:5.20}, \eqref{eq:5.21}. The equation \eqref{eq:5.20} express that $d\Theta$ is$\pmod{\Theta}$ reducible to an exterior quadratic form constructed with $2m$ independent linear forms, which is to say that the right hand side of the congruence \eqref{eq:5.22} is, when we replace the $\varpi^{i}$ by their values, reducible$\pmod{\Theta}$ to
\[
[\overline\omega^{1}\overline\omega^{2}]+[\overline\omega^{3}\overline\omega^{4}]+\dots+[\overline\omega^{2m-1}\overline\omega^{2m}],
\]  
where $\overline\omega^{i}$ is the sum of $\omega^{i}$ and a linear combination of the forms $\theta_{1},\theta_{2},\dots, \theta_{q}$ and of $n-2m-q-1$ other forms independent of $\omega^{i}$ and $\theta_{\alpha}$, which we write as $\omega^{2m+1},\dots,\omega^{n-q-1}$. But we see immediately that the right hand side of \eqref{eq:5.22} is incompatible with the presence of the last $n-q-1$ forms. We therefore finally have a congruence
\begin{align}
  \label{eq:5.23}
  d\Theta\equiv[(\omega^{1}&+a^{1\alpha}\theta_{\alpha})(\omega^{2}+a^{2\alpha}\theta_{\alpha})\\
  &+\dots+[(\omega^{2m-1}+a^{2m-1,\alpha}\theta_{\alpha})(\omega^{2m}+a^{2m,\alpha}\theta_{\alpha})]\pmod{\Theta}.\notag
\end{align}

It follows, by taking the coefficients of $\theta_{1},\theta_{2},\dots,\theta_{q}$ on the right hand side expanded,
\begin{align}
  \label{eq:5.24}
  \varpi^{\alpha}=a^{1\alpha}\omega^{2}-a^{2\alpha}\omega^{1}+\dots&+a^{2m-1,\alpha}\omega^{2m}-a^{2m,\alpha}\omega^{2m-1}\\
  &+b^{\alpha\lambda}\theta_{\lambda}+c^{\alpha}\Theta\qquad(\alpha=1,2,\dots,q).\notag
\end{align}

The identification with \eqref{eq:5.22} gives
\begin{align}
  \label{eq:5.25}
  b^{\alpha\beta}-b^{\beta\alpha}=a^{1\alpha}a^{2\beta}&-a^{1\beta}a^{2\alpha}+\dots+a^{2m-1,\alpha}a^{2m,\beta}\\
  &-a^{2m,\alpha}a^{2m-1,\beta}\qquad(\alpha,\beta=1,2,\dots,q).
  \notag
\end{align}

The $n$ dimensional element defined by the equations \eqref{eq:5.24}, where the coefficients satisfy the relations \eqref{eq:5.25}, satisfies equation \eqref{eq:5.20}. It then also satisfies automatically equation \eqref{eq:5.21}, since the form $d\Theta$, according to \eqref{eq:5.20}, being the sum of at most $m+1$ independent quadratic monomials, its $(m+2)$-th power is identically zero.

The number of independent parameters that the most general $n$ dimensional integral element of the closed system \eqref{eq:5.20}, \eqref{eq:5.21} is then equal to the number $2mq$ of the parameters $a^{i\alpha}$, added to the number $q(q+1)/2$ of independent parameters $b^{\alpha\beta}$ [which are $q^{2}$ parameters constrained by the $q(q-1)/2$ relations \eqref{eq:5.25}], and finally added to with the $q$ parameters $c^{\alpha}$, which gives the total number of independent parameters
\begin{equation}
  \label{eq:5.26}
  2mq+\frac{q(q+3)}{2}.
\end{equation}



\vspace{12pt}\fsec We now come to the determination of the reduced characters of the differential system. The equation \eqref{eq:5.20} being of degree $2m+3$, all $2m+1$ dimensional elements are integral. We therefore have
\[
s_{0}=s_{1}=s_{2}=\dots=s_{2m+1}=0.
\]

We now consider the following chain of integral elements $E_{2m+2},\dots,E_{n-1}$, which successively introduce between the differentials of the $n$ variables the following relations:
\begin{alignat*}{5}
  E_{2m+2}&:\omega^{2m+1}=\dots&=\omega^{n-q-1}&=\theta^{2}&=\theta^{3}=\dots=\theta_{q}&=0;\\
  E_{2m+3}&:\omega^{2m+1}=\dots&=\omega^{n-q-1}&&=\theta^{3}=\dots=\theta_{q}&=0;\\
  &\dots&&&&\dots\\
  E_{2m+q}&:\omega^{2m+1}=\dots&=\omega^{n-q-1}&&&=0;\\
  &\dots&&&&\dots\\
  E_{2m+q}&:&\quad\ \omega^{n-q-1}&&&=0;\\
\end{alignat*}

To form the reduced polar system of each of the integral elements, we need only the equation \eqref{eq:5.20}. We then obtain the ranks $\sigma_{2m+2},\sigma_{2m+2}+\sigma_{2m+3},\dots$, which will be at most equal to the rank $s_{2m}+2,s_{2m+2}+s_{2m}+3,\dots$. According to the sufficient criteria in \textsection\textbf{29}, the given differential system will be certainly in involution if the number \eqref{eq:5.26} of independent parameters of a generic $n$ dimensional integral element is equal to
\[
(2m+2)\sigma_{2m+2}+(2m+3)\sigma_{2m+3}+\dots+n\sigma_{n}.
\]



\vspace{12pt}\fsec To form the reduced polar system of $E_{2m+2}$, for which we hav
\[
\omega^{2m+1}=\omega^{2m+2}=\dots=\omega^{n-q-1}=0,\qquad\theta_{2}=\theta_{3}=\dots=\theta_{q}=0,
\]
it suffices, in calculating $(d\Theta)^{m+1}$, to take into consideration only the terms in $\omega^{1},\omega^{2},\dots,\omega^{2m},\theta^{1}$, which reduce $(d\Theta)^{m+1}$, up to a numerical factor, to the form $[\omega^{1}\omega^{2}\dots\omega^{2m}\theta_{1}\varpi^{1}]\pmod{\Theta}$. The reduce polar system is therefore simply the equation $\varpi^{1}=0$ and we have
\[
\sigma_{2m+2}=1.
\]

The calculation of the reduced polar system of $E_{2m+3}$ is done by reducing $d\Theta\pmod{\Theta}$ to
\[
[\omega^{1}\omega^{2}]+\dots+[\omega^{2m-1}\omega^{2m}]+[\theta_{1}\varpi^{1}]+[\theta_{2}\varpi^{2}],
\]
from which the equation
\[
\varpi^{1}=0,\qquad\varpi^{2}=0,
\]
we have therefore $\sigma_{2m+2}+\sigma_{2m+3}=2$, from which
\[
\sigma_{2m+3}=1.
\]

We continue in this way step by step to find
\[
\sigma_{2m+4}=\dots=\sigma_{2m+q+1}=1.
\]
As the sum of the $\sigma$ already calculated is equal to $q$, the number of unknown functions, all the following $\sigma$ are zero.

An immediate calculation now gives
\begin{align*}
  (2m+2)\sigma_{2m+2}+\dots+n\sigma&=2mq+(2+3+\dots+q+1)\\
  &=2mq+\frac{q(q+3)}{2}.
\end{align*}

This result shows that \emph{the system is in involution and its general solution depend on an arbitrary function of $2m+q+1$ variables}.

As there always exists one solution (and even an infinite number of solutions) for which the unknown functions $u^{1},u^{2},\dots,u^{q}$ can take, for the given numerical values $(x^{i})_{0}$ of the variables $x^{i}$, the arbitrarily given numerical values, we can find $q+1$ particular solutions such that, for $x^{i}=(x^{i})_{0}$, the $q+1$ corresponding forms $\Theta$ are linearly independent in $dx^{1},dx^{2},\dots,dx^{n}$. The theorem is thus proved.

\chapter[Prolongation of differential system]{Prolongations of\\differential system}
\label{cha:prol-diff-syst}

\section{A fundamental problem}
\label{sec:fundamental-problem}

\fsec In the previous chapters we have proved the existence theorems for certain differential systems with imposed independent variables, which we called systems in involutions. The solutions of these system whose existence we have showed by application of the Cauchy-Kowalewski theorem are those that constitute the \emph{general solution} of the system under consideration. However we know that there can also exist other solutions, which are the \emph{singular solutions}, given by new differential systems each of which is obtained by adjoining new relations between the dependent variables to the given systems of equations, and the new system is in general not in involution. A fundamental problem therefore consists of knowing what information we can have for the solutions of a system not in involution, in particular, \emph{given a particular solution of a given differential system, can the solution be obtained as a non-singular solution of a system in involution which may be deducted from the given system by a systematic procedure}?

We will try to answer this question in the present chapter. The systematic procedure which we mentioned concerns the notion of \emph{prolongation} of a differential system, which we will introduce in the following section.

\vspace{12pt}\fsec \emph{Reduction in the case of a linear Pfaffian system}. We can always, for sake of convenience, suppose that the given system contain only algebraic equations between the dependent and independent variables and linear differential equations with respect to both the dependent and independent variables. Indeed (\textsection\textbf{79}) every differential system can be regarded as a system of first order differential equations of $n-p$ unknown functions $z^{\lambda}$ in $p$ independent variables $x^{1},x^{2},\dots,x^{p}$. From this point of view we can write
\begin{equation}
  \label{eq:6.1}
  \left\{
    \begin{aligned}
      F_{\alpha}(x^{i},z^{\lambda},t_{i}^{\lambda})&=0&(i=1,2,\dots,p;\ \lambda&=1,2,\dots,n-p;\ \alpha=1,2,\dots,r_{0}),\\
      dz^{\lambda}-t_{i}^{\lambda}dx^{i}&=0&(\lambda&=1,2,\dots,n-p).
    \end{aligned}
  \right.
\end{equation}
It will be convient to adjoin to these equations those that can be deduced by exterior differentiation, that is to say
\begin{equation}
  \label{eq:6.2}
  \left\{
    \begin{aligned}
      \frac{\pd F_{\alpha}}{\pd x^{i}}dx^{i}+\frac{\pd F_{\alpha}}{\pd z^{\lambda}}dz^{\lambda}+\frac{\pd F_{\alpha}}{\pd t_{i}^{\lambda}}dt_{i}^{\lambda}&=0&&(\alpha=1,2,\dots,r_{0}),\\
    [dx^{i}dt_{i}^{\lambda}]&=0&&(\lambda=1,2,\dots,n-p).
  \end{aligned}
  \right.
\end{equation}

We now change notation: we denote by $\nu$ the total number of dependent variables $z^{\lambda}, t_{i}^{\lambda}$ which appear in the equations \eqref{eq:6.1} and \eqref{eq:6.2}. The closed system \eqref{eq:6.1}, \eqref{eq:6.2} of $\nu$ unknown functions, which we now denote together with the general notation $z^{\lambda}$ $(\lambda=1,2,\dots,\nu)$ can be written under the form
\begin{equation}
  \label{eq:6.3}
  \left\{
    \begin{aligned}
      F_{\alpha}(x,z)&=0&(\alpha&=1,2,\dots,r_{0}),\\
      \theta_{\alpha}&=0&(\alpha&=1,2,\dots,r_{1}),\\
      \varphi_{\alpha}\equiv C_{\alpha ij}[dx^{i}dx^{j}]+A_{\alpha ij}[dx^{i}\varpi^{\lambda}]&=0&(\alpha&=1,2,\dots,r_{r}),
    \end{aligned}
  \right.
\end{equation}

The $\theta_{\alpha}$ are linear forms in $dx^{1},dx^{2},\dots,dx^{p},dz^{1},dz^{2},\dots,dz^{\nu}$, constituted by the left hand sides of the last equations of \eqref{eq:6.1} and the first equations of \eqref{eq:6.2} or by independent linear combinations of them. The $\varphi_{\alpha}$ are the left hand sides of the last equations of \eqref{eq:6.2} or independent linear combinations of them. The $\varpi^{\lambda}$ are  linear differential forms which together with the $\theta_{\alpha}$ consist a system of $\nu$ independent Pfaffian forms with respect to $dz^{1},dz^{2},\dots,dz^{\nu}$. It may be useful to replace the differentials $dx^{1},dx^{2},\dots,dx^{p}$ with a system of $p$ independent linear combinations of these differentials, which we denote by $\omega^{1},\omega^{2},\dots,\omega^{p}$, the coefficients of $\omega^{i}$ may depend on the dependent as well as the independent variables.



\vspace{12pt}\fsec \emph{Remark}. We can suppose the $r_{1}$  forms $\theta_{\alpha}$ to be independent, that is to say that the rank of the linear system $\theta_{\alpha}=0$ is equal to $r_{1}$. Of course this supposes that if we are on a \emph{generic} point $(x,z)$, the $x^{i}$ and the $z^{\lambda}$ satisfy the algebraic equations of system \eqref{eq:6.3}; such a point is a \emph{regular integral point}. We will be only concerned with the integral manifolds of system \eqref{eq:6.3} whose generic points are regular. The other will be solutions of another system which can be deduced from the system \eqref{eq:6.3} by adjoining algebraic relations which express that the rank of the system $\theta_{\alpha}=0$ have a given value less than $r_{1}$.


\section{Prolongations of differential system}
\label{sec:prol-diff-syst}

\fsec The operation of the prolongation of a differential system is fundamentally identical to the operation consisting of, giving a system of partial differential equations, adjoining to these equations the system that can be deduced by applying partial or total derivations with respect to one or more independent variables. Consider the system \eqref{eq:6.3} and form the general equations which given the $p$ dimensional integral elements
\begin{equation}
  \label{eq:6.4}
  \varpi^{\lambda}=t_{i}^{\lambda}dx^{i}\qquad(\lambda=1,2,\dots,\nu-r_{1}),
\end{equation}
these equations are
\begin{equation}
  \label{eq:6.5}
  H_{\alpha ij}\equiv C_{\alpha ij}+A_{\alpha i\lambda}t_{j}^{\lambda}-A_{\alpha j\lambda}t_{i}^{\lambda}\qquad(\alpha=1,2,\dots,r_{2};\ i,j=1,2,\dots,p).
\end{equation}

We can regard $t_{i}^{\lambda}$ as new unknown functions subject to equations \eqref{eq:6.5}. We will have a prolongation of system \eqref{eq:6.3} by adjoining to them the algebraic equations \eqref{eq:6.5}, the linear equations \eqref{eq:6.4} and the equations following from \eqref{eq:6.5} and \eqref{eq:6.4} by exterior differentiation. Observe that in the new system thus obtained we can suppress the exterior quadratic equations $\varphi_{\alpha}=0$ which appear in the initial system \eqref{eq:6.3}, since they are algebraic consequences of equations \eqref{eq:6.4}.

We can also effect a partial prolongation by adjoining only some of the equations \eqref{eq:6.4}.

We can show that if the system \eqref{eq:6.3} is in involution, it is the same as the system of its total prolongation obtained by the preceding procedure \footnote{\textsc{E.~Cartan}, \emph{Sur la structure des groupes infinis de transformations}, Chapter 1 (Annales \'Ecole normale sup., \textbf{21}, 1904, pp.~154--175, especially pp.~166--171, \textsection{}7--9).}. However we will have no need of this theorem. The theorem can be false if we only apply a partial prolongation.

\vspace{12pt}\fsec Suppose that the given system \eqref{eq:6.3} is not in involution. Several cases appear.

\vspace{12pt}\textsc{First case}: \emph{the equations \eqref{eq:6.5} that furnish the $p$ dimensional integral elements of a generic integral point are incompatible.} In this case, the compatibility of equations \eqref{eq:6.5} contain relations between the coordinates $x^{i},z^{\alpha}$ of the origin of the integral element.

If these equations contain relations between the independent variables, or if they contain as consequences that the rank of the system $\theta_{\alpha}=0$ is less than $r_{1}$, the problem we have posed does not admit any solution.

If none of these two kinds of relations results, we must adjoin to equations \eqref{eq:6.3} the algebraic relations between the dependent and independent variables expressing the compatibility of equations \eqref{eq:6.5} as well as the equations that result from exterior differentiation. The numbers $r_{0}$ and $r_{1}$ are hence augmented, but the quadratic equations $\varphi_{\alpha}=0$ do not change. We have hence a new system containing the same dependent and independent variables as the original system, with an increase of the integers $r_{0}$ and $r_{1}$: in particular \emph{the integer $\nu-r_{1}$ has decreased}.

\vspace{12pt}\textsc{Second case}: \emph{the equations \eqref{eq:6.5} are compatible at every regular integral point of the space, but the system is not in involution}. In this case we will prolong this system as explained in \textsection\textbf{109}, and we will obtain a new system containing new dependent variables. With respect to the old system, there will be an increase of the integer $r_{0}$ because we will be adding  the algebraic relations \eqref{eq:6.5} between the independent and dependent variables, relations expressing that the $p$ dimensional element \eqref{eq:6.4} is integral. The integer $r_{1}$ will also increase by the addition of the equations \eqref{eq:6.4} and the equations resulting from differentiation of \eqref{eq:6.5}. As for the quadratic equations of the new system, they will no longer contain the equations $\varphi_{\alpha}=0$ of the old system if we effect a complete prolongation of the system, but we must adjoin the equations resulting from exterior differentiation of the equations \eqref{eq:6.4}. If the prolongation is only partial, certain equations of $\varphi_{\alpha}=0$ may be preserved.


\vspace{12pt}\fsec From the preceding discussion we see that if the given system is not in involution, we have a systematic procedure to deduce from it new systems admitting the same solutions as the given system. We can show that, \emph{under certain conditions which is not easy to make precise at this moment},  we will arrive at a system in involution.

We will not stop to prove the general case and we are just going to show how we can prove the most simple case, the case of two independent variables. The proof that we are going to give does not extend to the case of arbitrary number of independent variables.


\section{The case of two independent variables}
\label{sec:case-two-independent-1}

\fsec We denote by $x$ and $y$ the independent variables. We continue to use our previous notations. We suppose, for simplicity, that in the quadratic equations $\varphi_{\alpha}=0$, which we write as
\begin{equation}
  \label{eq:6.6}
  \varphi_{\alpha}\equiv C_{\alpha}[dx\,dy]+A_{\alpha\lambda}[dx\,\varpi^{\lambda}]+B[dy\,\varpi^{\lambda}]=0,
\end{equation}
we got rid of the forms $\theta_{\alpha}$, by means of adjoining to $\varphi_{\alpha}$ a quadratic form congruent to zero$\pmod{\theta_{1},\theta_{2},\dots,\theta_{r_{1}}}$. We all $\rho$ the difference of $\nu-r_{1}$,  so that the forms $\varphi_{\alpha}$ contain only $\rho$ forms $\varpi^{1},\varpi^{2},\dots,\varpi^{\rho}$ independent among themselves and with respect to $\theta_{\alpha}$.

We are going to state how we will prolong the system, when the equations \eqref{eq:6.5} are compatible and the system is not in involution. As we are going to indicate, we will only effect a partial prolongation.



\vspace{12pt}\fsec Recall the criterion for involution stated in \textsection\textbf{94} in the case where on a generic integral point there passes at least a two dimensional integral element. The necessary and sufficient condition for involution is that the reduced character $s_{1}$ is equal to the number of linearly independent quadratic forms $\varphi_{\alpha}$. In our case we can suppose the coefficients $C_{\alpha}$ of the formulae \eqref{eq:6.6} to vanish. By substituting the differentials $dx,dy$ with two independent linear combinations $\omega^{1},\omega^{2}$ and then writing
\[
\varphi_{\alpha}\equiv A_{\alpha\lambda}[\omega^{1}\varpi^{\lambda}]+B_{\alpha\lambda}[\omega^{2}\varpi^{\lambda}],
\]
we can suppose that the linear integral element $\omega^{2}=0$ is regular, such that $s_{1}$ is the number of forms $A_{\alpha\lambda}\varpi^{\lambda}$ which are independent. We can, by a change of writing, suppose that we have $A_{\alpha\lambda}\varpi^{\lambda}\equiv\varpi^{\alpha}$, such that we have
\begin{equation}
  \label{eq:6.7}
  \varphi_{\alpha}\equiv[\omega^{1}\varpi^{\alpha}]+B_{\alpha\lambda}[\omega^{2}\varpi^{\lambda}]\qquad(\alpha=1,2,\dots,s_{1}).
\end{equation}

If the system is not in involution there exists the independent forms $\varphi_{\alpha}$ of the $s_{1}$ preceding forms, but the coefficients of $\omega_{1}$ in these forms are linear combinations of $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$. We can therefore suppose that the form $\varphi_{s_{1}+1}$ for example does not contain terms in $\omega^{1}$. \emph{We are going to show that the coefficient of $\omega^{2}$ is a linear combination of the $s_{1}$ forms $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$}.

Indeed, suppose for example we have
\[
\varphi_{s+1}\equiv[\omega^{2}\varpi^{s_{1}+1}],
\]
it is easy to see that then the first reduced character is greater that $s_{1}$, since the first $s+1$  reduced equations of the polar element of the linear integral element $(\omega^{1}=1,\omega^{2}=m)$ will be
\begin{align*}
  \varpi^{\alpha}+mB_{\alpha\lambda}\varpi^{\lambda}&=0&(\alpha=1,2,\dots,s_{1}),\\
  m\varpi^{s_{1}+1}&=0,
\end{align*}
but if we give $m$ a sufficiently small value the rank of this system is $s_{1}+1$, so that the first character is at least equal to $s_{1}+1$.



\vspace{12pt}\fsec Following the about reasoning, we can suppose, by a linear change of variables of the $s_{1}$ forms $\varphi_{1},\varphi_{2},\dots,\varphi_{s_{1}}$, that $\varphi_{s_{1}+1}$ is a non-zero multiple of $\varpi^{1}$, so that we can write
\begin{equation}
  \label{eq:6.8}
  \varphi_{s_{1}+1}\equiv[\omega^{2}\varpi^{1}].
\end{equation}

We are going to show that the coefficient of $\omega^{2}$ in $\varphi_{1}$ depend only on $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$. In deed the reduced polar system of the linear integral element $(\omega^{1}=1,\omega^{2}=m)$ contains the equations
\begin{align*}
  \varpi^{1}+mB_{1\lambda}\varpi^{\lambda}&=0,\\
  \varpi^{\alpha}+mB_{\alpha\lambda}\varpi^{\lambda}&=0,&(\alpha=2,3,\dots,s_{1}),\\
  \varpi^{1}=0,
\end{align*}
the system, when $m$ tends to zero, tend to the system
\[
B_{1\lambda}\varpi^{\lambda}=0,\qquad\varpi^{2}=0,\qquad\dots\qquad\varpi^{s_{1}}=0,\quad\varpi^{1}=0,
\]
as the rank is equal to $s_{1}$, this implies that the form $B_{1\lambda}\varpi^{\lambda}$ can only contain terms in $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$.

We can suppose, by means of a linear change of variables for the $\varpi^{\alpha}$ $(\alpha\le s_{1})$, that the coefficient of $\omega^{2}$ in $\varphi_{1}$ is equal to $\varpi^{2}$:
\[
\varphi_{1}\equiv[\omega^{1}\varpi^{1}]+[\omega^{2}\varpi^{2}].
\]

We continue the argument. The coefficient of $\omega^{2}$ in $\varphi^{2}$ must be a linear combination of the $s_{1}$ forms $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$. If this combination is independent of $\varpi^{1}$ and $\varpi^{2}$, we can suppose that it is equal to $\varpi^{3}$ and so on. Ultimately we will have the coefficient of $\omega^{2}$ in the successive forms $\varphi_{1},\varphi_{2},\dots$ depend only on previously encountered forms. We have, for example,
\begin{align*}
  \varphi_{1}&\equiv[\omega^{1}\varpi^{1}]+[\omega^{2}\varpi^{2}],\\
  \varphi_{2}&\equiv[\omega^{1}\varpi^{2}]+[\omega^{2}\varpi^{3}],\\
  &\dots\\
  \varphi_{h-1}&\equiv[\omega^{1}\varpi^{h-1}]+[\omega^{2}\varpi^{h}],\\
  \varphi_{h}&\equiv[\omega^{1}\varpi^{h}]+B_{1}[\omega^{2}\varpi^{1}]+B_{2}[\omega^{2}\varpi^{2}]+\dots+B_{h}[\omega^{2}\varpi^{h}],\\
  \varphi_{s_{1}+1}&\equiv[\omega^{1}\varpi^{1}].
\end{align*}

With this, we can deduce, for all two dimensional integral elements,
\begin{equation}
  \label{eq:6.9}
  \left\{
    \begin{aligned}
      \varpi^{1}&=t_{1}\omega^{2},\\
      \varpi^{2}&=t_{1}\omega^{1}+t_{2}\omega^{2},\\
      \varpi^{3}&=t_{2}\omega^{1}+t_{3}\omega^{2},\\
      &\dots\\
      \varpi^{h}&=t_{h-1}\omega^{1}+t_{h}\omega^{2},
    \end{aligned}
  \right.
\end{equation}
with the relation
\begin{equation}
  \label{eq:6.10}
  t_{h}=B_{2}t_{1}+B_{3}t_{2}+\dots+B_{h}t_{h-1}.
\end{equation}


\vspace{12pt}\fsec After we obtain this first result, let us effect a \emph{partial} prolongation of the given differential system, a prolongation that introduces $h-1$ new unknown functions $t_{1},t_{2},\dots,t_{h-1}$. We then need to adjoin to the linear differential equations $\theta_{\alpha}=0$ of the given system the new independent linear equations
\begin{equation}
  \label{eq:6.11}
  \left\{
    \begin{aligned}
      \varpi^{1}-t_{1}\omega^{2}&=0,\\
      \varpi^{2}-t_{1}\omega^{1}-t_{2}\omega^{2}&=0,\\
      \varpi^{3}-t_{2}\omega^{1}-t_{3}\omega^{2}&=0,\\
      &\dots,\\
      \varpi^{h}-t_{h-1}\omega^{1}-(b_{2}t_{1}+B_{3}t_{2}+\dots+B_{h}t_{h-1})\omega^{2}&=0.\\
    \end{aligned}
  \right.
\end{equation}

To the old quadratic equations, whose number will now be reduced to $h$, we will adjoin the $h$ quadratic equations resulting from the exterior differentiation of the equations \eqref{eq:6.11}. The fundamental result obtained by this prolongation is that \emph{the integer $\rho$ has decreased}: indeed the integer $r_{1}$ has increased by $h$, whereas the number $\nu$ of dependent variables has only increased by $h-1$, \emph{therefore the integer $\rho=\nu-r_{1}$ has effectively decreased by one.}

\vspace{12pt}\fsec In the analysis in the previous section there results in a systematic method for obtaining, starting from a differential system not in involution, a set of differential systems admitting the same solutions as the initial system. If at a given moment the system obtained is incompatible, the same can be said about the initial system; if the system obtained does not admit two dimensional integral elements having as its origin a generic point of the space of independent and dependent variables, we deduce from this a new system for which the integer $\rho$ has decreased; if the system obtained admit two dimensional integral elements having as its origin a generic point, but the system is not in involution, we deduce from this a new system for which the integer $\rho$ has again decreased. As the integer $\rho$ cannot decrease indefinitely, \emph{we therefore at a certain moment arrive at a incompatible system or a system in involution.}
\begin{thm*}
  Every solution of a closed differential system of two independent variables can be regarded as arising from the general solution of a system in involution that can be formed in a systematic manner with finitely many steps of operation.
\end{thm*}



\vspace{12pt}\fsec\textsc{Remark.} In reality we have limited ourselves to the solution of the initial system \eqref{eq:6.3} for which the rank of the  system formed by the first order differential equations have its maximal value, and the same restriction has been implicitly made regarding the successive differential systems obtained. In particular if the initial differential system has singular solutions, these solutions have been left aside. If we want to give the theorem just stated in the previous section validity even in this case, we therefore need to focus our attention on \emph{a determined solution} of the initial system and, for each successive system, start by adjoining, if necessary, the relations between the dependent and independent variables expressing that the rank of the system of linear differential equations has a value corresponding to the solution considered. Unfortunately \emph{it is not obvious that the rank of the new system of linear differential equations has increased, which is to say that it is not clear that the integer $\rho$ has decreased}, even though the number of independent relations between the dependent and independent variables has increased. So we can, strictly speaking, affirm only the theorem as proved above. Nevertheless the considerations of this section provide a practical method for obtaining all solutions of a given system, so that each solution is part of the general solution of a system in involution formed without prior integration.

