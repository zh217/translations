\documentclass[leqno,11pt]{book}
\usepackage{geometry}
\geometry{a4paper}
%\usepackage[adobe-utopia,uppercase=upright,greeklowercase=upright]{mathdesign}
\usepackage{indentfirst}
%\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{sectsty}

\usepackage{makeidx}

\makeindex


\usepackage{fontspec}
\usepackage{xunicode}
\usepackage{xltxtra}
\defaultfontfeatures{Mapping=tex-text}

\nohang
\partfont{\mdseries\scshape\centering}
\chapterfont{\mdseries\scshape\centering}
\sectionfont{\mdseries\scshape\centering}
\subsectionfont{\bfseries\centering}

\numberwithin{equation}{chapter}
%\usepackage{epstopdf}
\usepackage[numbib,nottoc]{tocbibind}
%\usepackage{overcite}
\usepackage[pdfborder={0 0 0}]{hyperref}
%\usepackage[perpage,para,symbol]{footmisc}
\usepackage[british]{babel}
%\usepackage{slashed}

\usepackage{perpage}
\MakePerPage[2]{footnote}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[RE]{{\rightmark}}
\fancyhead[LO]{{\leftmark}}
\fancyfoot{}


\renewcommand{\sectionmark}[1]
{\markright{\itshape {#1}}}

\makeatletter
\def\cleardoublepage{\clearpage\if@twoside \ifodd\c@page\else
\hbox{}
\vspace*{\fill}
\begin{center}
%This page is intentionally blank.
\end{center}
\vspace{\fill}
\thispagestyle{empty}
\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}
\makeatother

\renewcommand{\chaptermark}[1]{\markboth{\thechapter.\ \ #1}{}}

\title{Exterior differential systems\\
and their geometric applications}
\author{by \'Elie Cartan\\
\\
Translation by Ziyang Hu\\
\\
\\
Translated from the original French\\
\\
\emph{Les syst\`emes diff\'erentiels ext\'erieurs}\\ 
\emph{et leurs applications g\'eometriques}\\
\\
published in 1945\\\\}
%\date{}                                         % Activate to display a given date or no date

\newcommand{\pd}{\partial}
\newcommand{\rs}{\mathbb{R}}

\DeclareMathOperator{\inp}{\lrcorner}


\newtheoremstyle{shape0}% name
  {9pt}%      Space above
  {9pt}%      Space below
  {}%         Body font
  {}%         Indent amount (empty = no indent, \parindent = para indent)
  {\itshape}% Thm head font
  {.}%        Punctuation after thm head
  {.5em}%     Space after thm head: " " = normal interword space;
        %       \newline = linebreak
  {}%         Thm head spec (can be left empty, meaning `normal')


\newtheoremstyle{shape1}% name
  {9pt}%      Space above
  {9pt}%      Space below
  {\itshape}%         Body font
  {}%         Indent amount (empty = no indent, \parindent = para indent)
  {\scshape}% Thm head font
  {.}%        Punctuation after thm head
  {.5em}%     Space after thm head: " " = normal interword space;
        %       \newline = linebreak
  {}%         Thm head spec (can be left empty, meaning `normal')

\newtheoremstyle{shape2}% name
  {9pt}%      Space above
  {9pt}%      Space below
  {}%         Body font
  {}%         Indent amount (empty = no indent, \parindent = para indent)
  {\itshape}% Thm head font
  {.}%        Punctuation after thm head
  {.5em}%     Space after thm head: " " = normal interword space;
        %       \newline = linebreak
  {}%         Thm head spec (can be left empty, meaning `normal')


\setlength{\parindent}{15pt}

\theoremstyle{shape1}
\newtheorem*{thm*}{\hspace{15pt}Theorem}
\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prin}[thm]{Principle}
\newtheorem{expr}[thm]{Experiment}
\newtheorem*{dfn*}{\hspace{15pt}Definition}
\newtheorem*{concl*}{\hspace{15pt}Conclusion}
\theoremstyle{shape0}
\newtheorem*{rmk*}{\hspace{15pt}Remark}
\newtheorem*{pcase*}{\hspace{15pt}Particular case}

\theoremstyle{shape2}
\newtheorem{ex}[thm]{Example}
\theoremstyle{definition}

\renewcommand{\bfdefault}{b}

\begin{document}

\renewcommand{\theequation}{\arabic{equation}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\maketitle

\newcounter{frenchsec}
\setcounter{frenchsec}{1}
\newcommand{\fsec}{\textbf{\arabic{frenchsec}. }\addtocounter{frenchsec}{1}}

\renewcommand{\qedsymbol}{\textsc{q.e.d.}}
\renewcommand{\thesection}{\roman{section}.}
\renewcommand{\thesubsection}{\Roman{subsection}.}


\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}

This work is the extensively revised reproduction of a course delivered during the first semester of 1936--1937 at the Faculty of Sciences at the University of Paris.

Part One of this work is devoted to the exposition of the theory of systems of total differential equations, which has been the subject of several articles, already dated, especially those published in the \emph{Annales de l'Ecole Normale Sup\'erieure} between the years 1901 and 1908. This theory has served as the basis of my theory on the structure of infinite transformation groups in the sense of Lie. It has since been generalised by different authors, particularly Mr.~\textsc{E.~K\"ahler}, who extended it to any system of exterior differential equations. In this work I adopt the notation proposed by Mr.~\textsc{E.~K\"ahler}, which consists of denoting what is called the \emph{exterior differential} of an exterior differential form $\omega$ of some degree by $d\omega$, which I previously denoted by $\omega'$ and called the \emph{exterior derivative} of the form $\omega$.\index{exterior derivative}\index{exterior derivative|see{exterior differential}}

After Chapter 1, is purely algebraic in nature, on exterior forms and systems of exterior equations, Chapter 2 is devoted to exterior differential forms (the symbolic forms of \textsc{E.~Goursat}) and the operation of exterior differentiation. Chapter 3 introduces the notion of closed exterior differential systems and characteristic systems and develops the theory of completely integrable systems with applications to the classical problems of Pfaff. Chapter 4, which introduces the notions of integral element, characters and genre, is furthermore devoted to two fundamental existence theorems. Chapter 5 is denoted to differential systems with imposed independent variables, especially systems in involution, together with existence theorems relative to these systems and the indication of several simple criteria for involution. This is the chapter which has been most extensively revised. Chapter 6 introduces the notion of prolongation of a differential system at last, with applications to finding solutions of systems that are not in involution.

In Part One, from Chapter 4 onwards, the system of equations considered involve only \emph{analytic} functions. The Cauchy-Kowalewski theorem, which the proofs of the existence theorems rely on, is valid and makes sense only if the data are analytic.

The second part of this work is devoted to applications to differential geometry and consists of two chapters. All problems treated in Chapter 7 are related to the classical theory of surfaces. Some are old, and quite a few are new. In each of them the degree of indeterminacy of the solution as well as the way that we should pose the Cauchy problem is indicated, including the cases where the data are characteristic. Chapter 8 is concerned with problems with more than two independent variables. It contains the problem of triple orthogonal systems and the general problem of systems of $p$-tuple orthogonal systems in $p$ dimensional Euclidean space: in the last case, the Cauchy problem has been presented in a simple form that I believe has not been considered so far. Finally the problem of realising a given $ds^{2}$ of three variables in six dimensional Euclidean space is treated in details, with indications of the singular solutions of the problem. Throughout Part Two, the method of moving frames, which is especially suited for exterior differential systems in involution, is employed exclusively.

In the preparation of this work, I have made use of a first draft by Mr.~Luc \textsc{Gauthier}, research associate, prepared from notes taken during the course. This first draft has rendered  me great services and I am pleased to express my gratitude to Mr.~Gauthier.

\hfill \'Elie \textsc{Cartan}

{\small
\tableofcontents
}

\part{The theory of exterior differential systems}
\label{part:theory-exter-diff}

\chapter{Exterior forms}
\label{cha:exterior-forms}

\section{Symmetric and alternating bilinear forms. Algebraic and exterior quadratic forms}
\label{sec:symm-altern-bilin}

\fsec In classical algebra,  a bilinear form \index{bilinear form} is an expression of two series of variables $u^{1}, u^{2}, \dots, u^{n}$ and $v^{1}, v^{2}, \dots, v^{n}$ of the same number $n$ of variables, of the form\footnote{We adopt the following now widespread convention from now on:  the summation sign in front of an expression that contains the same summation index repeated twice is suppressed, {e.g.}, in formula \eqref{eq:1},  the summation indices $i$ and $j$ are each repeated twice.}
\begin{equation}
  \label{eq:1}  
F(u,v)=a_{ij}u^{i}v^{j},
\end{equation}
where the coefficients $a_{ij}$ belong to a number field which, for simplicity, we assume to be the field of real numbers.

Such a form is  symmetric \index{bilinear form!symmetric} if it remains unchanged when we replace the variables $u^{i}$ with the variables $v^{i}$ with the corresponding indices and \emph{vice versa}, {i.e.}, if we have
\[
a_{ij}=a_{ji}\qquad(i,j=1,2,\dots,n).
\]

To a symmetric bilinear form we can associate a quadratic form \index{quadratic form}
\begin{equation}
  \label{eq:2}
  F(u)=a_{ij}u^{i}u^{j}
\end{equation}
with only a single series of variables. The form \eqref{eq:1} is the \emph{polar form} \index{polar form} of the quadratic form \eqref{eq:2}:
\[
F(u,v)=v^{i}\frac{\pd F}{\pd u^{i}}=u^{i}\frac{\pd F}{\pd v^{i}}.
\]

There is an \emph{intrinsic} relation between the form \eqref{eq:1} and the form \eqref{eq:2} in the sense that if we perform on the variables $u^{i}$ and $v^{i}$ the same linear transformation
\[
u^{i}=A^{i}{}_{k}U^{k},\qquad v^{i}=A^{i}{}_{k}v^{k},
\] 
then the bilinear form \eqref{eq:1} transforms into a bilinear form $\Phi(U,V)$, still symmetric, and the quadratic form \eqref{eq:2} into an associated quadratic form $\Phi(U)$. We have,
\begin{align*}
\Phi(U,V)&=a_{ij}A^{i}{}_{k}A^{j}{}_{h}U^{k}V^{h},\\
\Phi(U)&=a_{ij}A^{i}{}_{k}A^{j}{}_{h}U^{k}U^{h}.
\end{align*}

It can be easily verified  that the form $\Phi(U,V)$ is symmetric, since the coefficient of $U^{k}V^{h}$, being the sum $a_{ij}A^{i}{}_{k}A^{j}{}_{h}$ where $i$ and $j$ are two independent summation indices, can be written as $a_{ji}A^{j}{}_{k}A^{i}{}_{h}=a_{ij}A^{i}{}_{h}A^{j}{}_{k}$, which is precisely the coefficient of $U^{h}V^{k}$. This verification can be avoided if we observe that exchanging the two series of variables $U^{i}$ and $V^{i}$ entails exchanging the two variables $u^{i}$ and $v^{i}$, which does not change the initial form $F(u,v)$.

\vspace{12pt}\fsec The form \eqref{eq:1} is  alternating \index{bilinear form!alternating} if it changes sign when the two series of variables $u^{i}$ and $v^{i}$ are swapped:
\[
F(v,u)=-F(u,v);
\]
and this translates into the antisymmetry of the coefficients:
\[
a_{ij}=-a_{ji}.
\]

If we now substitute $v^{i}=u^{i}$ we obtain a form that vanishes identically. Nonetheless we can still associate  an alternating form $F(u,v)$ with a quadratic form,  but one with \emph{anti-commutative} multiplication. To proceed, note that if we combine the two terms of $F(u,v)$
\[
a_{12}u^{1}v^{2}+a_{21}u^{2}v^{1},
\]
we obtain
\[
a_{12}(u^{1}v^{2}-u^{2}v^{1})=a_{12}
\begin{vmatrix}
  u^{1}&u^{2}\\
  v^{1}&v^{2}
\end{vmatrix}
,
\]
which we will write as $a_{12}[u^{1}u^{2}]$, the notation $[u^{1}u^{2}]$ denoting the determinant whose first line consists of the two variables $u^{1}, u^{2}$, and whose second line two other variables $v^{1}, v^{2}$. The expression $a_{12}[u^{1}u^{2}]$ may then  be regarded as a quadratic monomial formed by Grassmann's non-commutative multiplication, in which the product of two variables $u^{1}, u^{2}$ changes sign when the order of factors is swapped. We can, therefore, in this case associate  an alternating bilinear form
\[
F(u,v)=a_{ij}u^{i}v^{j}\qquad(a_{ij}=-a_{ji})
\]
with a quadratic form of exterior multiplication, or, more briefly, an \emph{exterior quadratic form} \index{exterior form!quadratic}
\[
F(u)=\frac{1}{2}a_{ij}[u^{i}u^{j}]\qquad(a_{ij}=-a_{ji}).
\]
We have put in the numerical factor $\frac{1}{2}$ because in the second expression the product $[u^{1}u^{2}]$ enters twice, once in the form $[u^{1}u^{2}]$ and once in the form $[u^{2}u^{1}]=-[u^{1}u^{2}]$, which gives the total coefficient
\[
\frac{1}{2}a_{12}-\frac{1}{2}a_{21}=a_{12}.
\]

There is an intrinsic correspondence between alternating bilinear forms and quadratic exterior forms, and this correspondence persists when a linear change of variables is performed  to the two series of variables $u^{i}$ and $v^{i}$ at the same time. If we put
\[
u^{i}=A^{i}{}_{k}U^{k},\qquad v^{i}=A^{i}{}_{k}V^{k},
\]
the form $F(u,v)$ becomes
\[
\Phi(u,v)=a_{ij}A^{i}{}_{k}A^{j}{}_{h}U^{k}V^{h},
\]
with the coefficients of the products $U^{i}V^{j}$ antisymmetric, and the form $F(u)$ becomes
\[
\Phi(U)=\frac{1}{2}a_{ij}A^{i}{}_{k}A^{j}{}_{h}[U^{k}U^{h}];
\]
this formula can be deduced by replacing every $u^{i}$ in the form $F(u)$ by $A^{i}{}_{k}U^{k}$ and then multiplying the coefficients by following the usual rules of algebraic multiplication,  taking care not to inverse the order of the variables $U^{i}$ in the resulting product.

\vspace{12pt}\fsec There are certain analogies between the classical quadratic forms, which we will call algebraic\index{quadratic form!algebraic}, and exterior quadratic forms\index{quadratic form!exterior}. Let us define the partial derivative of an exterior quadratic form $F(u)$ by the relation
\[
\frac{\pd F}{\pd u^{i}}=a_{ik}u^{k}.
\]

The derivative of each monomial vanishes if $u^{i}$ does not appear in the factors of the monomial; if $u^{i}$ appears as the first factor, as in the term $a_{ij}[u^{i}u^{j}]$, the derivative is $a_{ij}u^{j}$; if $u^{i}$ appears as the second factor, we first pass $u^{i}$ to the front and  then apply the same rule: the derivative of $a_{ji}[u^{j}u^{i}]$ with respect to $u^{i}$ being the derivative of $-a_{ji}[u^{i}u^{j}]$, {i.e.}, $-a_{ji}u^{j}=a_{ij}u^{j}$. The derivative with respect to first $u^{i}$ and then $u^{j}$ is $a_{ij}$ and we write \footnote{The order of taking the derivative is wrong in the original work. --- \textsc{Translator.}}
\[
\frac{\pd ^{2}F}{\pd u^{j}\pd u^{i}}=a_{ij}=-\frac{\pd^{2}F}{\pd u^{i}\pd u^{j}}.
\]
Observe that the sum $u^{i}\pd F/\pd u^{i}=a_{ik}u^{i}u^{k}$ vanishes whereas, if the quadratic form is algebraic, the sum $u^{i}\pd F/\pd u^{i}$ equals $2F$, according to a theorem of Euler.

Suppose now that instead of multiplying $u^{i}$ and $\pd F/\pd u^{i}$ by ordinary multiplication, we perform exterior multiplication. As it is easy to see, we have, according to whether the form $F$ is algebraic or exterior,
\begin{equation}
  \label{eq:3}
  \left[u^{i}\frac{\pd F}{\pd u^{i}}\right]=0,\qquad
  \left[u^{i}\frac{\pd F}{\pd u^{i}}\right]=2F.
\end{equation}
\begin{thm*}
  An algebraic quadratic form $F$ satisfies the relations
\[
u^{i}\frac{\pd F}{\pd u^{i}}=2F,\qquad\left[u^{i}\frac{\pd F}{\pd u^{i}}\right]=0,
\]
whereas an exterior quadratic form $F$ satisfies the relations
\[
u^{i}\frac{\pd F}{\pd u^{i}}=0,\qquad\left[u^{i}\frac{\pd F}{\pd u^{i}}\right]=2F.
\]
\end{thm*}

\vspace{12pt}\fsec The exterior multiplication of two linear forms of the same variables $f(u), \varphi(u)$ gives rise to an exterior quadratic form associated to an alternating bilinear form
\[
f(u)\varphi(v)-\varphi(u)f(v)=
\begin{vmatrix}
  f(u)&\varphi(u)\\
  f(v)&\varphi(v)
\end{vmatrix}
;
\]
we denote by the notation $[f(u)\varphi(u)]$ the quadratic form $F$ which results from the term-by-term multiplication of two forms $f(u)$ and $\varphi(u)$ without reversing the order of the factors:
\begin{equation}
  \label{eq:4}
  F=[a_{i}u^{i}\cdot b_{j}u^{j}]=a_{i}b_{j}[u^{i}u^{j}].
\end{equation}

Observe that on the right side the coefficient $a_{i}b_{j}$ is not antisymmetric, even though the form is exterior. We can write, by exchanging two indices of summation,
\begin{equation}
  \label{eq:5}
  F=a_{i}b_{j}[u^{i}u^{j}]=a_{j}b_{i}[u^{j}u^{i}]=-a_{j}b_{i}[u^{i}u^{j}]\quad\text{where}\quad F=\frac{1}{2}(a_{i}b_{j}-a_{j}b_{i})[u^{i}u^{j}],
\end{equation}
and now the coefficients of $[u^{i}u^{j}]$ are antisymmetric.

We again have two corresponding theorems.
\begin{thm*}
  If $f^{1}, f^{2},\dots,f^{p}$ are $p$ independent linear forms of $n$ variables $u^{1}$, $u^{2}$,\dots, $u^{n}$, the relation
\[
f^{1}\varphi_{1}+f^{2}\varphi_{2}+\dots+f^{p}\varphi_{p}=0,
\]
where $\varphi_{1},\varphi_{2},\dots,\varphi_{p}$ are $p$ forms of the same variables, implies that  $\varphi_{i}$ are  linear combinations of the forms $f^{i}$ with antisymmetric coefficients
\[
\varphi_{i}=\alpha_{ih}f^{h}\qquad(\alpha_{ij}=-\alpha_{ji});
\]
on the other hand the relation
\[
[f^{1}\varphi_{1}]+[f^{2}\varphi_{2}]+\dots+[f^{p}\varphi_{p}]=0,
\]
implies that  $\varphi_{i}$ are  linear combinations of the forms $f^{i}$ with symmetric coefficients $\alpha_{ij}$.
\end{thm*}

Suppose first that $p=n$. In this case the forms $f^{i}$ are independent and all linear forms can be expressed as linear combinations of $f^{i}$. We can write
\[
\varphi_{i}=\alpha_{ik}f^{k},
\]
and we have
\begin{align*}
  f^{i}\varphi_{i}&=\alpha_{ik}f^{i}f^{k},\\
  [f^{i}\varphi_{i}]&=\alpha_{ik}[f^{i}f^{k}].
\end{align*}

The sum $f^{i}\varphi_{i}$ vanishes if $\alpha_{ij}=-\alpha_{ji}$ and the sum $[f^{i}\varphi_{i}]$ vanishes if $\alpha_{ij}=\alpha_{ji}$, which proves the theorem.

Suppose now $p<n$. Introduce $n-p$ new forms $f^{p+1},\dots,f^{n}$, independent among themselves and of the previous $p$ forms. We can apply the already proven part of the theorem for the case $p=n$ by taking the forms $\varphi_{p+1},\dots,\varphi_{n}$ to vanish identically. We then have, since the form $\varphi_{p+j}$ vanishes, $\alpha_{p+j,k}=0$ $(j=1,2,\dots,n-p;k=1,2,\dots,n)$ and hence $\alpha_{k,p+j}=0$ as well. The indices $p+1,p+2,\dots,n$ do not appear in the coefficients $\alpha_{ij}$ that have effects on the expressions of $\varphi_{i}$ in terms of $f^{j}$, and thus the theorem is established for the general case.

\begin{rmk*}
  Suppose both equations
\[
f^{i}\varphi_{i}=0\qquad[f^{i}\varphi_{i}]=0
\]
hold, then consequently
\[
\varphi_{i}=0.
\]
\end{rmk*}

\vspace{12pt}\fsec We know that all algebraic quadratic forms can be put into canonical forms\index{quadratic form!canonical form} in which all the off-diagonal terms vanish. There also exists canonical forms for exterior quadratic forms.

\begin{thm*}
  All exterior quadratic forms can be reduced to the form
  \begin{equation}
    \label{eq:6}
    F=[U^{1}U^{2}]+[U^{3}U^{4}]+\dots+[U^{2p-1}U^{2p}],
  \end{equation}
where $U^{i}$ are $2p$ independent linear forms.
\end{thm*}

The proof is very simple. Suppose the form $F(u)$ does not vanish identically and, for example, $a_{12}\neq 0$. We can write
\[
F=\left[\left(u^{1}+\frac{a_{23}}{a_{21}}u^{3}+\dots+\frac{a^{2n}}{a_{21}}u^{n}\right)(a_{12}u^{2}+a_{13}u^{3}+\dots+a_{1n}u^{n})\right]+\Phi,
\]
where the form $\Phi$ contains only the variables $u^{3},u^{4}, \dots, u^{n}$. It  then suffices to put
\[
u^{1}+\frac{a_{23}}{a_{21}}u^{3}+\dots+\frac{a^{2n}}{a_{21}}u^{n}=U^{1},\qquad a_{12}u^{2}+a_{13}u^{3}+\dots+a_{1n}u^{n}=U^{2}
\]
to obtain an exterior quadratic form $F-[U^{1}U^{2}]$ that depends only on the variables $u^{3},\dots,u^{n}$, the $n$ forms $U^{1},U^{2},u^{3},\dots,u^{n}$ being independent. If the form $\Phi$ vanishes identically then the theorem is proved, and the integer $p$ equals $1$. Otherwise we perform on $\Phi$ the same operation that we performed on $F$ and continue.

We can find the integer $p$ without going through the previous reduction. Indeed, note that for any linear change of  variables, the system of linear equations
\begin{equation}
  \label{eq:7}
  \frac{\pd F}{\pd u^{1}}=0,\quad \frac{\pd F}{\pd u^{2}}=0,\quad \dots\quad\frac{\pd F}{\pd u^{n}}=0
\end{equation}
is conserved, since if we set
\[
u^{i}=A_{k}{}^{i}U^{k},
\]
where the determinant of the coefficients $A_{k}{}^{i}$ is non-zero, we immediately  have
\[
\frac{\pd F}{\pd U^{i}}=A_{i}{}^{k}\frac{\pd F}{\pd u^{k}},
\]
and since the determinant of $A_{i}{}^{k}$ does not vanish, the vanishing of the partial derivatives $\pd F/\pd u^{i}$ implies to the vanishing of $\pd F/\pd U^{i}$ and \emph{vice versa}.

Now if we put $F$ in the canonical form
\[
F=[U^{1}U^{2}]+\dots+[U^{2p-1}U^{2p}],
\]
the system \eqref{eq:7} becomes, with the variables $U^{i}$,
\[
U^{1}=U^{2}=\dots=U^{2p-1}=U^{2p}=0.
\]
The integer $2p$ is hence the rank of the matrix of coefficients of the forms $\pd F/\pd u^{i}$. This matrix is skew-symmetric, and it is well known that the rank of such a matrix is always even.

The rank $2p$ also gives the minimum number of variables that must appear in the form $F$, if we perform a suitable linear transformation on the variables.

There are analogous theorems for algebraic quadratic forms where the rank of the matrix of coefficients of the forms $\pd F/\pd u^{i}$ also gives the minimum number of variables that must appear in the form $F$, and the simplest way to see it is to use the decomposition of the form into a sum of squares.

\section{Exterior forms of any degree}
\label{sec:exter-forms-unsp}
\fsec We consider generic \emph{exterior forms}\index{exterior form} of any degree. A cubic exterior form, for example, can be written as
\begin{equation}
  \label{eq:8}
  F=\frac{1}{6}a_{ijk}[u^{i}u^{j}u^{k}],
\end{equation}
where the coefficients $a_{ijk}$ are \emph{antisymmetric}: this means that if we perform a permutation of the three indices $i,j,k$, the coefficients remain either unchanged or undergo a change of sign depending on whether the permutation is even or odd. The symbol $[u^{i}u^{j}u^{k}]$ can be regarded as a product, but one that changes sign when we permute two factors. The product remains equal to itself under an even permutation of the three factors, and becomes equal to its negative under an odd permutation. With these conventions we see that if we count the different monomials that appear, ordered by the three factors $u^{1},u^{2},u^{3}$, we obtain six different terms, whose sum being $a_{123}[u^{1}u^{2}u^{3}]$. Obviously monomials with two identical factors vanish.

We can associate to the form $F$ an alternating trilinear form of three series of variables $u^{i},v^{i},w^{i}$
\[
\frac{1}{6}a_{ijk}
\begin{vmatrix}
  u^{i}&u^{j}&u^{k}\\
  v^{i}&v^{j}&v^{k}\\
  w^{i}&w^{j}&w^{k}
\end{vmatrix}.
\]

More generally we can consider exterior forms of degrees $4,5$, {etc.}, which we may associate with alternating forms of $4,5$, {etc.}, series of variables.

\vspace{12pt}\fsec \emph{Addition and multiplication of exterior forms.} \index{exterior form!addition}The sum of two exterior forms \emph{of the same degree} is the exterior form of the same degree whose coefficients are the sums of the coefficients of the two given forms. For example, the sum of the form \eqref{eq:8} and the form
\begin{equation}
  \label{eq:9}
  \Phi=\frac{1}{6}b_{ijk}[u^{i}u^{j}u^{k}]
\end{equation}
is
\[
F+\Phi=\frac{1}{6}(a_{ijk}+b_{ijk})[u^{i}u^{j}u^{k}].
\]

The \emph{exterior product}\index{exterior form!exterior product} of two forms, regardless of whether the degrees are equal, for example of the two forms
\begin{align*}
  F=&\frac{1}{2}a_{ij}[u^{i}u^{j}],\\
  \Phi=&=\frac{1}{6}b_{ijk}[u^{i}u^{j}u^{k}]
\end{align*}
is the form obtained by exterior multiplying in every possible way each monomial of the first  with each monomial of the second, taking care to respect the order of the two monomials, and adding the results obtained \footnote{The numerical coefficients of the forms can be put in at any position, the law of commutativity being valid for them.}
\begin{equation}
  \label{eq:10}
  [F\cdot\Phi]=\frac{1}{12}a_{ij}b_{khl}[u^{i}u^{j}u^{k}u^{h}u^{l}].
\end{equation}

On the right hand side of equation \eqref{eq:10} the coefficients are not antisymmetric with respect to the $5$ indices $i,j,k,h,l$. However we can make them antisymmetric  as we did for the exterior product of two linear forms.

The exterior product of two forms $F,\Phi$ may depend on the order in which the factors $F$ and $\Phi$ appear. Indeed, changing the order of the factors entails to replacing the monomial $[u^{i}u^{j}u^{k}u^{h}u^{l}]$ by the monomial $[u^{k}u^{h}u^{l}u^{i}u^{j}]$, and to effect this change we advance each of the three factors two places to the left, which requires $2\times 3$ successive changes of sign. In general we have, if $F$ and $\Phi$ are of degrees $p$ and $q$ respectively, the relation
\begin{equation}
  \label{eq:11}
  [\Phi\cdot F]=(-1)^{pq}[F\cdot\Phi],
\end{equation}
and hence the
\begin{thm*}
  The exterior product of two exterior forms of degrees $p$ and $q$ remains unchanged when we reverse the order of the two factors, unless the two forms are both of odd degrees, in which case the product undergoes a change of sign.
\end{thm*}

Another important theorem concerns the distributivity of multiplication with respect to addition. This theorem leads to the general equality
\[
  [(F_{1}+F_{2}+\dots+F_{h})(\Phi_{1}+\Phi_{2}+\dots+\Phi_{k})]=\sum_{ij}[F_{i}\Phi_{j}],
\]
where we suppose that the forms $F_{1},F_{2},\dots,F_{h}$ are of the same degree $p$ and the forms $\Phi_{1},\Phi_{2},\dots,\Phi_{k}$ are of the same degree $q$.

\vspace{12pt}\fsec \emph{Monomic exterior forms}.\index{exterior form!monomial} An exterior form of degree $p$ is \emph{monomic} if it can be written as an exterior product of $p$ linear forms. We will investigate what conditions the coefficients of a form must satisfy for it to be monomic.

Consider the system of linear equations (\emph{the associated system}) \index{associated system!of exterior forms} obtained by setting to zero all  partial derivatives of order $p-1$ of the form. These derivatives are defined in the case of any degree analogously to the case of a quadratic form, and depend on the order that we perform the derivations, but in fact in the present case the order does not matter since the only possible change in the result is a change of sign.

The associated system we consider have an \emph{intrinsic} significance, in the sense that if we perform a linear change of variables with non-vanishing determinant on the form $F(u^{1},u^{2},\dots,u^{p})$ to make it into $\Phi(U^{1},U^{2},\dots,U^{p})$, the same linear transformation will transform the associated system of $F$ into the associated system of $\Phi$, the reason being that the linear transformation
\[
u^{i}=A_{k}{}^{i}U^{k}
\]
entails
\[
\frac{\pd\Phi}{\pd U^{i}}=A_{k}{}^{i}\frac{\pd F}{\pd u^{k}},\qquad \frac{\pd^{2}\Phi}{\pd U^{i}\pd U^{j}}=A_{k}{}^{i}A_{h}{}^{j}\frac{\pd^{2}F}{\pd u^{k}u^{h}},\qquad\dots
\]

Now suppose we have a monomic exterior form. We can perform a linear transformation on the variables to make it contain only the single term $a[u^{1}u^{2}u^{3}\dots u^{p}]$ (we exclude here the case where the form vanishes identically). The associated system of this form is evidently
\[
u^{1}=0,\quad u^{2}=0,\quad\dots,\quad u^{p}=0,
\]
which are $p$ independent equations.

Conversely, if the associated system of a form $F$ of degree $p$ can be reduced to $p$ independent equations, we can, by a linear transformation on the variables, make the associated system into the form
\[
u^{1}=0,\quad u^{2}=0,\quad\dots,\quad u^{p}=0,
\]
but then no non-vanishing coefficients of the form can contain an index other than $1,2,\dots,p$, since a non-zero coefficient such as $a_{i_{1}i_{2}\dots i_{p-1} i_{p+1}}$, will contain the variable $u^{p+1}$ in the derivative $\pd^{p-1}F/\pd u^{i_{1}}\pd u^{i_{2}}\dots\pd u^{i_{p-1}}$, contrary to our assumption. The form is therefore $a[u^{1}u^{2}\dots u^{p}]$, which is monomic \footnote{The same reasoning shows that if the associated system is of rank $r$, it is possible, by a change of variables, to find an expression of the form that involves only $r$ variables, and it is obviously impossible to find an expression involving fewer variables, as otherwise the associated system would be of rank less than $r$.}.
\begin{thm*}
  An exterior form of degree $p$ is a monomial if and only if the associated system is of rank $p$.\footnote{The theorem has in effect already been proven for the case $p=2$ by the introduction of the canonical form.}
\end{thm*}

\vspace{12pt}\fsec We now apply this criterion to investigate the necessary and sufficient conditions for the coefficients of an exterior form to be a monomial. We begin with the simple case of quadratic forms.

Suppose we have a quadratic form
\[
F=\frac{1}{2}a_{ij}[u^{i}u^{j}],
\]
which we assume is non-zero. Without loss of generality, suppose $a_{12}\neq 0$. Among the equations of the associated system there are two equations
\begin{align*}
  \frac{\pd F}{\pd u^{1}}&\equiv a_{12}u^{2}+a_{13}u^{3}+\dots+a_{1n}u^{n}=0,\\
  \frac{\pd F}{\pd u^{2}}&\equiv a_{21}u^{1}+a_{23}u^{3}+\dots+a_{2n}u^{n}=0.
\end{align*}

These two equations are independent and give
\begin{align*}
  u^{1}&=\frac{1}{a_{12}}(a_{23}u^{3}+\dots+a_{2n}u^{n}),\\
  u^{2}&=-\frac{1}{a_{12}}(a_{13}u^{3}+\dots+a_{1n}u^{n}).
\end{align*}

Let us find the coefficient of $u^{k}$ in $\pd F/\pd u^{i}$, taking into account the values of $u^{1},u^{2}$ that we just obtained. We have
\[
\frac{\pd F}{\pd u^{i}}=\left(a_{ik}+\frac{a_{i1}}{a_{12}}a_{2k}-\frac{a_{i2}}{a_{12}}a_{1k}\right)u^{k}=\frac{1}{a_{12}}(a_{12}a_{ik}-a_{1i}a_{2k}+a_{1k}a_{2i})u^{k}.
\]

The right hand side must vanish if we want the associated system to be of rank $2$, and hence the \emph{necessary and sufficient} conditions, for the case $a_{12}\neq 0$, is
\begin{equation}
  \label{eq:12}
  a_{12}a_{ik}-a_{1i}a_{2k}+a_{1k}a_{2i}=0.
\end{equation}

This relation has been proved supposing $a_{12}\neq 0$. However it remains true even when $a_{12}=0$. Indeed, the indices $1,2,i,k$ here all play the same role, since any permutation of the four indices leaves the relation unaltered. The relation is true whenever any of the coefficients $a_{12},a_{2i},a_{2k},a_{1i},a_{1k},a_{ik}$ is non-zero, and it is \emph{a fortiori} true when all of them are zero. We can therefore replace the indices $1,2$ with unspecified indices and the relation \eqref{eq:12} will continue to hold.

\begin{thm*}
  If a quadratic exterior form is monomic, then  among the coefficients there are the relations
  \begin{equation}
    \label{eq:13}
    a_{ij}a_{kh}-a_{ik}a_{jh}+a_{ih}a_{jk}=0\qquad(i,j,k,h=1,2,\dots,n).
  \end{equation}
\end{thm*}

\emph{Conversely}, if these relations hold, the form is monomial, since if we suppose $a_{12}\neq 0$ the relations \eqref{eq:12}, which appear in the relations \eqref{eq:13}, will hold, and we know that under this condition the form is monomic.

\vspace{12pt}\fsec \label{fsec:10} The same method will apply to a form of any degree. To make things concrete, take for now $p=5$ and the coefficient $a_{12345}$  non-vanishing. The associated system of the form contains the $5$ independent equations
\begin{align*}
  \frac{\pd^{4}F}{\pd u^{2}\pd u^{3}\pd u^{4}\pd u^{5}}&\equiv a_{12345}u^{1}+a_{2345k}u^{k}=0,\\
  \frac{\pd^{4}F}{\pd u^{1}\pd u^{3}\pd u^{4}\pd u^{5}}&\equiv -a_{12345}u^{2}+a_{1345k}u^{k}=0,\\
  \frac{\pd^{4}F}{\pd u^{1}\pd u^{2}\pd u^{4}\pd u^{5}}&\equiv a_{12345}u^{3}+a_{1245k}u^{k}=0,\\
  \frac{\pd^{4}F}{\pd u^{1}\pd u^{2}\pd u^{3}\pd u^{5}}&\equiv -a_{12345}u^{4}+a_{1235k}u^{k}=0,\\
  \frac{\pd^{4}F}{\pd u^{1}\pd u^{2}\pd u^{3}\pd u^{4}}&\equiv a_{12345}u^{5}+a_{1234k}u^{k}=0.
\end{align*}

For the form to be monomic it is necessary and sufficient that all the equations of the associated system follow from these five equations, which is to say if we replace $u^{1},u^{2},u^{3},u^{4},u^{5}$ by their values determined by these five equations, the other equations are identically satisfied. Consider for example the equation
\[
\frac{\pd ^{4} F}{\pd u^{i_{1}}\pd u^{i_{2}}\pd u^{i_{3}}\pd u^{i_{4}}}=0;
\]
the coefficients of $u^{k}$ on the left hand side, when $u^{1},u^{2},u^{3},u^{4},u^{5}$ have been replaced by their values and  multiplied by $a_{12345}$, will be equal to
\begin{align*}
a_{12345}a_{i_{1}i_{2}i_{3}i_{4}k}-a_{i_{1}i_{2}i_{3}i_{4}1}a_{2345k}&+a_{i_{1}i_{2}i_{3}i_{4}2}a_{1345k}-a_{i_{1}i_{2}i_{3}i_{4}3}a_{1245k}\\
&+a_{i_{1}i_{2}i_{3}i_{4}4}a_{1235k}-a_{i_{1}i_{2}i_{3}i_{4}5}a_{1234k}.
\end{align*}

We see that this expression contain $10$ indices, which can be divided into two groups, the indices $i_{1},i_{2},i_{3},i_{4}$ being the first group and the indices $1,2,3,4,5,k$ the second group, and within each group all the indices play the same role. The expression is antisymmetric with respect to the first group and also  with respect to the second group. The expression obtained vanishes when the form is monomial, supposing that $a_{12345}\neq 0$. It also vanishes when $a_{12345}=0$ but the coefficients $a_{1234k}, a_{1235k},a_{1245k},a_{1345k},a_{2345k}$ are not all zero, because if one of them is non-zero, for example the first, given that the indices $1,2,3,4,5,k$ play the same role we will find the same expression starting from the hypothesis $a_{1234k}\neq0$. On the other hand, obviously  if the $5$ coefficients $a_{12345},a_{1234k},a_{1235k},a_{1245k},a_{1345k},a_{2345k}$ are all zero, the expression still vanishes. We hence have the following theorem:
\begin{thm*}
  A form of degree $5$ is monomic if and only if the $C^{4}_{n}C^{6}_{n}$ quantities
  \begin{align}
    \label{eq:14}
    H_{i_{1}i_{2}i_{3}i_{4},j_{1}j_{2}j_{3}j_{4}j_{5}j_{6}}&\equiv
a_{i_{1}i_{2}i_{3}i_{4}j_{1}}a_{j_{2}j_{3}j_{4}j_{5}j_{6}}
-a_{i_{1}i_{2}i_{3}i_{4}j_{2}}a_{j_{1}j_{3}j_{4}j_{5}j_{6}}\\
&+a_{i_{1}i_{2}i_{3}i_{4}j_{3}}a_{j_{1}j_{2}j_{4}j_{5}j_{6}}
-a_{i_{1}i_{2}i_{3}i_{4}j_{4}}a_{j_{1}j_{2}j_{3}j_{5}j_{6}}\notag\\
&+a_{i_{1}i_{2}i_{3}i_{4}j_{5}}a_{j_{1}j_{2}j_{3}j_{4}j_{6}}
-a_{i_{1}i_{2}i_{3}i_{4}j_{6}}a_{j_{1}j_{2}j_{3}j_{4}j_{5}}\notag
  \end{align}
are all zero.
\end{thm*}

We have already shown that the condition is necessary. It is also sufficient, since if it holds and the coefficient $a_{12345}$ does not vanish, among the equations $\eqref{eq:14}$ we can pick out 
\[
H_{i_{1}i_{2}i_{3}i_{4},12345k}=0
\]
which, as we have already seen, is necessary and sufficient under the assumption $a_{12345}\neq 0$ for the form to be monomic.

Observe that all of these conditions involve quadratic relations between the coefficients of the given form.

If the form is cubic, we will find the relations
\[
H_{ij,khlm}\equiv a_{ijk}a_{hlm}-a_{ijh}a_{klm}+a_{ijl}a_{khm}-a_{ijm}a_{khl}=0
\]
which by naive counting are $(n(n-1)/2)(n(n-1)(n-2)(n-3)/24)$ in number, but in fact contain fewer equations since if $k=i,h=j$, the expression $H_{ij,ijhm}$ vanishes identically. If $k=i$, the expression has only three terms instead of four:
\[
H_{ij,ihlm}=-a_{ijh}a_{ilm}+a_{ijl}a_{ihm}-a_{ijm}a_{ihl}.
\]

\section{Systems of exterior equations}
\label{sec:syst-exter-equat}
\index{exterior equations, system of}
\fsec Consider a system of equations obtained by setting to zero one or more exterior forms of $n$ variables $u^{1},u^{2},\dots,u^{n}$. Each of these equations has a certain degree, but these degrees need not be the same. We can think of $u^{1},u^{2},\dots,u^{n}$ as the Cartesian coordinates of a point in a $n$ dimensional space. We say that a flat manifold passing through the origin of the space (\emph{we do not consider other cases}) satisfies the given system, or provides a \emph{solution} of the system, if the equations in the system are implied by the defining equations of the manifold, which are linear and homogeneous with respect to the variables. For example, if the flat manifold is of dimension $p$ (a $p$-plane), it is defined by $n-p$ independent linear relations of the coordinates. On this flat manifold we can choose $n-p$ coordinates, for example the last $n-p$ ones, to depend on the other $p$ coordinates $u^{1},u^{2},\dots,u^{p}$, and then we can replace $u^{p+1},\dots,u^{n}$ in the system of equations by their expressions in terms of $u^{1},u^{2},\dots,u^{p}$. We thus obtain a set of exterior forms in $u^{1},u^{2},\dots,u^{p}$ which must vanish identically. More generally we may express $u^{1},u^{2},\dots,u^{n}$ in terms of linear forms of $p$ variables $t^{1},t^{2},\dots,t^{p}$ and we then obtain a set of exterior forms in $t^{1},t^{2},\dots,t^{p}$ that must vanish identically.

We make the following observation:
\begin{thm*}
  All exterior equations of degree $p$ are automatically satisfied by flat manifolds of less than $p$ dimensions.
\end{thm*}

This follows from the fact that an exterior form of degree $p$ in $q<p$ variables vanishes identically.

\emph{To show that a $p$-plane is a solution of a system of exterior equations, it is therefore unnecessary to consider the equations in the system that are of degrees higher  than $p$.}

\vspace{12pt}\fsec To determine if a given $p$-plane is a solution of a given system of exterior equations, we can proceed by considering the so-called \emph{Pl\"ucker coordinates}\index{Plucker coordinates@Pl\"ucker coordinates} or  \emph{Grassmannians}\index{Grassmannian|see{Pl\"ucker coordinates}} of a $p$-plane (passing through the origin). Such a $p$-plane is completely determined if we specify $p$ independent vectors from the origin, the components of which we denote by $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$ respectively. From these we can form the matrix of $p$ rows and $n$ columns
\[
\begin{pmatrix}
  \xi_{1}{}^{1}&\xi_{1}{}^{2}&\dots&\xi_{1}{}^{n}\\
  \xi_{2}{}^{1}&\xi_{2}{}^{2}&\dots&\xi_{2}{}^{n}\\
  \hdotsfor[1.5]{4}\\
  \xi_{p}{}^{1}&\xi_{p}{}^{2}&\dots&\xi_{p}{}^{n}\\
\end{pmatrix},
\]
and we denote by $u^{i_{1}i_{2}\dots i_{p}}$ the determinant formed by the $p$ rows and the $i_{1},i_{2},\dots,i_{p}$ columns of the matrix. The quantities $u^{i_{1}i_{2}\dots i_{p}}$ are antisymmetric with respect to the $p$ indices. If we replace these $p$ vectors by $p$ other independent vectors taken from the same $p$-plane of components $\bar\xi_{1}{}^{i},\bar\xi_{2}{}^{i},\dots,\bar\xi_{p}{}^{i}$, the components $\bar\xi_{1}{}^{i},\bar\xi_{2}{}^{i},\dots,\bar\xi_{p}{}^{i}$ can be calculated from the components $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$ (for a given $i$) by a linear transformation, this being true for all $i$. Then we see that the new determinant $\bar u^{i_{1}i_{2}\dots i_{p}}$ is obtained by multiplying the old determinant of the corresponding indices by the same factor, {i.e.}, the determinant of the transformation that changes $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$ into $\bar\xi_{1}{}^{i},\bar\xi_{2}{}^{i},\dots,\bar\xi_{p}{}^{i}$. The coordinates $u^{i_{1}i_{2}\dots i_{p}}$ of a $p$-plane are therefore determined up to a non-zero factor: these are  the \emph{Pl\"ucker coordinates} of the $p$-plane; they are homogeneous and they are overabundant if $p>1$.

Conversely, knowledge about the Pl\"ucker coordinates of a $p$-plane completely determines the $p$-plane, because the equations of a $p$-plane can be obtained by expressing the vector from the origin, of components $u^{i}$,  as a linear combination of the vectors $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$, {i.e.}, the determinants of the minors of degree $p+1$ of the matrix
\[
\begin{pmatrix}
  u^{1}&u^{2}&\dots&u^{n}\\
  \xi_{1}{}^{1}&\xi_{1}{}^{2}&\dots&\xi_{1}{}^{n}\\
  \xi_{2}{}^{1}&\xi_{2}{}^{2}&\dots&\xi_{2}{}^{n}\\
  \hdotsfor[1.5]{4}\\
  \xi_{p}{}^{1}&\xi_{p}{}^{2}&\dots&\xi_{p}{}^{n}\\
\end{pmatrix}
\]
all vanish. Now each of these determinants is linear in $u^{1},u^{2},\dots,u^{n}$ and the coefficients  are just the determinants $u^{i_{1}i_{2}\dots i_{p}}$.

\vspace{12pt}\fsec This leads us to investigate, first, if a $p$-plane with Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ annihilates a given exterior form of degree $p$
\[
F=\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}].
\]

For this, introduce in the $p$-plane the coordinates $v^{1},v^{2},\dots,v^{p}$,  obtained by setting
\[
u^{i}=v^{k}\xi_{k}{}^{i}\qquad (i=1,2,\dots,n),
\]
the summation index $k$ taking the values $1,2,\dots,p$. We have
\[
[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}]=\xi^{i_{1}}_{k_{1}}\xi^{i_{2}}_{k_{2}}\dots\xi^{i_{p}}_{k_{p}}[v^{k_{1}}v^{k_{2}}\dots v^{k_{p}}];
\]
on the right hand side we only have to consider the non-zero monomials, i.e., the terms where all the indices $k_{1},k_{2},\dots,k_{p}$ are distinct. The monomial $[v^{k_{1}}v^{k_{2}}\dots v^{k_{p}}]$ is none other than the monomial $[v^{1}v^{2}\dots v^{p}]$ up to the sign $+$ or $-$ according to whether the permutation of the $p$ indices $k_{1},k_{2},\dots,k_{p}$ is even or odd. The coefficients of $[v^{1}v^{2}\dots v^{p}]$ are then easily seen to be the determinant
\[
\begin{vmatrix}
  \xi_{1}^{i_{1}}&\xi_{1}^{i_{2}}&\dots&\xi_{1}^{i_{p}}\\
  \xi_{2}^{i_{1}}&\xi_{2}^{i_{2}}&\dots&\xi_{2}^{i_{p}}\\
  \hdotsfor[1.5]{4}\\
  \xi_{p}^{i_{1}}&\xi_{p}^{i_{2}}&\dots&\xi_{p}^{i_{p}}\\
\end{vmatrix}
=u^{i_{1}i_{2}\dots i_{p}}.
\]

It therefore follows that
\[
F=\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}u^{i_{1}i_{2}\dots i_{p}}[v^{1}v^{2}\dots v^{p}].
\]

The condition for a given $p$-plane to be a solution of the exterior equation $F=0$ that we are seeking is hence that $F$ vanishes when we replace the exterior product $[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}]$ by the Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ of the $p$-plane.
\begin{thm*}
  A $p$-plane of Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ is a solution of the equation
\[
F\equiv\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}]=0,
\]
if and only if \footnote{Observe that it suffices to substitute the monomial $[u^{i_{1}}u^{i_{2}}\dots u^{i_{p}}]$ by the associated linear alternating $p$-form of the $p$ variables  $\xi_{1}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$.}
\[
a_{i_{1}i_{2}\dots i_{p}}u^{i_{1}i_{2}\dots i_{p}}=0.
\]
\end{thm*}

\vspace{12pt}\fsec We will now find the conditions for a $p$-plane given in terms of its Pl\"ucker coordinates to annihilate an exterior form $\Phi$ of degree $q<p$.

We begin with some geometrical remarks. If a $p$-plane annihilates the form $\Phi$, then all $q$-planes contained in the $p$-plane ($q<p$) also annihilate the form $\Phi$, since the form $\Phi$ already vanishes on the equations determining the $p$-plane, and will \emph{a fortiori} vanish when we \emph{in addition} take into account more equations which, together with the first set of equations, define the $q$-plane. Conversely suppose that the form $\Phi$ is annihilated by all the $q$-planes contained in a given $p$-plane. If we take into account the equations of the $p$-plane, in which for example the coordinates $u^{p+1},\dots,u^{n}$ have already been solved, the form $\Phi$ becomes a form $\Psi$ of $p$ variables $u^{1},u^{2},\dots,u^{p}$. To say that the form vanishes in all the $q$-planes contained in the given $p$-plane is the same as saying that it vanishes when we reduce the $p$ variables $u^{1},u^{2},\dots,u^{p}$ with any $p-q$ independent linear relations. Now if we take the linear relations
\[
u^{q+1}=u^{q+2}=\dots=u^{p}=0,
\]
there will remain only a single term in $\Psi$, which is $[u^{1}u^{2}\dots u^{q}]$. The coefficient of the monomial $[u^{1}u^{2}\dots u^{p}]$ in $\Psi$ is therefore zero, and the same holds for all the other coefficients.
\begin{thm*}
  A $p$-plane  annihilates an exterior form $\Phi$ of degree $q<p$ if and only if all the $q$-planes contained in the given $p$-plane annihilate the form.
\end{thm*}

Analytically we can proceed in the following manner. If the form $\Phi$ vanishes on the given $p$-plane, all the forms of degree $p$
\[
[\Phi u^{\alpha_{1}}u^{\alpha_{2}}\dots u^{\alpha_{p-q}}]\qquad(\alpha_{1},\alpha_{2},\dots,\alpha_{p-q}=1,2,\dots,n)
\]
vanish \emph{a fortiori} on the $p$-plane. This condition is necessary and sufficient. Indeed, suppose that by using the equations of the $p$-plane in which for example the variables $u^{p+1},\dots,u^{n}$ have been solved, then the form $\Phi$ becomes a certain form $\Psi$ in $u^{1},u^{2},\dots,u^{p}$. If each of the forms $[\Psi u^{\alpha_{1}}u^{\alpha_{2}}\dots u^{\alpha_{p-q}}]$ of degree $p$ obtained by taking $u^{\alpha_{1}},u^{\alpha_{2}},\dots,u^{\alpha_{p-q}}$ to be $p-q$ functions of the variables $u^{1},u^{2},\dots,u^{p}$ is zero, then each of the coefficients of the form $\Psi$ is zero and the $p$-plane annihilate the form $\Phi$.
\begin{thm*}
  A given $p$-plane  annihilates an exterior form $\Phi$ of degree $q<p$
\[
\Phi=\frac{1}{q!}b_{i_{1}i_{2}\dots i_{q}}[u^{i_{1}}u^{i_{2}}\dots u^{i_{q}}],
\]
if and only if the Pl\"ucker components of the $p$-plane satisfy the $C_{n}^{p-q}$ linear equations
\[
b_{i_{1}i_{2}\dots i_{q}}u^{i_{1}i_{2}\dots i_{q}\alpha_{1}\alpha_{2}\dots \alpha_{p-q}}=0\qquad (\alpha_{1},\alpha_{2},\dots,\alpha_{p-q}=1,2,\dots,n).
\]
\end{thm*}

For example, if a tri-plane is a solution of the equations
\begin{align*}
  a_{i}u^{i}&=0,\\
  a_{ij}[u^{i}u^{j}]&=0,\\
  a_{ijk}[u^{i}u^{j}u^{k}]&=0,\\
\end{align*}
then its Pl\"ucker components satisfy $({n^{2}+n+2})/{2}$ linear equations
\begin{align*}
  a_{i}u^{i\alpha\beta}&=0&&(\alpha,\beta=1,2,\dots,n),\\
  a_{ij}u^{ij\alpha}&=0&&(\alpha=1,2,\dots,n),\\
  a_{ijk}u^{ijk}&=0.&
\end{align*}

\vspace{12pt}\fsec \emph{Conditions for the antisymmetric quantities $u^{i_{1}i_{2}\dots i_{p}}$ to be the Pl\"ucker coordinates of a $p$-plane.} Suppose a system of quantities $u^{i_{1}i_{2}\dots i_{p}}$ antisymmetric with respect to the $p$ indices $i_{1},i_{2},\dots,i_{p}$ taking values of the integers $1,2,\dots,n$ are \emph{a priori} given. In general these numbers will not be the Pl\"ucker coordinates of any $p$-plane.
\begin{thm*}
   A system of quantities $u^{i_{1}i_{2}\dots i_{p}}$ antisymmetric with respect to the $p$ indices is the Pl\"ucker coordinates of a $p$-plane if and only if the exterior form of degree $p$
\[
F=\frac{1}{p!}u^{i_{1}i_{2}\dots i_{p}}[z_{i_{1}}z_{i_{2}}\dots z_{i_{p}}]
\]
of $n$ variables $z_{1},z_{2},\dots, z_{n}$ is a monomial.
\end{thm*}
Indeed, if $u^{i_{1}i_{2}\dots i_{p}}$ are the Pl\"ucker coordinates of a $p$-plane determined by the $p$ independent vectors $\xi_{i}{}^{i},\xi_{2}{}^{i},\dots,\xi_{p}{}^{i}$, we have
\[
F=[f_{1}f_{2}\dots f_{p}],
\]
by putting
\[
f_{1}=\xi_{1}{}^{i}z_{i},\quad,f_{2}=\xi_{2}{}^{i}z_{i},\quad\dots,\quad f_{p}=\xi_{p}{}^{i}z_{i}.
\]

Conversely if $F$ can be put into the form just stated, with $f_{1},f_{2},\dots,f_{p}$ being $p$ arbitrary linear forms, then the coefficients $u^{i_{1}i_{2}\dots i_{p}}$ are the Pl\"ucker coordinates of the $p$-plane determined by the vectors $\xi_{1}{}^{i},\dots,\xi_{p}{}^{i}$. \footnote{The linear relations $u^{i_{1}i_{2}\dots i_{p-1}k}z_{k}=0$ give the necessary and sufficient conditions to be satisfied by the quantities $z_{k}$ for the hyperplane $z_{1}u^{1}+z_{2}u^{2}+\dots+z_{n}u^{n}=0$ to contain the $p$-plane under consideration.}\qed

We can now apply the conditions we have found in \textsection
\textbf{10}. In particular, \emph{a system of quantities $u^{ijk}$ antisymmetric with respect to the three indices $i,j,k$ is the coordinates of a tri-plane if and only if}
\begin{gather*}
H^{ij,khlm}=u^{ijk}u^{hlm}-u^{ijh}u^{klm}+u^{ijl}u^{khm}-u^{ijm}u^{khl}=0\\
(i,j,k,h,l,m=1,2,\dots,n).
\end{gather*}

In previous discussions we have obtained  \emph{linear} equations to be satisfied by the Pl\"ucker coordinates of a $p$-plane for such a $p$-plane to provide a solution of a given system of exterior equations. For completeness, we must also adjoin  the \emph{quadratic} equations that must hold for the coordinates to be a $p$-plane to these linear equations.

\section{Algebraically equivalent systems of exterior equations}
\label{sec:algebr-equiv-syst}

\fsec \emph{Ring of exterior forms.}\index{exterior form!ring of} The \emph{ring} determined by $h$ homogeneous exterior forms $F_{1},F_{2},\dots,F_{h}$ of $n$ variables is the set of exterior forms
\[
\Phi=[F_{1}\varphi^{1}]+[F_{2}\varphi^{2}]+\dots+[F_{h}\varphi^{h}],
\]
where the $\varphi^{i}$ are homogeneous exterior forms subject to the  condition that  the terms $[F_{i}\varphi^{i}]$ on the right hand side have the sum of degrees of the two factors, positive or zero, the same for all  terms.

Obviously all forms $\Phi$ in the ring determined by $h$ given forms $F_{1}$, $F_{2}$, \dots , $F_{h}$ vanish for all solutions of the system
\[
F_{1}=0,\qquad F_{2}=0,\qquad\dots\qquad F_{h}=0.
\]

However, the converse is not always true. We will show that it is true for a fairly general case, and give an example where it is false.

\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape1}
\newtheorem*{thm17}{\hspace{15pt}\textbf{17.} Theorem}
\begin{thm17}
  An exterior form $\Phi$  vanishes on all solutions of a system of independent linear equations
  \begin{equation}
    \label{eq:15}
    F_{1}=0,\qquad F_{2}=0,\qquad\dots\qquad F_{h}=0,
  \end{equation}
if and only if $\Phi$ belongs to the ring determined by the forms $F_{i}$. Moreover,  $\Phi$  belongs to the ring if and only if the form $[F_{1}F_{2}\dots F_{h}\Phi]$ vanishes.
\end{thm17}

We have already seen that if $\Phi$ belongs to the ring under consideration, the form $\Phi$ vanishes on all the solutions of the linear equations \eqref{eq:15}. This condition is necessary: by a change of variables the forms $F_{i}$ can be reduced to the variables
\[
u^{1},u^{2},\dots,u^{h},
\]
and to say that the form $\Phi$ vanishes when we set the variables $u^{1},u^{2},\dots,u^{h}$ to zero is the same as saying that all monomials of $\Phi$ contain at least one of the variables $u^{1},u^{2},\dots,u^{h}$. Denote by $\varphi^{1}$ the coefficient of $u^{1}$ in the terms of $\Phi$ which contain the variable $u^{1}$ after we pass $u^{1}$ to the first factor of each of these terms, and $\varphi^{2}$ the form obtained in the analogous manner with respect to the variable $u^{2}$ for the form $\Phi-[u^{1}\varphi^{1}]$, {etc.},  we obviously have
\begin{equation}
  \label{eq:16}  
  \Phi=[u^{1}\varphi^{1}]+[u^{2}\varphi^{2}]+\dots+[u^{h}\varphi^{h}].
\end{equation}
The first part of the theorem is thus proved.

Clearly the form
\[
[F_{1}F_{2}\dots F_{h}\Phi]
\]
vanishes identically, since on replacing $\Phi$ by our expresson \eqref{eq:16}, each term in the product contains two identical factors \emph{of degree $1$} and hence vanishes.

Conversely if the form $[F_{1}F_{2}\dots F_{h}\Phi]$ vanishes and we again by a suitable change of variables put the forms into $F_{i}=u^{i}$ $(i=1,2,\dots,h)$, then for each monomial in the expression of $\Phi$ its factors cannot be all distinct from $u^{1},u^{2},\dots,u^{h}$, and consequently $\Phi$ vanishes when $u^{1},u^{2},\dots,u^{h}$ are zero.\qed

\vspace{12pt}\fsec Now consider the following example. Suppose we have the \emph{non-linear} forms
\[
F_{1}=[u^{1}u^{3}],\qquad F_{2}=[u^{1}u^{4}],\qquad F_{3}=[u^{1}u^{2}]-[u^{3}u^{4}].
\]
For all the bi-planes of coordinates $u^{ij}$ annihilating these three forms, if we take into consideration the quadratic relation
\[
u^{13}u^{24}-u^{14}u^{23}-u^{12}u^{34}=0,
\]
then the coordinate $u^{12}$ must be zero, {i.e.}, the following equation holds
\[
\Phi\equiv[u^{1}u^{2}]=0.
\]

The same holds for all $p$-planes annihilating the same $F_{1},F_{2},F_{3}$, as all bi-planes contained in the $p$-plane have to annihilate the form $\Phi$, and hence the form $\Phi$ vanishes on this $p$-plane.

\emph{However, the form $\Phi$, which vanishes on all solutions of the equations $F_{1}=F_{2}=F_{3}=0$, does not belong to the ring determined by the forms $F_{1},F_{2},F_{3}$.}

\begin{dfn*}
  A system of exterior equations is  \emph{complete} \index{exterior equations, system of!complete} if all the forms that vanish on the solution of the system belong to the ring of the forms defining the equations of the system.
\end{dfn*}

\vspace{6pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape1}
\newtheorem*{def19}{\hspace{15pt}\textbf{19.} Definition}
\begin{def19}
  Two systems of exterior equations are  algebraically equivalent\index{exterior equations, system of!algebraically equivalent} if the forms determining the equations of either of the systems belong to the ring of the forms determining the equations of the other system.
\end{def19}

It is clear that two algebraically equivalent systems admit the same solutions. If a system is complete, all other systems admitting the same solutions are algebraically equivalent to it, but the converse may not be true.

It is easy to construct a more general system algebraically equivalent to a given system. Consider for example the system of degree three
\begin{equation}
  \label{eq:17}
  \left\{
    \begin{aligned}
      F_{1}&\equiv A_{1i}u^{i}=0,\\
      F_{2}&\equiv A_{2i}u^{i}=0,\\
      \Phi&\equiv \frac{1}{2}B_{ij}[u^{i}u^{j}]=0,\\
      \Psi&\equiv \frac{1}{6}C_{ijk}[u^{i}u^{j}u^{k}]=0.
    \end{aligned}
  \right.
\end{equation}

It suffices to construct the equations
\begin{equation}
  \label{eq:17'}
  \tag{17$^{\prime}$}
  \left\{
    \begin{aligned}
      \bar{F}_{1}&\equiv aF_{1}+b F_{2}=0,\\
      \bar{F}_{2}&\equiv a'F_{1}+b'F_{2}=0,\\
      \bar\Phi&\equiv c\Phi+[\varpi^{1}F_{1}]+[\varpi^{2}F_{2}]=0,\\
      \bar\Psi&\equiv h\Psi+[\varpi^{3}\Phi]+[\psi^{1}F_{1}]+[\psi^{2}F_{2}]=0,\\
    \end{aligned}
  \right.
\end{equation}
where $a,b,a',b',c,h$ are constants ($ab'-ba'\neq 0,c\neq 0,h\neq 0$) and $\varpi^{1},\varpi^{2},\varpi^{3}$ are arbitrary linear forms, $\psi^{1},\psi^{2}$ arbitrary quadratic exterior forms.
\section[Associated system of a system of exterior equations]{Associated system of a system\\of exterior equations}
\label{sec:assoc-syst-syst}

\fsec We have already considered (\textsection\textbf{8})  the so-called associated system of an exterior form. We now define the associated system of a system of exterior equations. 

\begin{dfn*}
  A line $\Delta$ through the origin is  distinguished\index{distinguished line} with respect to a system of exterior equations $\Sigma$ if it is a solution of the system and moreover, given any flat manifold $V$ which provides a solution of the system $\Sigma$, the smallest flat manifold containing $V$ and $\Delta$ is also a solution of $\Sigma$.
\end{dfn*}

We now find the necessary and sufficient conditions for a line of parameters $\xi^{i}$ to be distinguished for a given system of exterior equations.

Consider a system $\Sigma$, which for simplicity we assume to be of degree three
\begin{equation}
  \label{eq:18}
  \left\{
    \begin{aligned}
      F_{\alpha}&\equiv A_{\alpha i}u^{i}=0&(\alpha&=1,2,\dots,r_{1}),\\
      \Phi_{\alpha}&\equiv A_{\alpha ij}[u^{i}u^{j}]=0&(\alpha&=1,2,\dots,r_{2}),\\
      \Psi_{\alpha}&\equiv A_{\alpha ijk}[u^{i}u^{j}u^{k}]=0&(\alpha&=1,2,\dots,r_{3}).
    \end{aligned}
  \right.
\end{equation}

For a line ($\xi^{i}$) to be distinguished, it must satisfy three kinds of conditions  (\textsection\textbf{13} and \textsection\textbf{14}) :

1. It must be itself a solution of $\Sigma$, that is to say it annihilates the forms $F_{\alpha}$:
\begin{equation}
\label{eq:19}
A_{\alpha i}\xi^{i}=0\qquad (\alpha=1,2,\dots,r_{1});
\end{equation}

2. Given any line $(u^{i})$ which is a solution of $\Sigma$, the bi-plane formed by the line $(u^{i})$ and the line $(\xi^{i})$ must annihilate the forms $\Phi_{\alpha}$, which is to say we have
\begin{equation}
\label{eq:20}
A_{\alpha ij}u^{i}\xi^{j}=0\qquad(\alpha=1,2,\dots,r_{2}),
\end{equation}
whenever the components $u^{i}$ satisfy the $r_{1}$ relations
\begin{equation}
\label{eq:21}
A_{\beta i}u^{i}=0\qquad (\beta=1,2,\dots,r_{1});
\end{equation} 

3. Given a bi-plane of coordinates $u^{ij}$ which is a solution of $\Sigma$, the tri-plane determined by the bi-plane and the line $(\xi^{i})$ must annihilate the forms $\Psi_{\alpha}$; the coordinates $u^{ijk}$ of the tri-plane are
\[
u^{ijk}=u^{ij}\xi^{k}-u^{ik}\xi^{j}+u^{jk}\xi^{i};
\]
therefore we must have
\begin{equation}
\label{eq:22}
  A_{\alpha ijk}u^{ij}\xi^{k}=0\qquad (\alpha=1,2,\dots,r_{3}),
\end{equation}
whenever the coordinates $u^{ij}$ of the given bi-plane satisfy the $nr_{1}+r_{2}$ relations
\begin{equation}
\label{eq:23}
  \left\{
    \begin{aligned}
      A_{\beta i}u^{i1}&=A_{\beta i}u^{i2}=\dots =A_{\beta i}u^{i n}=0,&(\beta&=1,2,\dots,r_{1}),\\
      A_{\gamma ij}u^{ij}&=0,&(\gamma&=1,2,\dots,r_{2}).
    \end{aligned}
  \right.
\end{equation}

These conditions are necessary and sufficient. Indeed, let $V$ be a $p$-plane which is a solution of the system $\Sigma$ and $W$ the $(p+1)$-plane determined by $V$ and the line $\xi^{i}$. For the $(p+1)$-plane to satisfy the system $\Sigma$, it is necessary and sufficient that all tri-planes contained in $W$ are solutions of $\Sigma$. This is the case if the tri-plane belongs to $V$. On the other hand, if it is formed by the line $(\xi^{i})$ and a bi-plane contained in $V$, the bi-plane is therefore a solution of $\Sigma$, and the conditions under which the tri-plane is a solution of $\Sigma$ are precisely those of equations \eqref{eq:19}, \eqref{eq:20} and \eqref{eq:22}, taking into consideration \eqref{eq:21} and \eqref{eq:23}.

\begin{concl*}
  A line $(\xi^{i})$ is distinguished with respect to the system $\Sigma$, if and only if the $\xi^{i}$ satisfy the equations \eqref{eq:19}, \eqref{eq:20} and \eqref{eq:22}, the coefficients $u^{i}$ which appear in the equations \eqref{eq:20}  subject to the  condition that they annihilate the forms $F_{\alpha}$, and the coefficients $u^{ij}$ which appear in the equations \eqref{eq:22}  subject to the condition that they satisfy, first, the quadratic equations dictating that they are the coordinates of a bi-plane, and second, the equation \eqref{eq:23} dictating that the bi-plane is a solution of $\Sigma$.
\end{concl*}

It is easy to see how the search for distinguished lines must proceed given a system of any degree.

\vspace{12pt}\fsec \emph{Associated system of a system of exterior equations}.\index{associated system!of system of exterior equations} For now, we \emph{do not adjoin} the quadratic equations dictating that the $u^{ij}$ are the coordinates of a bi-plane to the conditions \eqref{eq:23} that must be  satisfied by the quantities contained in equation \eqref{eq:22} . We then have, for $\xi^{h}$, a system of conditions \eqref{eq:19},\eqref{eq:20}, \eqref{eq:22} which may be more\footnote{In the original work, the word here and in the following is ``less'', but apparently ``more'' is more appropriate here, since there are \emph{less} restrictions on the undetermined parameters in the constraints. --- \textsc{Translator}} restrictive than the conditions for distinguished lines. We can show that the new conditions obtained express the fact that the exterior forms in $u^{1},u^{2},\dots,u^{n}$,
\[
\xi^{i}\frac{\pd F_{\alpha}}{\pd u^{i}},\qquad
\xi^{i}\frac{\pd \Phi_{\alpha}}{\pd u^{i}},\qquad
\xi^{i}\frac{\pd \Psi_{\alpha}}{\pd u^{i}}
\]
belong to the ring of the forms $F_{\alpha},\Phi_{\alpha},\Psi_{\alpha}$, {i.e.}, the ring of the system $\Sigma$.

\begin{dfn*}
  The associated system to a system of exterior equations $\Sigma$ is the set of linear equations in $\xi^{1},\xi^{2},\dots,\xi^{n}$  expressing the fact that the exterior forms $\xi^{i}\pd H/\pd u^{i}$, where $H$ are the forms determining the system $\Sigma$, belong to the ring of the system.
\end{dfn*}

The associated system of an exterior form $F$, introduced in \textsection\textbf{8}, can now be defined as the set of linear equations in $\xi^{1},\xi^{2},\dots,\xi^{n}$ which express the fact that the form $\xi^{i}\pd F/\pd u^{i}$, considered as an exterior form in $u^{1},u^{2},\dots,u^{n}$, vanishes identically, {i.e.}, belongs to the ring of $F$.\footnote{As an example of the case where the associated system is more restrictive than the system that dictates the lines are distinguished, let us consider the system $\Sigma$ of equations ({c.f.}~\textsection\textbf{18})
\[
[u^{1}u^{3}]=
[u^{1}u^{4}]=
[u^{1}u^{2}]-[u^{3}u^{4}]=
[u^{1}u^{2}u^{5}]-[u^{3}u^{4}u^{6}]=
0.
\]
Its associated system is
\[
\xi^{1}=
\xi^{2}=
\xi^{3}=
\xi^{4}=
\xi^{5}-
\xi^{6}=
0,
\]
whereas the distinguished lines are determined by the equations
\[
\xi^{1}=
\xi^{2}=
\xi^{3}=
\xi^{4}=
0.
\]
}

\vspace{12pt}\fsec It is easily seen that two algebraically equivalent systems have the same associated system: the associated system is intrinsically linked to $\Sigma$. We now prove the following theorem:
\begin{thm*}
  If the associated system is of rank $p$ and, by a suitable change of variables, reduces to the relations $\xi^{1}=\xi^{2}=\dots=\xi^{p}=0$, then there exists a system algebraically equivalent to $\Sigma$ which contains only the variables $u^{1},u^{2},\dots,u^{p}$.
\end{thm*}

Indeed, consider the line where all the variables $\xi^{i}$  vanish except $\xi^{p+1}=1$. The forms $\pd F_{\alpha}/\pd u^{p+1}$, which are constants, cannot belong to the ring of $\Sigma$ unless they are zero: the $F_{\alpha}$ does not contain the variables $u^{p+1}$. The forms
\begin{align*}
  \Phi^{*}_{\alpha}&=\Phi_{\alpha}-\left[u^{p+1}\frac{\pd\Phi_{\alpha}}{\pd u^{p+1}}\right],\\
  \Psi^{*}_{\alpha}&=\Psi_{\alpha}-\left[u^{p+1}\frac{\pd\Psi_{\alpha}}{\pd u^{p+1}}\right],
\end{align*}
obviously do not contain the variable $u^{p+1}$ either. But the form $\pd\Phi_{\alpha}/\pd u^{p+1}$ belongs to the ring of $F_{\alpha}$ and the form $\pd\Psi_{\alpha}/\pd u^{p+1}$ to the ring of $F_{\alpha}$ and $\Phi_{\alpha}$, and hence the system
\[
F_{\alpha}=0,\qquad \Phi^{*}_{\alpha}=0,\qquad \Psi^{*}_{\alpha}=0
\]
is algebraically equivalent to $\Sigma$: the equations of the system do not contain $u^{p+1}$. We can then proceed step by step to obtain a system algebraically equivalent to $\Sigma$ and that contains only the variables $u^{1},u^{2},\dots,u^{p}$.\qed

\begin{rmk*}
  The integer $p$ is obviously the minimal number of variables in which a system algebraically equivalent to $\Sigma$ can be written, since if such a system involves only $q<p$ variables, its associated system, which is the same as that of $\Sigma$, can be at most of rank $q$. Moreover, we see that the $p$ variables that appear in a system algebraically equivalent to $\Sigma$ in the minimal case are  well determined when we consider them as a whole.
\end{rmk*}

\begin{pcase*}
  If the system $\Sigma$ is of order only two, it can be seen easily that the associated system is obtained by adjoining to the equations of degree $1$ in $\Sigma$ the equations of the associated system of $r_{2}$ forms
\[
[F_{1}F_{2}\dots F_{r_{1}}\Phi_{\alpha}]=0\qquad(\alpha=1,2,\dots,r_{2}),
\]
equations which are obtained by setting to zero all the partial derivatives of order $r_{1}+1$ on the left hand side.
\end{pcase*}


\chapter{Exterior differential forms}
\label{cha:exter-diff-forms}

\section{Definition. Exterior differentiation}
\label{sec:defin-exter-diff}

\fsec Consider a space of $n$ dimensions, or a certain domain $\mathcal{D}$ therein. We suppose that this space is Euclidean. Although this assumption is not necessary, it serves  to simplify the language. Denote by $x^{1},x^{2},\dots,x^{n}$ the coordinates of this space.  \emph{Exterior differential forms of degree $p$}\index{differential form} are exterior forms whose \emph{variables} are the differentials $dx^{1},dx^{2},\dots,dx^{n}$ and whose coordinates are functions of the coordinates $x^{i}$ defined in the domain $\mathcal{D}$. A differential form of degree $1$, or a linear form, can be therefore written as
\[
a_{i}(x)dx^{i},
\]
and a quadratic differential form can be written as
\[
\frac{1}{2}a_{ij}(x)[dx^{i}dx^{j}]\qquad(a_{ij}=-a_{ji}),
\]
and so on.

By a convenient abuse of language, we consider a function of $x^{1},x^{2},\dots,x^{n}$ to be a differential form of degree zero.

By a change of coordinates, which acts on the \emph{variables} $dx^{i}$ as a linear transformation of non-zero determinant, exterior forms of degree $p$ transform to exterior forms of the same degree. We make the natural assumption that the new coordinates can be differentiated with respect to the old ones.

\vspace{12pt}\fsec \emph{Exterior differential of differential forms}. If a form is of degree zero, {i.e.}, a function $f(x)$, its exterior differential is by definition the ordinary differential $df$.\index{exterior differential}

The exterior differential of a linear differential form
\[
\omega=a_{i}dx^{i}
\]
is \emph{by convention} the form of degree two
\[
d\omega=[da_{i}dx^{i}];
\]
and in the same manner the exterior differential of a form of degree two
\[
\omega=\frac{1}{2}a_{ij}[dx^{i}dx^{j}]
\]is
\[
d\omega=\frac{1}{2}[da_{ij}dx^{i}dx^{j}]
\]
and so on.

This definition presupposes that the coefficients of the forms under consideration admit derivatives to first order, but we shall see that there are cases where we can define the exterior differentials of a form, even if the coefficients are not differentiable.

\theoremstyle{shape1}
\newtheorem*{gdef*}{\hspace{15pt}General definition}

\begin{gdef*}
  If $\omega$ is an exterior differential form of degree $p$\index{differential form}
  \begin{equation}
    \label{eq:2.1}
    \omega=\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}[dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}],
  \end{equation}
then its exterior differential $d\omega$ is
\begin{equation}
  \label{eq:2.2}
    d\omega=\frac{1}{p!}[da_{i_{1}i_{2}\dots i_{p}}dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}].
\end{equation}
\end{gdef*}

\vspace{12pt}\fsec We must show that the preceding definition is consistent. We  prove the following theorem:

\begin{thm*}
  If an exterior differential form $\omega(x,dx)$ transforms under a change of coordinates into the form $\varpi(y,dy)$, then the exterior differential $d\omega$ of the form $\omega$ transforms, under the same change of variables of the coordinates into the exterior differential form $d\varpi$ of the form $\varpi$.
\end{thm*}

We will prove several lemmas, important in themselves, before proving the theorem.

\newtheorem*{lem1*}{\hspace{15pt}Lemma I}

\begin{lem1*}
  The exterior differential of the differential of a function $f(x)$ vanishes.\index{exterior differential!of differential of a function}
\end{lem1*}
Indeed, consider the form
\[
\omega=\frac{\pd f}{\pd x^{i}}dx^{i},
\]
its exterior differential is by definition
\[
d\omega=\left[d\frac{\pd f}{\pd x^{i}}dx^{i}\right]=\frac{\pd^{2} f}{\pd x^{i}\pd x^{j}}[dx^{i}dx^{j}],
\]
and the right hand side is zero because of the symmetry of the coefficients of $[dx^{i}dx^{j}]$ with respect to their indices.

\newtheorem*{lem2*}{\hspace{15pt}Lemma II}
\begin{lem2*}\index{exterior differential!of exterior product}
  The exterior differential of the exterior product of two forms $\omega,\varpi$ of degrees respectively $p$ and $q$ is\footnote{In the particular case where one of the form is of degree $0$, {i.e.}, a function $a$, we have
\[
d(a\omega)=[da\,\omega]+a\,d\omega,\qquad\text{or}\qquad d(\omega a)= d\omega\, a+(-1)^{p}[\omega\, da].
\]}
\begin{equation}
  \label{eq:2.3}
  d(\omega\varpi)=[d\omega\varpi]+(-1)^{p}[\omega d\varpi].
\end{equation}
\end{lem2*}

Indeed, consider
\[
\omega=\frac{1}{p!}a_{i_{1}i_{2}\dots i_{p}}[dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}],\qquad
\varpi=\frac{1}{q!}b_{j_{1}j_{2}\dots j_{q}}[dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}].
\]

We have
\[
[\omega\varpi]=\frac{1}{p!}\frac{1}{q!}a_{i_{1}i_{2}\dots i_{p}}b_{j_{1}j_{2}\dots j_{q}}[dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}],
\]
from which
\[
d[\omega\varpi]=\frac{1}{p!}\frac{1}{q!}[d(a_{i_{1}i_{2}\dots i_{p}}b_{j_{1}j_{2}\dots j_{q}})dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}].
\]

We also have
\[
d(ab)=b\,da+a\,db,
\]
from which
\begin{align*}
d[\omega\varpi]&=\frac{1}{p!}\frac{1}{q!}b_{j_{1}j_{2}\dots j_{q}}[da_{i_{1}i_{2}\dots i_{p}}dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}]\\
&+\frac{1}{p!}\frac{1}{q!}a_{i_{1}i_{2}\dots i_{p}}[db_{j_{1}j_{2}\dots j_{q}}dx^{i_{1}}dx^{i_{2}}\dots dx^{i_{p}}dx^{j_{1}}dx^{j_{2}}\dots dx^{j_{q}}].
\end{align*}

The first sum on the right hand side is nothing but the exterior product $[d\omega\,\varpi]$. As for the second sum, it is multiplied by $(-1)^{p}$ when the factor $db_{j_{1}j_{2}\dots j_{q}}$ is passed to  behind the $p$-th factor $dx^{i_{p}}$, and up to  sign it is equal to $[\omega\,d\varpi]$. From this we deduce the formula
\[
d[\omega\varpi]=d[\omega\varpi]+(-1)^{p}[\omega\, d\varpi].
\]

This lemma generalises to products of any number of factors. For example
\begin{equation}
  \label{eq:2.4}
  d(\omega\varpi\chi)=[d\omega\,\varpi\chi]+(-1)^{p}[\omega \,d\varpi\,\chi]+(-1)^{p+q}[\omega\varpi\, d\chi],
\end{equation}
where $\omega$ is of degree $p$ and $\varpi$ is of degree $q$.

\vspace{12pt}\fsec We now come to the proof of the theorem. Take for example a differential form of degree $2$, and let
\[
\omega=a_{ij}[dx^{i}dx^{j}]
\]
to be the terms in the form. Let us express the variables $x^{i}$ using new variables $y^{i}$. The term under consideration is the exterior product of three factors, the first $a_{ij}$ a form of degree zero, and the second and third linear forms. We therefore have, passing to the variables $y^{i}$ and calling $\varpi$ the form obtained,
\[
d\varpi=[da_{ij}dx^{i}dx^{j}]+a_{ij}[(ddx^{i})dx^{j}]-a_{ij}[dx^{i}(ddx^{j})],
\]
but according to Lemma I the exterior differentials of $dx^{i}$ and $dx^{j}$ vanish and consequently the form $d\varpi$ is obtained by replacing in the expression $[da_{ij}dx^{i}dx^{j}]$ of $d\omega$ the coordinates $x^{i}$ by functions of $y^{i}$.

\vspace{12pt}\fsec We could, with regards to the linear differential forms
\[
\omega=a_{i}dx^{i},
\]
relate exterior differentiation to the notion of \emph{bilinear covariant}.\index{bilinear covariant}

Let us introduce a second symbol of differentiation $\delta$. We can regard the symbols $d\delta x^{i}$ as well as $\delta d x^{i}$ as constituting two new series of variables.  We  assume that the variables $\delta dx^{i}$ are identical to  $d\delta x^{i}$: this convention is consistent in the sense that it continues to hold with respect to a change of variables, since by expressing the new variables $y^{i}$ as functions of the old ones $x^{i}$, we have
\[
dy^{i}=\frac{\pd y^{i}}{\pd x^{k}}dx^{k},\qquad \delta dy^{i}=\frac{\pd^{2}y^{i}}{\pd x^{k}\pd x^{h}}\delta x^{h}dx^{k}+\frac{\pd y^{i}}{\pd x^{k}}\delta dx^{k},
\]
and
\[
\delta y^{i}=\frac{\pd y^{i}}{\pd x^{k}}\delta x^{k},\qquad d\delta y^{i}=\frac{\pd^{2}y^{i}}{\pd x^{k}\pd x^{h}}d x^{h}\delta x^{k}+\frac{\pd y^{i}}{\pd x^{k}}d\delta x^{k},
\]
and the comparison of $\delta d y^{i}$ and $d\delta y^{i}$ shows immediately their equality, if we take into consideration the symmetry properties of the second derivatives $\pd ^{2}y^{i}/\pd x^{k}\pd x^{h}$ with respect to the two indices of differentiation.

Denote by $\omega(d)$ and $\omega(\delta)$ the given differential forms according to whether we use the symbol of differentiation $d$ or $\delta$, and consider the expression
\[
d(\omega(\delta))-\delta(\omega(d))=d(a_{i}\delta x^{i})-\delta(a_{i}dx^{i})=da_{i}\delta x^{i}-\delta a_{a}dx^{i}+a_{i}(d\delta x^{i}-\delta dx^{i}).
\]
We obtain an alternating bilinear form in two series of variables $dx^{i}$ and $\delta x^{i}$. To this alternating bilinear form, which can be written
\[
\begin{vmatrix}
  da_{i}&dx^{i}\\
  \delta a_{i}&\delta x^{i}
\end{vmatrix},
\]
is associated the exterior form $[da_{i}dx^{i}]$, which is nothing but the exterior differential of the form $a_{i}dx^{i}$. The obvious \emph{intrinsic} (covariant) character of the expression $d\omega(\delta)-\delta\omega(d)$ immediately entails the intrinsic character of the form $[da_{i}dx^{i}]$.

Also, to the exterior differential of a quadratic exterior form $\omega$ we can associate  the alternating trilinear form
\[
d_{1}\omega(d_{2},d_{3})-d_{2}\omega(d_{1},d_{3})+d_{3}\omega(d_{1},d_{2}),
\]
which involves three symbols of differentiations that are exchangeable amongst themselves.
\index{exterior differential!of exterior differential|see{Poincar\'e's theorem}}
\vspace{12pt}\addtocounter{frenchsec}{1}\index{Poincare's theorem@Poincar\'e's theorem}
\theoremstyle{shape1}
\newtheorem*{thm28}{\hspace{15pt}\textbf{28.} Poncar\'e's Theorem}

\begin{thm28}
  The exterior differential of the exterior differential of a differential form vanishes.
\end{thm28}

The proof is immediate. It suffices to verify the theorem for a monomic form
\[
\omega=a[dx^{1}dx^{2}\dots dx^{p}].
\]
We have
\[
d\omega=[da\,dx^{1}dx^{2}\dots dx^{p}].
\]
As each factor on the right hand side is an exact differential, the theorem obviously follows from the formula for the exterior differential of products.

Take for example, \emph{as a verification}, the form
\begin{equation}
  \label{eq:2.5}
  \omega=P\,dx+Q\,dy+R\,dz,
\end{equation}
we have
\begin{align}
  \label{eq:2.6}
  d\omega&=[dP\,dx]+[dQ\,dy]+[dR\,dz]\\
  &=\left(\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}\right)[dy\,dz]+\left(\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}\right)[dz\,dx]+\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)[dx\,dy].\notag
\end{align}

Exterior differentiating a second time, we obtain the cubic form
\begin{align*}
  &\left[d\left(\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}\right)dy\,dz\right]+
  \left[d\left(\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}\right)dz\,dx\right]+
  \left[d\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)dx\,dy\right]\\
=&\left\{
\frac{\pd}{\pd x}\left(\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}\right)+
\frac{\pd}{\pd y}\left(\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}\right)+
\frac{\pd}{\pd z}\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)\right\}[dx\,dy\,dz]=0.
\end{align*}

Poincar\'e's theorem admits a converse, which will not be very helpful for us. Here is the theorem.
\begin{thm*}\index{Poincare's theorem@Poincar\'e's theorem!converse}
  If a differential form of degree $p$ whose exterior differential vanishes is defined in the domain $\mathcal{D}$ homeomorphic to to the interior of a hypersphere, then there exists a differential form of degree $p-1$ defined in the same domain, whose exterior differential is the given form.
\end{thm*}

Let us for the moment content ourselves with the verification of the theorem for $p=1$. The form \eqref{eq:2.5} for example has its exterior differential equal to zero since we have, according to \eqref{eq:2.6},
\[
\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}=0,\qquad
\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}=0,\qquad
\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}=0,
\]
which are the necessary and sufficient conditions for the form $P\,dx+Q\,dy+R\,dz$ to be an exact differential form, {i.e.}, the exterior differential of an exterior form of degree zero, or a function $f(x,y,z)$.

\section[{Exterior differentiation and the general Stokes' formula}]{Exterior differentiation\\and the general Stokes' formula}
\label{sec:exter-diff-gener}

\fsec The classical formulae of Cauchy-Green,  Stokes and  Ostrogradsky point to a remarkable link between the operation of exterior differentiation and  of integral calculus, which consists of converting an integral over the boundary of a $p+$ dimensional domain in the space to an equal integral over the domain itself. Consider for example the Cauchy-Green formula
\[
\int_{C}P\,dx+Q\,dy=\iint_{A}\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)dx\,dy,
\]
in which the left hand side is a line integral over the close contour $C$ of a plane and  right hand side is a double integral over the area $A$ bounded by the contour. As the exterior differential of $P\,dx+Q\,dy$ is
\[
[dP\,dx]+[dQ\,dy]=\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)[dx\,dy],
\]
we see that Cauchy's formula can be written as
\begin{equation}
  \label{eq:2.7}
  \int_{C}\omega=\iint_{A}d\omega.
\end{equation}

Note however, for this formula to be meaningful, the curve $C$ and the area $A$ must be oriented in a consistent way, and the sign of each of the integral is only determined when the curve or the area over which the integral is taken is oriented. In fact, the rule in this case is as follows. We first orient the area $A$ by agreeing that the area of the parallelogram constructed by two vectors $\vec{e}_{1}, \vec{e}_{2}$ in the order $\vec{e}_{1},\vec{e}_{2}$ is positive, and then the orientation of the curve $C$ is the following: on each point of $C$ we have a vector $\vec{\varepsilon}_{1}$ pointing out of the area, and this leads to a vector $\vec{\varepsilon}_{2}$ tangent to the boundary $C$ subject to the condition that the system $(\vec{\varepsilon}_{1},\vec{\varepsilon}_{2})$ is of positive orientation.

Stokes' formula can also be put into the form \eqref{eq:2.7} by taking $\omega$ to be the form \eqref{eq:5} and $d\omega$ the form \eqref{eq:2.6}, the first integral defined over a curve $C$ bounding over a portion of the surface $A$, and the second integral defined over the portion of the surface. There is also a consistent choice of orientation here for the boundary and the area, which is, \emph{mutatis mutandis}, the same  as indicated in the case of Cauchy's formula.

In fact, Ostrogradsky's formula takes the same form \eqref{eq:2.7} by putting
\begin{align*}
  \omega&=P[dy\,dz]+Q[dz\,dx]+R[dx\,dy],\\
  d\omega&=[dP\,dy\,dz]+[dQ\,dz\,dx]+[dR\,dx\,dy]=\left(\frac{\pd P}{\pd x}+\frac{\pd Q}{\pd y}+\frac{\pd R}{\pd z}\right)[dx\,dy\,dz],
\end{align*}
the integral $\int \omega$ is defined over a closed surface $S$ bounding a volume $V$ and the integral $\int d\omega$ is defined over the volume. We orient the volume by giving a triad $\vec{e}_{1},\vec{e}_{2},\vec{e}_{3}$ which can be drawn as a right trihedral, and the volume of the oriented parallelepiped constructed by the three vectors $\vec{e}_{1},\vec{e}_{2},\vec{e}_{3}$ is taken to be positive. To orient the surface $S$, we take at a point $M$ of the surface three vectors $\vec{\varepsilon}_{1},\vec{\varepsilon}_{2},\vec{\varepsilon}_{3}$ forming a right trihedral, the first of which pointing out of the volume $V$ and the two other tangent to the surface $S$. Then the oriented parallelogram constructed from $\vec{\varepsilon}_{2}$ and $\vec{\varepsilon}_{3}$ is considered positive, which orients the surface.

We can prove that with analogous conventions of orientations, Stokes' formula becomes very general\index{Stokes' formula}
\[
\int \omega=\int d\omega,
\]
the first integral being defined over a $p$-dimensional boundary of a $p+1$-dimensional domain over which the the second integral is defined.

\vspace{12pt}\fsec If the exterior differential of a form $\omega$ of degree $p$ vanishes (following de Rham, we call such forms \emph{closed}\index{differential form!closed}), the integral $\int\omega$ over the boundary of a domain of $p+1$ dimensions is zero. In cases where the conditions we have implicitly assumed when we calculate the exterior differential does not hold ({i.e.}, the condition that the coefficients are differentiable), it may happen that the integral $\int \omega$ over the boundary of any $p+1$ dimensional domain is still zero.  In this case we say that the form has vanishing exterior differential. More generally, given a form $\omega$ of degree $p$, if there exists a form $d\omega$ of degree $p+1$ such that the general Stokes' formula \eqref{eq:2.7} holds for any domain of $p+1$ dimensions, we then say that the form $d\omega$ is the exterior differential of the form $\omega$, even if the coefficients of $\omega$ are \emph{only continuous} functions.

\vspace{12pt}\fsec We can prove Poincar\'e's theorem\index{Poincare's theorem@Poincar\'e's theorem} easily in the case where the coefficients of the exterior differential $d\omega$ of a form $\omega$ are not differentiable: it suffices to suppose the existence of the exterior differential. Indeed suppose $\omega$ is of degree $p$, then the general Stokes' formula tells us that the integral $\int d\omega$ over any $p+1$ dimensional domain is equal to the integral $\int \omega$ over the boundary of the domain. Then let $\Sigma$ be a closed manifold of $p+1$ dimensions, and divide it into two parts $\Sigma_{1}$ and $\Sigma_{2}$ by a closed manifold $C$ of $p$-dimensions. The integral $\int d\omega$ over $\Sigma_{1}$ and the integral $\int d\omega$ over $\Sigma_{2}$ are both equal to the integral $\int \omega$ over $C$, but the first integral with respect to a certain orientation of $C$ and the second the opposite orientation. The total integral $\int d\omega$ over $\Sigma$ is then zero: the form $d\omega$ is closed, which proves Poincar\'e's theorem.
\vspace{12pt}\fsec \index{Poincare's theorem@Poincar\'e's theorem!converse}We can also prove the converse of Poincar\'e's theorem set out in \textsection
\textbf{22}. We will limit ourselves to the case $p=2$, which suffices to show how the proof could proceed in the general case.\footnote{This proof and exposition is due to \textsc{Henri Cartan}, {c.f.}, for the particular case considered here, an article by \textsc{A.~Sz\"ucs}, \emph{Sur la variation des int\'egrales triples et le th\'eor\`eme de Stokes} (Acta Szeged, 3, 1927, pp.~81--95).}

Suppose $n=3$ and we place in the space a sphere $\Sigma$ with centre $O$.  Consider then the closed form
\begin{equation}
  \label{eq:2.8}
  \omega=P[dy\,dz]+Q[dz\,dx]+R[dx\,dy]
\end{equation}
whose integral over any closed surface inside the sphere $\Sigma$ have the value zero. Let $(C)$ be a closed curve lying inside the sphere. We are going to calculate the integral $\int\omega$ over the surface $S$ of the cone with the apex $O$ and the base curve $(C)$. We assume that the coordinates $x,y,z$ of a point under consideration of $(C)$ are expressed as a function of a curvilinear abscissa traced in the direction determined by the orientation of the curve. Every point $M$ on the surface $S$ have coordinates of the form
\[
X=tx,\qquad Y=ty,\qquad Z=tz,\qquad(0\le t\le 1),
\]
$x,y,z$ denoting the coordinates of the the point where the generating line $OM$ meets $(C)$. We then have
\[
\omega(X,Y,Z;dX,dY,dZ)=P(X,Y,Z)[(t\,dy+y\,dt)(t\,dz+z\,dt)]+\dots
\]
but $x,y,z$ are functions of the single parameter $s$, and hence the monomial $[dy\,dz]$ vanishes and we have
\[
[(t\,dy+y\,dt)(t\,dz+z\,dt)]=t[dt(y\,dz-z\,dy)],
\]
and consequently
\begin{align}
  \label{eq:2.9}
  \iint_{S}\omega&=\iint_{S}t\left[P(X,Y,Z)\left(y\frac{dz}{ds}-z\frac{dy}{ds}\right)+Q(X,Y,Z)\left(z\frac{dx}{ds}-x\frac{dz}{ds}\right)\right.\\
&+\left.R(X,Y,Z)\left(x\frac{dy}{ds}-y\frac{dx}{ds}\right)\right]dt\,ds\notag
\end{align}

The orientation of $S$ consistent with that of $(C)$ is that of of regarding  the area of a small parallelogram constructed with two vectors originating from the point $(x,y,z)$ of $(C)$, the first $\vec{\varepsilon}_{1}$ along the generating lines point out of $S$, {i.e.}, increasing $t$, the second $\vec{\varepsilon}_{2}$ along the tangent to $(C)$, {i.e.}, increasing $s$, as positive. If we regard $t$ and $s$ as the rectangular coordinates of a plane, this amounts to taking the rotation that takes the $t$-axis to the $s$-axis as positive. We then have
\[
\iint_{S}H\,dt\,ds=\int_{0}^{l}ds\int_{0}^{1}H\,dt,
\]
where $l$ is the length of $(C)$, and $H$ is the coefficient of $dt\,ds$ on the right hand side of the equation \eqref{eq:2.9}. By putting

\begin{equation}
  \label{eq:2.10}
  \left\{
    \begin{aligned}
      A(x,y,z)&=\int_{0}^{1}t\,P(X,Y,Z)\,dt,\\
      B(x,y,z)&=\int_{0}^{1}t\,Q(X,Y,Z)\,dt,\\
      C(x,y,z)&=\int_{0}^{1}t\,R(X,Y,Z)\,dt,\\
    \end{aligned}
  \right.
\end{equation}
we have
\[
\iint_{S}\omega=\int_{(C)}A(y\,dz-z\,dy)+B(z\,dx-x\,dz)+C(x\,dy-y\,dx).
\]

By putting
\begin{equation}
  \label{eq:2.11}
  \varpi=A(y\,dz-z\,dy)+B(z\,dx-x\,dz)+C(x\,dy-y\,dx)
\end{equation}
we then obtain the relation
\[
\iint_{S}\omega=\int_{(C)}\varpi.
\]

The form $\omega$ is closed, and the integral $\iint\omega$ over any surface bounded by the curve $(C)$ (and lying inside of the sphere $\Sigma$) is the same as $\iint_{S}d\varpi$. Consequently \emph{we have constructed a form $\varpi$ of degree one which has its exterior differential the form $\omega$ that was assumed to be closed.}

\vspace{12pt}\fsec We can verify that \emph{in the case where the coefficients $P,Q,R$ of the form $\omega$ are differentiable}, the exterior differential of the form $\varpi$ is equal to $\omega$. Indeed, the form $\omega$ being closed, we have the relation
\begin{equation}
  \label{eq:2.12}
  \frac{\pd P}{\pd x}+\frac{\pd Q}{\pd y}+\frac{\pd R}{\pd z}=0.
\end{equation}

On the other hand we have
\[
d\varpi=\left\{ 2A+x\frac{\pd A}{\pd x}+y\frac{\pd A}{\pd y}+z\frac{\pd A}{\pd z}-x\left(\frac{\pd A}{\pd x}+\frac{\pd B}{\pd y}+\frac{\pd C}{\pd z}\right) \right\}[dy\,dz]+\dots
\]

We immediately see that $\pd A/\pd x+\pd B/\pd y+\pd C/\pd z$ is zero as a consequence of \eqref{eq:2.12}. On the other hand a simple calculation gives
\begin{align*}
  2A+x\frac{\pd A}{\pd x}+y\frac{\pd B}{\pd y}+z\frac{\pd C}{\pd z}&=\int_{0}^{1}\left[2t\,P(X,Y,Z)+t\left(X\frac{\pd P}{\pd X}+Y\frac{\pd P}{\pd Y}+Z\frac{\pd P}{\pd Z}\right)\right]dt\\
  &=\int_{0}^{1}\frac{\pd}{\pd t}[t^{2}P(X,Y,Z)]dt=P(x,y,z),
\end{align*}
which gives
\[
d\varpi=\omega.
\]

\vspace{12pt}\addtocounter{frenchsec}{1}

\theoremstyle{shape0}
\newtheorem*{rmk34}{\hspace{15pt}\textbf{34.} Remark I}
\newtheorem*{rmkii}{\hspace{15pt}Remark II}
\begin{rmk34}
  Sometimes the integral $\int\omega$, where $\omega$ is a closed differential form of degree $p$, does not vanish when it is taken over certain closed manifolds of $p$ dimensions: this happens if there does not exist a $p+1$ dimensional domain in which this manifold is a boundary of or if the domain contains points where the coefficients of the form becomes infinite. This is so for the integral
\[
\iint\frac{x\,dy\,dz+y\,dz\,dx+z\,dx\,dy}{(x^{2}+y^{2}+z^{2})^{3/2}},
\]
which is zero if it is taken over a closed surface that does not contain the origin in its interior, but not otherwise, in which case its value becomes the value of the solid angle when the surface is viewed from the origin ($4\pi$ for the sphere). In all cases the value of the integral does not vary when the closed surface is continuously deformed without passing through the origin.  
\end{rmk34}
\begin{rmkii}
  Finally we can prove that whenever we can define the exterior differential of a form $\omega$ with the procedure in \textsection\textbf{24}, the general theorems stated and proved in the case where we used the primitive definition by  derivation of coefficients,  continue to be true in a certain sense. For example, the formula
\[
d[\omega\varpi]=[d\omega\varpi]+(-1)^{p}[\omega d\varpi]
\]
is true whenever $\omega$ and $\varpi$ admit generalised exterior differentials.\footnote{Regarding this subject, see a memoir of \textsc{P.~Gillis}, \emph{Sur les formes diff\'erentielles et la formule de Stokes} (M\'em.~Acad.~Blgique, 20, 1943).}
\end{rmkii}

\chapter{Exterior differential systems. Characteristic system}
\label{cha:exter-diff-syst}
\index{exterior differential system}
\section[Generalities. Completely integrable systems]{Generalities.\\Completely integrable systems}
\label{sec:gener-compl-integr}

\fsec The differential systems that we are going to study are obtained by setting to zero a certain number of functions of $n$ variables $x^{1},x^{2},\dots,x^{n}$, which we always consider as the coordinates of  points in a $n$ dimensional space, or a domain $\mathcal{D}$ therein, and a certain number of exterior differential forms defined in the domain that may have any degree.  In the general case we are obliged  to assume that  the functions that we introduce are  \emph{analytic} and we further assume  that they are analytic on the real domain\footnote{For more details on the analytic functions of real variables, see \textsc{G.~Valiron}, \emph{Sur les fonctions analytiques d'une variable r\'eelle} (Nouvelles Annales, 1, 1922, pp.~321--329).}. The problems that we will be concerned with and the theorems we will prove will always be local in character. Clearly we can still change variables, but in  problems where the variables are assumed to be analytic, the new variables are necessarily analytic functions of the old ones.

\vspace{12pt}\fsec \index{exterior differential system!linear}The differential systems of linear equations have been extensively studied. Let
\begin{equation}
  \label{eq:3.1}
  \theta_{\alpha}\equiv A_{\alpha i}dx^{i}=0\qquad(\alpha=1,2,\dots,r)
\end{equation}
be the equations of such a system. We suppose that the linear forms $\theta_{\alpha}$ are linearly independent when we give the variables $x^{i}$ in the coefficients $A_{\alpha i}$ generic values. In general, we say that the point $(x^{i})$ in the space is generic\index{generic point} if the rank of the matrix of the coefficients $A_{\alpha i}$  at the point is equal to $r$.

An \emph{integral variety}\index{integral variety}\index{integral submanifold|see{integral variety}} of  system \eqref{eq:3.1} is  the submanifold defined by a certain number of relations between the variables such that the relations between the variables and those that  can be deduced from them by differentiation imply the identical vanishing of the forms $\theta_{\alpha}$. This naturally presupposes that the left hand side of the equations defining the submanifold are differentiable.

\emph{In particular}, consider integral varieties of $(n-r)$ dimensions.  Let us find whether there exists an integral variety in the space passing through a \emph{given generic} point $M_{0}$ with coordinates $(x^{i})_{0}$. Suppose at the point the degree $r$ determinant constructed with the $r$ rows and the last $r$ columns of the matrix $A_{\alpha i}$ is non-zero.  Near this point the equations \eqref{eq:3.1} can be solved with respect to differentials of  $r$ of the coordinates, which we will denote for convenience as $z^{1},z^{2},\dots,z^{r}$. We see that the integral variety, if it exists, can be defined by specifying  $z^{1},z^{2},\dots,z^{r}$ in terms of functions of $x^{1},x^{2},\dots,x^{n-r}$ in a suitable manner.

\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape1}
\newtheorem*{thm37}{\hspace{15pt}\textbf{37.} Theorem I}
\begin{thm37}
  If there exists an integral variety passing through a given generic point, we can obtain it by integrating a system of ordinary differential equations and the integral variety is unique.
\end{thm37}

Regard $x^{1},x^{2},\dots,x^{h}$ $(h=n-r)$ as the coordinates of points in an Euclidean space of $h$ dimensions and denote by $O$ the point with coordinates $(x^{1})_{0},(x^{2})_{0},\dots,(x^{h})_{0}$ in the space. Inside the hypersphere $\Sigma$ of centre $O$ and radius $R$ in this space we have the radii originating from $O$, each determined by the parameters $a^{1},a^{2},\dots,a^{h}$ of a unit vector based at the centre. For any integral variety found, the $z^{\alpha}$ $(\alpha=1,2,\dots,r)$ are the functions of the coordinates of points interior to $\Sigma$, and the coordinates can be written
\[
x^{1}=a^{1}t,\quad x^{2}=a^{2}t,\quad\dots\quad x^{h}=a^{h}t,\qquad(0\le t\le R).
\] 

When we move along on the ray, the unknown functions $z^{\alpha}$ satisfy the equations obtained by replacing, in the forms $\theta_{\beta}$, the $x^{i}$ by $a^{i}t$ and the $dx^{i}$ by $a^{i}dt$. We have therefore a system of ordinary differential equations
\begin{equation}
  \label{eq:3.2}
  \frac{d z^{\alpha}}{dt}=\varphi^{\alpha}(a^{1},a^{2},\dots,a^{h},t)\qquad(\alpha=1,2,\dots,r),
\end{equation}
and we can integrate these with the initial conditions
\[
z^{\alpha}=(x^{n-r+\alpha})_{0}\qquad\text{for}\qquad x^{i}=(x^{i})_{0}.
\]

For each ray we are sure that the integration can be carried out for a certain interval $(0,t_{0})$, $t_{0}$ being a continuous function of $a^{1},a^{2},\dots,a^{h}$. Hence $t_{0}$ has a lower bound, and it is this lower bound that we take the value of $R$ to be.

Therefore we see that if there exists an integral variety of $n-r$ dimensions passing through the point $M_{0}$, then it is given, in the interior of the hypersphere $\Sigma$, by integration of a system of ordinary differential equations and it is unique.\qed

\vspace{12pt}\fsec \emph{Completely integrable systems of differential equations}.\index{exterior differential system!completely integrable}\index{Frobenius' theorem} \emph{The system \eqref{eq:3.1} is completely integrable if for every generic point of the space and a sufficiently small neighbourhood around it, there passes an integral variety of dimension $n-r$.}

We have seen, in the previous section, how we can find the integral variety in the case where it exists.

To find the complete integrability condition of the system \eqref{eq:3.1}, we make the following remark, \emph{which plays a fundamental role in the general theory of differential systems}, and which can be no simpler, namely that \emph{every manifold that annihilates an exterior differential form also annihilates the form resulting from it by exterior differentiation}. All integral varieties of the system \eqref{eq:3.1} therefore must annihilate the $r$ forms $d\theta_{\alpha}$. If we calculate these forms we can, instead of expressing them as quadratic forms in $dx^{1},dx^{2},\dots,dx^{h},dz^{1},dz^{2},\dots,dz^{r}$, express them as a quadratic forms of the following $r$ forms, \emph{which are independent in a neighbourhood of $M_{0}$},
\[
dx^{1},dx^{2},\dots,dx^{h},\theta_{1},\theta_{2},\dots,\theta_{r}.
\]
We therefore write
\begin{equation}
  \label{eq:3.3}
  d\theta_{\alpha}=\frac{1}{2}C_{\alpha ij}[dx^{i}dx^{j}]+D^{\lambda}_{\alpha i}[dx^{i}\theta_{\lambda}]+\frac{1}{2}E_{\alpha}^{\lambda\mu}[\theta_{\lambda}\theta_{\mu}],
\end{equation}
in the formula the summation indices $i$ and $j$ varies from $1$ to $h$, and the summation indices $\lambda$ and $\mu$ from $1$ to $r$. All integral varieties passing through $M_{0}$, which annihilate the form $\theta_{\alpha}$, will annihilate the form $\tfrac{1}{2}C_{\alpha ij}dx^{i}dx^{j}$ and therefore, on all points of this manifold lying in a neighbourhood of $M_{0}$, the coefficients $C_{\alpha ij}$ are zero. As these must be zero regardless of the initial values for the functions $z^{\alpha}$ for $x^{i}=(x^{i})_{0}$, we can conclude that the functions $C_{\alpha ij}$ must be zero in a sufficiently small neighbourhood of $M_{0}$, and we have the

\begin{thm*}
   The system \eqref{eq:3.1} is completely integrable if in the neighbourhood in every generic point of the space, the forms $d\theta_{\alpha}$, which are exterior differentials of the forms $\theta_{\alpha}$, belong to the ring of the forms $\theta_{\alpha}$. \rm{We can express the condition by writing it as a congruence}
\[
d\theta_{\alpha}\equiv 0\pmod{\theta_{1},\theta_{2},\dots,\theta_{r}}.
\]
\end{thm*}

This can again be expressed in a more precise manner, as we have already seen, by using the existence of linear forms $\varpi^{i}{}_{\alpha}$ regular in a neighbourhood of a generic point under consideration in the space to write
\begin{equation}
  \label{eq:3.4}
  d\theta_{\alpha}=[\theta_{1}\varpi^{1}_{\alpha}]+[\theta_{2}\varpi^{2}_{\alpha}]+\dots+[\theta_{r}\varpi^{r}_{\alpha}].
\end{equation}

\vspace{12pt}\fsec We now prove the converse. Let us go back to the manifold of $n-r$ dimensions defined by the differential system \eqref{eq:3.2} and passing through the point $M_{0}$ of coordinates $(x^{i})_{0}$, $(z^{\alpha})_{0}$. If we replace the functions $z^{\alpha}$ by their values as functions of the arguments $a^{i}$, $t$, we have, according to the same manner that we have obtained them,
\[
\theta_{\alpha}=P_{\alpha k}(a,t)da^{k}\qquad(\alpha=1,2,\dots,r),
\]
and also
\[
\varpi^{i}_{\alpha}=Q^{i}_{\alpha}(a,t)dt+Q^{i}_{\alpha k}(a,t)da^{k}\qquad(\alpha=1,2,\dots,r).
\]

The relations \eqref{eq:3.4} will only hold when the two terms containing $dt$ satisfies
\[
\frac{\pd P_{\alpha k}}{\pd t}[dt\,da^{k}]=P_{\lambda k}Q^{\lambda}_{\alpha}[da^{k}dt],
\]
or
\begin{equation}
  \label{eq:3.5}
  \frac{\pd P_{\alpha i}}{\pd t}+Q^{\lambda}_{\alpha}P_{\lambda i}=0.
\end{equation}

For each value of the index $i$, the $r$ functions $P_{\alpha i}(a,t)$, considered as functions of $t$, satisfy a linear and homogeneous system of equations (a system which remains the same for any value of $i$). For $t=0$, the functions $P_{\alpha i}$ are all zero since for $t=0$, the functions $x^{i}$ and $z^{\alpha}$ are fixed and independent of $a^{k}$, and hence when we set $t=0$ in their coefficients, their differentials does not contain terms in $da^{1},da^{2},\dots,da^{k}$. The initial values of the unknown functions $P_{\alpha i}$ of system \eqref{eq:3.5} are zero, hence these functions are identically zero, and therefore the submanifold determined by integration of equations \eqref{eq:3.2} identically annihilate the forms $\theta_{\alpha}$, hence it is an integral variety.

We express this result in the following form:
\begin{thm*}
  A system of $r$ linear differential equations is completely integrable if and only if at all generic points of the space, the exterior differentials $d\theta_{\alpha}$ of the forms $\theta_{\alpha}$ defining the equations of the system belong to the ring of these forms.
\end{thm*}


\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape0}
\newtheorem*{rmk40}{\hspace{15pt}\textbf{40.} Remark I}
\begin{rmk40}
  The complete integrability condition requires only the existence of an integral variety of $n-r$ dimensions passing through  \emph{generic} points of the space, and it is only in a neighbourhood of a generic point that the forms $d\theta_{\alpha}$ must belong to the ring of the forms $\theta_{\alpha}$. It can fail to be so in the neighbourhood of a non-generic point. For example, the equation
\[
\theta\equiv x\,dy-y\,dx=0
\]
is completely integrable, as any other ordinary differential equation. However we cannot find any linear form $\varpi=A\,dx+B\,dy$, regular and continuous in a neighbourhood of every given point in the plane and satisfy
\[
d\theta=[\theta\varpi]
\]
or, in other words,
\[
2[dx\,dy]=-(Ax+By)[dx\,dy].
\]
Indeed, for $x=y=0$, it is impossible to satisfy
\[
2=-(Ax+By).
\]
\begin{rmkii}
  We can put the complete integrability condition into a form which does not require us to specially check  for generic points. Indeed the equations \eqref{eq:3.4} give the relations
  \begin{equation}
    \label{eq:3.6}
    [\theta_{1}\theta_{2}\dots\theta_{r}d\theta_{\alpha}]=0\qquad(\alpha=1,2,\dots,r),
  \end{equation}
and conversely in the neighbourhood of any generic point where the linear forms $\theta_{1},\theta_{2},\dots,\theta_{r}$ are independent, these relations imply the existence of the forms $\varpi^{\alpha}_{\beta}$ satisfying the relations \eqref{eq:3.4}. As all non-generic points can be regarded as the limit of a sequence of generic points, the relations \eqref{eq:3.6} hold for all generic points and continue to hold for all non-generic points.

\emph{The relations \eqref{eq:3.6} therefore give the necessary and sufficient conditions for complete integrability of system \eqref{eq:3.1}}, and is of a more convenient form than the original criterion  we  found.
\end{rmkii}

\newtheorem*{rmkiii}{\hspace{15pt}Remark III}

\begin{rmkiii}
  If the system is completely integrable, then there exists $r$ independent functions $\varphi_{i}(x^{1},x^{2},\dots,x^{n})$ defined in a neighbourhood of a generic point in the space and they are constants on any integral variety (\emph{first integrals} of the system), so that the given system is equivalent to the system $d\varphi_{1}=d\varphi_{2}=\dots=d\varphi_{r}=0$. The converse is obvious.
\end{rmkiii}
\end{rmk40}

\newtheorem*{rmkiv}{\hspace{15pt}Remark IV}
\begin{rmkiv}
  Everything that has been said does not require the analyticity of the coefficients $A_{\alpha i}$ of the given equations, and only the existence of first order partial derivatives of the coefficients is needed \footnote{The existence of the partial derivatives are necessary  to ensure the existence of the forms $d\theta_{\alpha}$.}, the reason being that we have reduced the task of finding  integral varieties to the integration of ordinary differential equations.
\end{rmkiv}

\section{Closed differential systems. Characteristic system}
\label{sec:clos-diff-syst}

\fsec Let us return to differential equations obtained by setting to zero a certain number of exterior differential forms, some of which can be of degree zero, {i.e.,} functions of some variables. A solution of such a system can be regarded as analytically representing  a submanifold in the space of $n$ dimensions, which we will call \emph{integral variety}\index{integral variety}. The integral variety is defined by a certain number of relations between the variables such that by differentiating these relations and the linear relations between the differentials $dx^{1},dx^{2},\dots,dx^{n}$, we can identically deduce the  vanishing of the differential forms that define the given system. If this system involves any algebraic relation between the variables, these relations will necessarily appear in the relations of any integral variety.

It is clear that any integral variety is a solution of the given system adjoined by all equations obtained by exterior differentiation of the equations in the system, since if a differential form vanishes, its exterior differential also vanishes. The new differential system obtained  obviously cannot be extended further with the same procedure, according to Poincar\'e's theorem.

\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape1}
\newtheorem*{dfn42}{\hspace{15pt}\textbf{42.} Definition}
\begin{dfn42}
  A differential system is closed\index{exterior differential system!closed} with respect to the operation of exterior differentiation, or simply closed, if the exterior differentials of the forms defining the equations belong to the ring of these forms.
\end{dfn42}

It is clear that if we enlarge a system by adjoining to it the equations obtained by exterior differentiation, we obtain a closed system, since exterior differentiating the new system gives us only zero or equations already in the system. We say that the system thus enlarged is the \emph{closure}\index{exterior differential system!closure of} of the given system.

It is easily seen that if wo differential systems are algebraically equivalent, the closure of them are also algebraically equivalent.

Following the considerations in \textsection\textbf{41},  the  closure of a given system admit the same solutions as the original system, regardless of the dimension of integral varieties under consideration. We have the
\newtheorem*{prin*}{\hspace{15pt}Principle}
\begin{prin*}
  The search for  solutions of a differential system can always be reduced to the search for  solutions of a closed differential system.
\end{prin*}

\vspace{12pt}\fsec\emph{Characteristic system of a differential system}\index{characteristic system} We have seen in chapter 1 that a system of exterior equations can always be expressed, by means of change of variables and by replacing the given system by an algebraically equivalent system, as a system with a minimum number of variables: the number is well defined and is given by the rank of the associated system, and the new variables are linear combinations of the old variables and, when set to zero, give the associated system.

When we are concerned with a system of exterior differential equations, we can also ask if we can apply a change of variables and replace the system by an algebraically equivalent system in such a way that the new system contains in its coefficients and differentials a minimum number of variables. We will see that this is possible and the solution of the problem is provided by the consideration of the \emph{characteristic system}.

\begin{dfn*}
  The characteristic system of a differential system is the associated system of the closure of the given system.
\end{dfn*}

We are going to prove the following theorem:

\vspace{12pt}\addtocounter{frenchsec}{1}
\newtheorem*{thm44}{\hspace{15pt}\textbf{44.} Theorem}

\begin{thm44}
  The characteristic system of a differential system $\Sigma$ is completely integrable. Moreover, if $y^{1},y^{2},\dots,y^{p}$ constitute a system of independent first integrals, there then exists a system algebraically equivalent to $\Sigma$, constructed by the differentials $dy^{1},dy^{2},\dots,dy^{p}$, the coefficients being functions of $y^{1},y^{2},\dots,y^{p}$.
\end{thm44}

For simplicity and without loss of generality, we assume that the system $\Sigma$ does not contain algebraic equations in $x^{1},x^{2},\dots,x^{n}$. It suffices to prove the theorem in the case where the system $\Sigma$ is closed.  Consider the a system of degree $3$ defined by the following equations,
\begin{equation}
  \label{eq:3.7}
  \left\{
    \begin{aligned}
      \theta_{\alpha}&\equiv A_{\alpha i}dx^{i}=0&(\alpha&=1,2,\dots,r_{1}),\\
      \varphi_{\alpha}&\equiv \frac{1}{2}A_{\alpha ij}[dx^{i}dx^{j}]=0&(\alpha&=1,2,\dots,r_{2}),\\
      \psi_{\alpha}&\equiv \frac{1}{6}A_{\alpha ijk}[dx^{i}dx^{j}dx^{k}]=0&(\alpha&=1,2,\dots,r_{3}).
    \end{aligned}
  \right.
\end{equation}

If the rank of the characteristic system is equal to $n$, the theorem is trivial. If the rank is an integer $p<n$, in the case of $p<n-1$, let us adjoin to the equations of the characteristic system of another $n-1-p$ linear equations independent among themselves and with respect to the original equations. We then obtain a system of ordinary differential equations which we suppose, in order not to clutter notation, that the variables $x^{1},x^{2},\dots,x^{n-1}$ are first integrals.

First, we know that we can find a system algebraically equivalent to $\Sigma$ which does not contain the differential $dx^{n}$ (\textsection\textbf{22}).

Suppose that this has already been done, that the left hand sides of equations \eqref{eq:3.7} does not contain $dx^{n}$.  Notice that the derivative with respect to $x^{n}$ of any left hand side of these equations, $\varphi$ for example, is nothing but the derivative of the form $d\varphi_{\alpha}$ with respect to $dx^{n}$, and  it must also belong to the ring of the system. We therefore have the congruences
\begin{equation}
  \label{eq:3.8}
  \left\{
    \begin{aligned}
      \frac{\pd \theta_{\alpha}}{\pd x^{n}}&\equiv 0&&\pmod{\theta_{1},\theta_{2},\dots,\theta_{r_{1}}},\\
      \frac{\pd \varphi_{\alpha}}{\pd x^{n}}&\equiv 0&&\pmod{\varphi_{1},\dots,\varphi_{r_{2}},\theta_{1},\theta_{2},\dots,\theta_{r_{1}}},\\
      \frac{\pd \psi_{\alpha}}{\pd x^{n}}&\equiv 0&&\pmod{\psi_{1},\dots,\psi_{r_{3}},\varphi_{1},\dots,\varphi_{r_{2}},\theta_{1},\theta_{2},\dots,\theta_{r_{1}}}.
    \end{aligned}
  \right.
\end{equation}

The first congruences of \eqref{eq:3.8} can be written
\begin{equation}
  \label{eq:3.9}
  \frac{\pd \theta_{\alpha}}{\pd x^{n}}=H_{\alpha}^{\beta}\theta_{\beta}.
\end{equation}

Consider the system of ordinary differential equations
\begin{equation}
  \label{eq:3.10}
  \frac{dz_{\alpha}}{dx^{n}}=H^{\beta}_{\alpha}z_{\beta},
\end{equation}
where the coefficients $H_{\alpha}^{\beta}$ are functions of $x^{1},x^{2},\dots,x^{n}$. Let $\bar z^{(1)}_{\alpha},\bar z^{(2)}_{\alpha},\dots,\bar z^{(r_{1})}_{\alpha}$ be a system of $r_{1}$ independent solutions of the system. There will exist linear forms \emph{independent} of $x^{n}$, which we will denote by $\bar\theta_{1},\bar\theta_{2},\dots,\bar\theta_{r_{1}}$, such that we have
\[
\theta_{\alpha}=\bar\theta_{1}\bar z^{(1)}_{\alpha}+\bar\theta_{2}\bar z^{(2)}_{\alpha}+\dots+\bar\theta_{r_{1}}\bar z^{(r_{1})}_{\alpha},
\]
but then the system of equations $\theta_{\alpha}=0$ is equivalent to the system of equations $\bar\theta_{\alpha}=0$ whose left hand side does not involve either $x^{n}$ or $dx^{n}$. Consequently, we can suppose, by replacing $\Sigma$ by an algebraically equivalent system, the the forms $\theta_{\alpha}$ contains neither $x^{n}$ nor $dx^{n}$.

Consider now the forms $\varphi_{\alpha}$. We have
\begin{equation}
  \label{eq:3.11}
  \frac{\pd \varphi_{\alpha}}{\pd x^{n}}=K^{\beta}_{\alpha}\varphi_{\beta}+[\varpi_{\alpha}^{\gamma}\theta_{\gamma}],
\end{equation}
the $\varpi_{\alpha}^{\gamma}$ being linear forms not involving $dx^{n}$ and the $\theta_{\gamma}$ depends on neither $x^{n}$ nor $dx^{n}$.

Consider this time the system of differential equations
\begin{equation}
  \label{eq:3.12}
  \frac{du_{\alpha}}{dx^{n}}=K_{\alpha}^{\beta}u_{\beta}\qquad(\alpha=1,2,\dots,r_{2})
\end{equation}
and a system of $r_{2}$ independent solutions $\bar u_{\alpha}^{(i)}$ $(i=1,2,\dots,r_{2})$. Set
\[
\varphi_{\alpha}=\varphi_{\beta}^{*}\bar u_{\alpha}^{(\beta)},
\]
where the $\varphi^{*}_{\beta}$ are new quadratic exterior forms. The system \eqref{eq:3.11} take the form
\[
\frac{\pd \varphi_{\alpha}^{*}}{\pd x^{n}}=[(\varpi_{\alpha}^{\beta})^{*}\theta_{\beta}].
\]
If we denote by $\chi_{\alpha}^{\beta}$ the primitive functions of $(\varpi_{\alpha}^{\beta})^{*}$, considered as functions of $x^{n}$,  the form
\[
\varphi_{\alpha}^{*}-[\chi_{\alpha}^{\beta}\theta_{\beta}]
\]
depends on neither $x^{n}$ nor $dx^{n}$. Now the system $\theta_{\alpha}=\varphi_{\alpha}^{*}=0$ is algebraically equivalent to the system $\theta_{\alpha}=\varphi_{\alpha}=0$. We can therefore suppose, by replacing $\Sigma$ by an algebraically equivalent system, that the left hand side of first and second degree equations of \eqref{eq:3.7} depends on neither $x^{n}$ nor $dx^{n}$.

We continue in the same manner for the left hand sides of  third degree equations and step by step we complete the proof of the existence of a system algebraically equivalent to the given system which does not contain $x^{n}$ or $dx^{n}$.

If $p<n-1$, we apply the same procedure on this system $\Sigma$ to obtain an algebraically equivalent system which involves only $n-2$ variables. Continue further in the same way we will eventually arrive at a system which contains only $p$ variables and their differentials.

The theorem is hence proven, since if the $p$ variables are $y^{1},y^{2},\dots,y^{p}$, the characteristic system is
\[
dy^{1}=0,\qquad dy^{2}=0,\qquad \dots\qquad dy^{p}=0,
\]
and this system is completely integrable and its most general integral variety is obtained by setting $y^{1},y^{2},\dots,y^{p}$ to arbitrary constants.

On the other hand, it is evident that we can never find a system algebraically equivalent to the given system which involves less than $p$ variables and their differentials\footnote{The application of the theorem in \textsection\textbf{38} to prove that the characteristic system is completely integrable is a simple  exercise in calculus if the given system is linear. We leave this calculation aside.}.\qed

The \emph{class}\index{class of exterior differential system} of an exterior differential system is the rank of its characteristic system.


\vspace{12pt}\fsec \textsc{Definition.}  {A} \emph{characteristic variety}\index{characteristic variety} {is a $n-p$ dimensional submanifold that is a solution of the characteristic system. The following property is evident.}

\begin{thm*}
  Given any integral variety $V$ of a system $\Sigma$, the manifold obtained by enlarging with the characteristic variety of each point of $V$ passing through the point is also an integral variety.
\end{thm*}

In particular, this leads to the 
\begin{thm*}
  If the integral variety $V$ of a system $\Sigma$ is not contained in any integral variety of larger dimensions, then it is generated by  characteristic varieties \footnote{ This theorem would fail if the rank of the characteristic system is less than its usual value  at every point of the integral variety $V$. We would then be dealing with a singular integral variety. An example is provided by the singular solutions of an first order partial differential equation.}.
\end{thm*}

Indeed, if it were not so, the characteristic varieties at  different points of $V$ would together form an integral variety with more dimensions that $V$.

\vspace{12pt}\fsec If the system $\Sigma$ is not complete (\textsection\textbf{18}) and are able to complete it, we would pass from the characteristic system of $\Sigma$ to a new characteristic system whose rank may drop. If the rank remains the same, the characteristic system does not change.

Take as an example the system
\begin{equation}
  \label{eq:3.13}
  [dx^{1}dx^{3}]=[dx^{1}dx^{4}]=[dx^{3}dx^{4}]-x^{5}[dx^{1}dx^{3}]=0,
\end{equation}
which is closed by the new equation
\begin{equation}
  \label{eq:3.14}
  [dx^{1}dx^{2}dx^{5}]=0.
\end{equation}
The characteristic system is
\[
dx^{1}=dx^{2}=dx^{3}=dx^{4}=dx^{5}=0.
\]

The system \eqref{eq:3.13} is not complete. A complete system admitting the same solutions as the system \eqref{eq:3.13} is provided by the equations
\begin{equation}
  \label{eq:3.15}
  [dx^{1}dx^{3}]=[dx^{1}dx^{4}]=[dx^{1}dx^{2}]=[dx^{3}dx^{4}]=0.
\end{equation}

The characteristic system is
\[
dx^{1}=dx^{2}=dx^{3}=dx^{4}=0.
\]

\section{Applications to the Pfaffian problem}
\label{sec:appl-pfaff-probl}
\index{Pfaffian problem}
\fsec Now we apply the preceding results to the case of a single linear differential equation (Pfaffian equation)
\begin{equation}
  \label{eq:3.16'}
  \theta\equiv A_{i}dx^{i}=0.
\end{equation}
The characteristic system is formed by the equation \eqref{eq:3.16'} together with the associated system of the exterior differential $d\theta$ after replacing any differential $dx^{i}$ by its value determined by \eqref{eq:3.16'}. The associated system being of even rank, it gives the

\begin{thm*}
  For every linear differential equation, the class is an odd number.
\end{thm*}

If the class is equal to $1$, then the characteristic system must be the equation \eqref{eq:3.16'}, which is therefore completely integrable. If $Z$ is a first integral, the given equation is equivalent to $dZ=0$.

In the general case, let the class be $2p+1$. Let $X_{1}$ be a  first integral of the characteristic system. If we constrain the $n$ variables by the equation $X^{1}=C^{1}$ where $C^{1}$ is an arbitrary constant, we remove at least one equation from the characteristic system, but as the class is always odd, we have removed at least two equations.  Let $X^{2}$ be a first integral of the new characteristic system, $X^{2}$ being a function of $x^{i}$ and $C^{1}$, or just of $x^{i}$ (if we replace $C^{1}$ by $X^{1}$). By constraining the variables with the two relations
\[
X^{1}=C^{1},\qquad X^{2}=C^{2},
\]
{i.e.}, constraining the differentials $dx^{i}$ by the two relations
\[
dX^{1}=0,\qquad dX^{2}=0,
\]
the class of the system is again reduced by at least two. Continuing step by step,  there will be $p$ independent first integrals $X^{1},X^{2},\dots,X^{p}$ such that if we constrain the variables by the $p$ relations
\[
X^{1}=C^{1},\qquad X^{2}=C^{2},\qquad\dots\qquad X^{p}=C^{p},
\]
the equation $\theta=0$ becomes completely integrable, and hence reducible to the form $dZ=0$.

This reduction is only valid because we have assumed that the functions $X^{i}$ are constants. If we no longer assume it, then the equation $\theta=0$ is reducible to the form
\[
dZ-Y_{1}dX^{1}-Y_{2}dX^{2}-\dots-Y_{p}dX^{p}=0,
\]
the coefficients $Y_{i}$ being $p$ suitably chosen functions of the given variables. The functions $X^{i}$, $Y_{i}$ and $Z$ now constitute a system of $2p+1$ independent functions, otherwise the equation \eqref{eq:3.16'} can be expressed by  less than $2p+1$ variables, which would imply that the class is less than $2p+1$.

\begin{thm*}
  Every Pfaffian form of class $2p+1$ is reducible to the canonical form
\[
dZ-Y_{1}dX^{1}-Y_{2}dX^{2}-\dots-Y_{p}dX^{p}=0.
\]
\end{thm*}


\vspace{12pt}\fsec  \textsc{Remark.} {We immediately see that if the equation \eqref{eq:3.16'} is of class $2p+1$, the integer $p$ is the largest integer such that the form of degree $2p+1$}
\[
[\theta(d\theta)^{p}]
\]
{does not  vanish identically. For example, the equation}
\[
\theta\equiv P\,dx+Q\,dy+R\,dz=0
\]
{is in  general  of class $3$, unless we have}
\begin{align*}
  [\theta d\theta]&\equiv[(P\,dx+Q\,dy+R\,dz)(dP\,dx+dQ\,dy+dR\,dz)]\\
&=\left\{P\left(\frac{\pd R}{\pd y}-\frac{\pd Q}{\pd z}\right)+Q\left(\frac{\pd P}{\pd z}-\frac{\pd R}{\pd x}\right)+R\left(\frac{\pd Q}{\pd x}-\frac{\pd P}{\pd y}\right)\right\}[dx\,dy\,dz]=0.
\end{align*}

{The form $[\theta(d\theta)^{p}]$ is, up to a  factor,  the monomial}\[
[dX^{1}dX^{2}\dots dX^{p}dY_{1}dY_{2}\dots dY_{p}dZ].
\]


\vspace{12pt}\fsec \emph{First order partial differential equations.}\index{partial differential equations!first order} We can relate to the Pfaffian problem the problem of integrating a first order partial differential equation
\begin{equation}
  \label{eq:3.16}
  F\left(x^{1},x^{2},\dots,x^{n},z,\frac{\pd z}{\pd x^{1}},\frac{\pd z}{\pd x^{2}},\dots,\frac{\pd z}{\pd x^{n}}\right)=0.
\end{equation}

Integrating this equation, in the sense of problem of S.~Lie, is in effect  searching for the integral varieties of $n$ dimensions of the differential system
\begin{equation}
  \label{eq:3.17}
  \left\{
    \begin{aligned}
      F(x^{1},x^{2},\dots,x^{n},z,p_{1},p_{2},\dots,p_{n})&=0,\\
      dz-p_{1}dx^{1}-p_{2}dx^{2}-\dots-p_{n}dx^{n}&=0,
    \end{aligned}
  \right.
\end{equation}
where we regard $x^{1},x^{2},\dots,x^{n},z,p_{1},p_{2},\dots,p_{n}$ as $2n+1$ independent variables [coordinates of a \emph{contact element}\index{contact element} constructed by a point $(x^{i},z)$ of a $n+1$ dimensional space and a hypersurface passing through the point].

The system \eqref{eq:3.17} is closed by adjoining the equations
\begin{equation}
  \label{eq:3.18}
  \left\{
    \begin{aligned}
      \left(\frac{\pd F}{\pd x^{i}}+p_{i}\frac{\pd F}{\pd z}\right) dx^{i}+\frac{\pd F}{\pd p_{i}}dp_{i}&=0,\\
      [dx^{i}dp_{i}]&=0.
    \end{aligned}
  \right.
\end{equation}

The second equation of \eqref{eq:3.17}, where we suppose that the $2n+1$ variables $x_{i},z,p^{i}$ are related by the relation $F=0$, is a Pfaffian equation, and we can practically regard it as of $2n$ variables and the class is then at most to $2n-1$. We are going to verify that it is equal to $2n-1$ by forming the characteristic system of \eqref{eq:3.17}.

The characteristic system is formed by the equations \eqref{eq:3.17}, the first equation of \eqref{eq:3.18} and the equations
\[
u^{i}dp_{i}-v_{i}dx^{i}=0,
\]
where we suppose the coefficients $u^{i}$, $v_{i}$ are related by the relation
\[
\left(\frac{\pd F}{\pd x^{i}}+p_{i}\frac{\pd F}{\pd z}\right)u^{i}+\frac{\pd F}{\pd p_{i}}v_{i}=0,
\]
which gives immediately
\begin{equation}
  \label{eq:3.19}
  \frac{dx^{1}}{\frac{\pd F}{\pd p_{1}}}=
  \frac{dx^{2}}{\frac{\pd F}{\pd p_{2}}}=
  \dots=
  \frac{dx^{n}}{\frac{\pd F}{\pd p_{n}}}=
  \frac{-dp_{1}}{\frac{\pd F}{\pd x^{1}}+p_{1}\frac{\pd F}{\pd z}}=
  \dots=
  \frac{-dp_{n}}{\frac{\pd F}{\pd x^{n}}+p_{n}\frac{\pd F}{\pd z}}=
  \frac{dz}{p_{i}\frac{\pd F}{\pd p_{i}}}.
\end{equation}

Note moreover that the second equation of \eqref{eq:3.17} and the first equation of \eqref{eq:3.18} are consequences of the equations \eqref{eq:3.19}.

\vspace{12pt}\fsec In $2n+1$ dimensional space, or rather in the $2n$ dimensional manifold defined by the equation $F=0$, the \emph{characteristics}\index{characteristic}\index{characteristic bands|see{Cauchy characteristic}}\index{Cauchy characteristic} are the lines (characteristic bands) obtained by integrating the equations \eqref{eq:3.19}. To integrate  these equations is to put the Pfaffian equation
\[
dz-p_{i}dx^{i}=0,
\]
where the variables $x^{i},p_{i},z$ are related by the relation $F=0$,  into its canonical form
\begin{equation}
  \label{eq:3.20}
  dZ-Y_{1}dX^{1}-\dots-Y_{n-1}dX^{n-1}=0.
\end{equation}

After this integration, we only have to find the $n-1$ dimensional solutions of equation \eqref{eq:3.20} in the $2n-1$ dimensional space of $X^{i},Y_{i}$ and $Z$.

We obtain it in the following way.

First suppose that there are no relations between $X^{1},X^{2},\dots,X^{n-1}$. We then have
\begin{equation}
  \label{eq:3.21}
  Z=f(X^{1},X^{2},\dots,X^{n-1}),\quad P_{1}=\frac{\pd f}{\pd X^{1}},\quad P_{2}=\frac{\pd F}{\pd X^{2}},\ \dots\ P_{n-1}=\frac{\pd f}{\pd X^{n-1}}.
\end{equation}

Now suppose that there are $p<n-1$ relations between $X^{1},X^{2},\dots,X^{n-1}$ which we suppose can be solved for $X^{n-p},X^{n-p+1},\dots,X^{n-1}$, giving
\begin{equation}
  \label{eq:3.22}
  X^{n-i}=f^{i}(X^{1},X^{2},\dots,X^{n-p-1})\qquad (i=1,2,\dots,p);
\end{equation}
$Z$ being also a function of the $n-p-1$ quantities $X^{1},X^{2},\dots,X^{n-p-1}$:
\begin{equation}
  \label{eq:3.23}
  Z=f(X^{1},X^{2},\dots,X^{n-p-1}),
\end{equation}
and it then gives
\begin{equation}
  \label{eq:3.24}
  \frac{\pd f}{\pd X^{i}}-\left(Y_{i}+Y_{n-k}\frac{\pd f^{k}}{\pd X^{i}}\right)=0\qquad(i=1,2,\dots,n-p-1).
\end{equation}
The $n$ equations \eqref{eq:3.22}, \eqref{eq:3.23}, \eqref{eq:3.24} define a new class of solutions.

Finally, if the $n-1$ functions $X^{i}$ are constants, then the same is true for $Z$ and we have the solution
\begin{equation}
  \label{eq:3.25}
  X^{1}=a^{1},\qquad X^{2}=a^{2},\qquad\dots\qquad X^{n-1}=a^{n-1},\qquad Z=b.
\end{equation}

We observe that the equation \eqref{eq:3.20} does not admit integral varieties of more than $n$ dimensions.

We can regard the solution \eqref{eq:3.25} as defining a \emph{complete integral}\index{complete integral} of the given partial differential equation. We effectively obtain this integral by eliminating $p_{1},p_{2},\dots,p_{n}$ from the $n$ equations \eqref{eq:3.25} and the equation $F=0$. Once we know this complete integral, we can, by differentiation, deduce the general integral \footnote{See, for the general theory of first order partial differential equations, \textsc{E.~Goursat}, \emph{Le\c{c}ons sur l'int\'egration des \'equations aux d\'eriv\'ees partielles du premier ordre}, second edition (Paris, J.~Hermann, 1921).}.

\chapter{Integral elements, characters, genre. Existence theorems}
\label{cha:integr-elem-char}

\section{Integral elements of a differential system}
\label{sec:integr-elem-diff}

\fsec In this chapter we will discuss certain existence theorems concerning  integral varieties of closed exterior differential systems (we have seen that we can always reduce other systems to closed systems). In certain cases these existence theorems will solve the so-called \emph{Cauchy problem}\index{Cauchy problem}, which will in turn clarify the statements of the theorems. From this chapter on we are obliged to assume, as we have already remarked, that the functions that enter the equations of the given systems are \emph{analytic}, whereas in the preceding chapters,  the existence of continuous partial derivatives up to low order ($1$ or $2$) is sufficient.

The theory that we are going to develop was first created by \textsc{E.~Cartan} in order to study linear differential equations which correspond to closed differential systens containing second order equations at most. The theory then has been substantially extended by \textsc{E.~K\"ahler} to systems of any degree.




\vspace{12pt}\fsec The exterior differential systems that we will consider are of the form
\begin{equation}
  \label{eq:4.1}
  \left\{
    \begin{aligned}
      f_{\alpha}(x^{1},x^{2},\dots,x^{n})&=0,&(\alpha&=1,2,\dots,r_{0}),\\
      \theta_{\alpha}\equiv A_{\alpha i}dx^{i}&=0,&(\alpha&=1,2,\dots,r_{1}),\\
      \varphi_{\alpha}\equiv\frac{1}{2}A_{\alpha ij}[dx^{i}dx^{j}]&=0,&(\alpha&=1,2,\dots,r_{2}),\\
      \psi_{\alpha}\equiv\frac{1}{6}A_{\alpha ijk}[dx^{i}dx^{j}dx^{k}]&=0,&(\alpha&=1,2,\dots,r_{3}),\\
      &\hdots
    \end{aligned}
  \right.
\end{equation}

As the system is closed, the linear equations $df_{\alpha}=0$ must appear among the equations \eqref{eq:4.1}, or rather the form $df_{\alpha}$ must belong the ring of the forms $\theta_{\alpha}$. Similarly, the exterior differential $d\theta_{\alpha}$ must belong to the ring of the forms $\theta_{\alpha}$ and $\varphi_{\alpha}$ and so on.


\vspace{12pt}\fsec A key observation can be made about the equations $f_{\alpha}=0$ that appear in the system \eqref{eq:4.1}. They define an analytic variety $\mathcal{V}$ of a certain dimension $\rho$ in the $n$ dimensional space. We suppose that, on a general point of $\mathcal{V}$, the rank of the matrix of the partial derivatives $\pd f_{\alpha}/\pd x^{i}$ is equal to $\rho$ (we remark in passing here that $\rho$ might be less than the number $r_{0}$ of equations $f_{\alpha}=0$. For example, certain algebraic varieties of $n-\rho$ dimensions must be defined by more than $\rho$ algebraic equations if we do not want to miss any points of the variety). This condition that we want to impose on the equations $f_{\alpha}=0$ may not be achieved, if for example we have the single equation
\[
f\equiv[(x^{1})^{2}+(x^{2})^{2}+\dots+(x^{n})^{2}-1]^{2}=0.
\]

In such cases the arguments we are going to pursue will not be valid.

If the preceding condition holds and if we consider a general point $(x^{i})_{0}$ in the variety $\mathcal{V}$, the equations $df_{\alpha}=0$ appear among the equations $\theta_{\alpha}=0$ as $\rho$ linearly independent equations, for example, corresponding to the lines defined by the matrix of $\pd f_{\alpha}/\pd x^{i}$ of the determinant for $x^{i}=(x^{i})_{0}$. Any variety satisfying these $\rho$ equations $df_{x}=0$ and containing the point $(x^{i})_{0}$ of $\mathcal{V}$ is completely contained in $\mathcal{V}$, at least in a neighbourhood of the point.


\vspace{12pt}\fsec \emph{Integral plane elements} A $p$ dimensional plane element\index{plane element} is the set formed by a point $(x^{1},x^{2},\dots,x^{n})$ and a $p$-plane passing through the point. The point is said to be the origin of the element. A $p$ dimensional plane element of a given origin may be defined by a system of $n-p$ independent linear relations of $dx^{1},dx^{2},\dots,dx^{n}$, regarded as the coordinates of a Cartesian reference frame which has the point $(x^{i})$ as its origin. We can also define it by $p$ linearly independent vectors based on the point $(x^{i})$, or by its Pl\"ucker coordinates\index{Plucker coordinates@Pl\"ucker coordinates} $u^{i_{1}i_{2}\dots i_{p}}$ (\textsection\textbf{12}).

A $p$ dimensional plane element is said to be \emph{integral}\index{integral element} if it satisfies the following two conditions:

1. Its origin belongs to the variety $\mathcal{V}$ (we can say the the point is an \emph{integral point}\index{integral point});

2. The exterior forms defining the equations of the system are annihilated by the plane element considered.

It is clear that if a variety is integral, each of its points is an integral point and their tangent plane elements are integral as well. The converse is obvious.

\emph{It is natural to investigate the integral elements as a preliminary study of integral varieties.}

\vspace{12pt}\fsec \emph{Determining if a $p$ dimensional plane element is integral.} This is the problem that we have already solved in chapter 1 (\textsection\textbf{12}--\textbf{14}). We now briefly review the results obtained.

A linear element of origin $(x^{i})$ and of parameters $u^{i}$ is integral if its origin is an integral point and the linear element $(u^{i})$ annihilates the forms $\theta_{\alpha}$ of degree $1$ defining the system \eqref{eq:4.1}:
\[
A_{\alpha i}u^{i}=0.
\]

A two dimensional plane element is integral if its origin is an integral point and its Pl\"ucker coordinates $u^{ij}$ annihilate the exterior quadratic forms $[\theta_{\alpha}dx^{1}],\dots,[\theta_{\alpha}dx^{n}]$ and $\varphi^{\alpha}$, or alternatively if the Pl\"ucker coordinates $u^{ij}$ annihilate all quadratic exterior forms in the ring of the system \eqref{eq:4.1} \footnote{Recall that the coordinates $u^{ij}$ annihilate an exterior quadratic form $[H_{ij}dx^{i}dx^{j}]$ if we have $H_{ij}u^{ij}=0$. The ring of system \eqref{eq:4.1} is the ring determined by the forms $\theta_{\alpha},\varphi_{\alpha},\psi_{\alpha},\dots$ defining the equations in \eqref{eq:4.1}.}.

More generally \emph{a $p$ dimensional plane element is integral if its origin is an integral point and its Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ annihilate all degree $p$ forms belonging to the ring of the given system.}


\vspace{12pt}\fsec \emph{Regular integral point, linear ordinary integral element}. Let $(x^{i})$ be a generic point of $\mathcal{V}$. The linear integral elements having the point as the origin are defined by the condition that their parameters $u^{i}$ satisfy the equations
\begin{equation}
  \label{eq:4.2}
  A_{\alpha i}u^{i}=0\qquad(\alpha=1,2,\dots,r_{1}).
\end{equation}

Let $s_{0}$ be the number of these equations that are linearly independent, {i.e.}, the rank of the matrix of $A_{\alpha i}$, when the point $(x^{i})$ is \emph{generic}. \emph{The point $(x^{i})$ is said to be regular\index{regular integral element!point} if  the number of independent equations of \eqref{eq:4.2} at this point is not less than $s_{0}$. A linear integral element is said to be ordinary\index{ordinary integral element!linear} if its origin is a regular point of $\mathcal{V}$.}

The integer $s_{0}$ is the zeroth order \emph{character}\index{character!zeroth order}  of the system \footnote{If we solve the equations $f_\alpha=0$ with respect to a certain number of variables $x^{i}$ so as to make the equations \eqref{eq:4.1} contain only $n-\rho$ variables and their differentials, the character $s_{0}$ is naturally also reduced as a consequence.}.

Every regular point is necessarily a generic point of the variety $\mathcal{V}$, but the converse may not be true. As the conditions for a point not to be regular result in additional equations for the coordinates of the point, every sufficiently small neighbourhood of a regular point in the variety $\mathcal{V}$ contains only regular points. We can also say that if a point of $\mathcal{V}$ is not regular, in any neighbourhood of this point in $\mathcal{V}$ there are infinitely many regular points, since all non-regular points are limits of infinite sequences of regular points.

Observe also that the number of equations defining a linear integral element with a given origin can never exceed $s_{0}$. Finally, for any point of the space, \emph{integral or not}, sufficiently close to a regular point, the rank of equations \eqref{eq:4.2} is at least equal to $s_{0}$ and may be greater.

\vspace{12pt}\fsec \emph{Regular linear integral element, ordinary two-dimensional integral element.} Let $(E_{1})$ be an ordinary integral element of parameters $u^{i}$. To know more about the two dimensional integral elements that contain $(E_{1})$,  we  form the \emph{polar element}\index{polar element!of linear element} of $(E_{1})$: this is the subspace formed by linear elements $(dx^{i}
)$ such that the plane element determined by $(u^{i})$ and $(dx^{i})$ are integral. The conditions which the parameters $dx^{i}$ have to satisfy are
\begin{align}
  \label{eq:4.3}
  A_{\alpha i}dx^{i}&=0&(\alpha&=1,2,\dots,r_{1}),\\
  \label{eq:4.4}
  A_{\alpha ij}u^{i}dx^{j}&=0&(\alpha&=1,2,\dots,r_{2}),
\end{align}
these equations constitute what we call the \emph{polar system} of $(E_{1})$.

Let $s_{0}+s_{1}$ be the rank of the polar system of a \emph{generic} ordinary integral element $(E_{1})$. This value signifies that the $r_{1}$ equations \eqref{eq:4.3}  and $r_{2}$ equations \eqref{eq:4.4} can be reduced to $s_{0}+s_{1}$ independent relations if we take into account that the $u^{i}$ satisfy the equations
\[
A_{\alpha i}u^{i}=0.
\]

\emph{The ordinary integral element $(E_{1})$ is regular\index{regular integral element!linear} if the rank of its polar system does not decrease from its normal value $s_{0}+s_{1}$. A two dimensional integral element  is  ordinary\index{ordinary integral element!two dimensional} if it contains at least one regular linear integral element.}

The integer $s_{1}$ is  the first order \emph{character}\index{character!first order} of the given differential system.

Observe that if $s_{0}+s_{1}$ is greater or equal to $n-1$, the polar element of a regular integral element $(E_{1}) $ is reduced to the element itself. In this case there does not exist any ordinary integral element of two dimensions.

As in the previous section, we  remark that every sufficiently small neighbourhood of a regular linear integral element  in the variety contains only regular elements.

\vspace{12pt}\fsec \emph{Generalisations.} Suppose $s_{0}+s_{1}<n-1$. Let $(E_{2})$ be a two dimensional ordinary integral element, defined for example by two linear integral elements $(u^{i})$ and $(v^{i})$. The \emph{polar system}\index{polar system} of $(E_{2})$ is formed by the equations expressing that the linear integral element $(dx^{i})$ determines with $(E_{2})$ a three dimensional integral element. The equations of the polar system are
\begin{equation}
  \label{eq:4.5}
  \left\{
    \begin{aligned}
      A_{\alpha i}dx^{i}&=0&(\alpha&=1,2,\dots,r_{1}),\\
      A_{\alpha ij}u^{i}dx^{j}&=0,\quad A_{\alpha ij}v^{i}dx^{j}=0&(\alpha&=1,2,\dots,r_{2}),\\
      A_{\alpha ijk}u^{i}u^{j}dx^{k}&=0&(\alpha&=1,2,\dots,r_{3}),\\
    \end{aligned}
  \right.
\end{equation}

Let $s_{0}+s_{1}+s_{2}$ be the rank of this system for a \emph{generic} ordinary integral element.

The ordinary integral element $(E_{2})$ is  \emph{regular}\index{regular integral element} if the rank of its polar system does not fall below $s_{0}+s_{1}+s_{2}$, and a three dimensional integral element is  \emph{ordinary}\index{ordinary integral element} if it contain at least one two dimensional regular integral element.

The integer $s_{2}$ is the \emph{character}\index{character} of order $2$ of the given differential system.

If $s_{0}+s_{1}+s_{2}$ is greater or equal to $n-2$, the polar element of $(E_{2})$ is two dimensional and there does not exist any ordinary three dimensional integral element.

These definitions can be generalised easily by proceeding step by step to higher dimensions.

Ultimately, a $p$ dimensional integral element $(E_{p})$ is ordinary if it contains at least a regular integral element $(E_{p-1})$, which in turn contains at least a regular integral element $(E_{p-2})$ and so on, until we reach the regular linear element $(E_{1})$ whose origin is a regular point.

There  exists a $p$ dimensional ordinary integral element only if we have
\[
s_{0}+s_{1}+\dots+s_{p-1}<n-p+1.
\]

\vspace{12pt}\fsec \emph{Genre of a closed differential system}.\index{genre of differential system} As we increase the dimension, we will eventually come to a certain dimension, say $h+1$, such that there does not exist any ordinary integral element. The integer $h$ is  the genre of the differential system. It is the smallest integer such that
\[
s_{0}+s_{1}+\dots+s_{h}=n-h.
\]

There exists $h$ dimensional regular integral elements, but there does not exist any $h+1$ dimensional ordinary integral element.

The integers $s_{0},s_{1},s_{2},\dots,s_{h}$ are the characters\index{character} of the differential system.

Recall once more that in the variety of integral elements of $p\le h$ dimensions, any sufficiently small neighbourhood of a regular integral element contains only regular integral elements. An integral element $(E_{p})$ may be defined analytically by the coordinates $x^{1},x^{2},\dots,x^{n}$ of its origin and its Pl\"ucker coordinates $u^{i_{1}i_{2}\dots i_{p}}$ subject to the condition that they satisfy a system of quadratic relations that we have derived in chapter 1. A neighbourhood of an element $(E_{p})$ may be defined by the condition that the coordinates $x^{i},u^{i_{1}i_{2}\dots i_{p}}$ of any element $(E_{p})$ in this neighbourhood does not deviate too much from the coordinates of the element $(E_{p})$.

\section{Two existence theorems}
\label{sec:two-theor-exist}

\fsec Given a differential system $\Sigma$ of genre $h$, we will prove the existence of integral variety of dimensions $p\le h$. This does not exclude the possibility of existence of integral varieties of more than $h$ dimensions, and nor does it mean that the $p$ dimensional varieties whose existence  we will prove exhaust all integral varieties of $p$ dimensions. We will prove the theorem using the Cauchy-Kowalewski theorem, which we will state later.


\vspace{12pt}\fsec Consider now a closed differential system $\Sigma$ of genre $h$ and let $p\le h$. We have
\[
s_{0}+s_{1}+\dots+s_{p-1}\le n-p.
\]
Consider a $p$ dimensional integral element $(E_{p})_{0}$ which is \emph{ordinary}. We can suppose that its equations involve no linear relations in $dx^{1},dx^{2},\dots,dx^{p}$. Let us now change  notation and denote by $z^{\lambda}$ $(\lambda=1,2,\dots,n-p=\nu)$ the variables $x^{p+1},\dots,x^{n}$. The element $(E_{p})_{0}$ is defined by the equations of the form
\begin{equation}
  \label{eq:4.6}
  dz^{\lambda}=a_{i}{}^{\lambda}dx^{i}\qquad(\lambda=1,2,\dots,\nu),
\end{equation}
the index of summation $i$ varies from $1$ to $p$.

There exists a chain of regular integral elements $(E_{p-1})_{0},(E_{p-2})_{0},\dots,(E_{1})_{0}$ in which each element is contained in the previous element and in $(E_{p})_{0}$, and their origin is a regular point of $\mathcal{V}$. To simplify the exposition, we suppose that the first coordinates $x^{i}$ of the point is zero, and the other are $z^{\lambda}=a^{\lambda}$. Finally we can assume, by means of a linear transformation with constant coefficient on the $p$ coordinates $x^{i}$, that the equations of the linear elements $(E_{p-1})_{0},(E_{p-2})_{0},\dots$ are obtained by successively adjoining to the equations \eqref{eq:4.6} the equations
\[
dx^{p}=0,\qquad dx^{p-1}=0,\qquad\dots\qquad dx^{2}=0.
\]
The parameters of $(E_{1})_{0}$ are then
\[
1,\quad 0\quad\dots\quad 0,\quad a^{1}_{1},\quad a^{2}_{1},\quad\dots\quad a^{\nu}_{1}.
\]

Granted such conventions, for every $p$ dimensional integral variety $V_{p}$ tangent to the element $(E_{p})_{0}$, its tangent plane elements in a neighbourhood of the origin $(x^{i}=0,z^{\lambda}=a^{\lambda})$ of $(E_{p})_{0}$ will be ordinary integral elements. The integral variety can be defined by $\nu$ equations
\begin{equation}
  \label{eq:4.7}
  z^{\lambda}=\varphi^{\lambda}(x^{1},x^{2},\dots,x^{p})\qquad (\lambda=1,2,\dots,\nu),
\end{equation}
the $\varphi^{\lambda}$ are holomorphic functions of $x^{i}$ in a neighbourhood of the values $x^{i}=0$, and take on $x^{i}=0$ the values $z^{\lambda}=a^{\lambda}$.

We are going to prove the following theorem (generalised Cauchy theorem)\index{Cauchy's theorem, generalised|see{existence theorem (Cartan-K\"ahler)}}, the significance of which will be explained later.\index{existence theorem (Cartan-Kaehler)@existence theorem (Cartan-K\"ahler)}
\begin{thm*}
  There exists at least one analytic integral variety that is tangent to a $p$ dimensional ordinary integral element $(E_{p})_{0}$, such that $(E_{p})_{0}$ contains a $p-1$ dimensional integral variety $V_{p-1}$ tangent to the $(p-1)$ dimensional  regular integral element $(E_{p-1})_{0}$.
\end{thm*}


\vspace{12pt}\fsec \emph{The Cauchy-Kowalewski theorem.}\index{Cauchy-Kowalewski theorem} For the proof of the above theorem, it suffices to make use of a classical theorem, which we state in the following form \footnote{For the general formulation, see \textsc{E.~Goursat}, \emph{Cours d'Analyse math\'ematique}, 2nd edition, volume 2, pp.~652--637 (Paris, Gauthier-Villars, 1911).}.

\emph{Given a system of $q$ first order partial differential equations of $q$ unknown functions $z^{\lambda}$ in $p$ independent variables $x^{i}$, whose left hand sides are $\pd z^{\lambda}/\pd x^{p}$ and whose right hand sides are  functions of $x^{i},z^{\lambda},\pd z^{\lambda}/\pd x^{1},\dots,\pd z^{\lambda}/\pd x^{p-1}$, holomorphic in a neighbourhood of the values $x^{i}=0,z^{\lambda}=a^{\lambda},\pd z^{\lambda}/\pd x^{i}=a^{\lambda}_{i}$, the system admits one and only one analytic solution where the unknown functions are holomorphic functions of $x^{1},x^{2},\dots,x^{p}$ in a neighbourhood of $x^{i}=0$ that at $x^{p}=0$ reduce to some given holomorphic functions $z^{\lambda}=\chi^{\lambda}(x^{1},x^{2},\dots,x^{p-1})$, which in turn take on $x^{1}=x^{2}=\dots=x^{p-1}=0$ the values $a^{\lambda}$ and whose derivatives $\pd z^{\lambda}/\pd x^{i}$ take the values $a^{\lambda}_{i}$.}


\vspace{12pt}\fsec We now prove the following theorem,  a particular case of the theorem stated in \textsection\textbf{61}.\index{existence theorem (Cartan-Kaehler)@existence theorem (Cartan-K\"ahler)!first}

\newtheorem*{thmexs1}{\hspace{15pt}First existence theorem}
\begin{thmexs1}
  Consider a closed differential system which satisfies
\[
s_{0}+s_{1}+\dots+s_{p-1}=n-p.
\]
Let $(E_{p})_{0}$ be a $p$ dimensional ordinary integral element and $V_{p-1}$ a $p-1$ dimensional integral variety tangent to the regular integral plane element $(E_{p-1})_{0}$ contained in $(E_{p})_{0}$. Then there exists one and only one $p$ dimensional integral variety containing $V_{p-1}$ and this variety is tangent to the element $(E_{p})_{0}$.
\end{thmexs1}

We can immediately see that the last part of the statement is evident, since on the regular integral element $(E_{p-1})_{0}$, due to the value $n-p$ of the sum $s_{0}+s_{1}+\dots+s_{p-1}$, there passes only one $p$ dimensional integral element, which is therefore $(E_{p})_{0}$.

We will prove the theorem for the case $p=3$, which will suffice to give the idea of the proof in the general case.


\vspace{12pt}\fsec \emph{Preliminary remarks on the proof.} We suppose, as we did in \textsection\textbf{61}, that the element $(E_{3})_{0}$ has its origin at the coordinates $x^{i}=0,z^{\lambda}=a^{\lambda}$, $(\lambda=1,2,\dots,n-3=\nu)$ and it is defined by the equations
\begin{equation}
  \label{eq:4.6}
  dz^{\lambda}=a^{\lambda}_{a}dx^{1}+a^{\lambda}_{2}dx^{2}+a^{\lambda}_{3}dx^{3}\qquad (\lambda=1,2,\dots,\nu).
\end{equation}

The regular integral element $(E_{2})_{0}$ is obtained by adjoining to the equations \eqref{eq:4.6} the equation $dx^{3}=0$, and the regular element $(E_{1})_{0}$ is obtained by adjoining further the equation $dx^{2}=0$.

Let
\[
x^{3}=0,\qquad z^{\lambda}=\Phi^{\lambda}(x^{1},x^{2})\qquad(\lambda=1,2,\dots,\nu)
\]
be the equations of the two dimensional integral variety $V_{2}$. For $x^{1}=x^{2}=0$, the functions $\Phi^{\lambda}$ and their partial derivatives $\pd \Phi^{\lambda}/\pd x^{1}$, $\pd \Phi^{\lambda}/\pd x^{2}$ take respectively the values $a^{\lambda},a_{1}{}^{\lambda},a_{2}{}^{\lambda}$.

Now let
\[
z^{\lambda}=F^{\lambda}(x^{1},x^{2},x^{3})
\]
be the equations of the unknown three dimensional variety $V_{3}$. For $x^{3}=0$, the functions $F^{\lambda}$ reduce to the given functions $\Phi^{\lambda}$, and for $x^{1}=x^{2}=x^{3}=0$, we must have $\pd F^{\lambda}/\pd x^{3}=a_{3}{}^{\lambda}$.

The equations that the functions $F^{\lambda}$ must satisfy are, according to the equations \eqref{eq:4.1} of the system $\Sigma$,
\begin{equation}
  \label{eq:4.8}
  \left\{
    \begin{aligned}
      f_{\alpha}(x,z)&=0\qquad(\alpha=1,2,\dots,r_{0}),\\
      H_{\alpha i}&\equiv A_{\alpha i}+A_{\alpha \lambda}\frac{\pd z^{\lambda}}{\pd x^{i}}=0\qquad(i=1,2,3;\alpha=1,2,\dots,r_{1}),\\
      H_{\alpha ij}&\equiv A_{\alpha ij}+A_{\alpha i\lambda}\frac{\pd z^{\lambda}}{\pd x^{j}}-A_{\alpha j\lambda}\frac{\pd z^{\lambda}}{\pd x^{i}}+A_{\alpha \lambda \mu}\frac{\pd z^{\lambda}}{\pd x^{i}}\frac{\pd z^{\mu}}{\pd x^{j}}=0\\
      &\qquad\qquad\qquad\qquad\qquad\qquad\qquad(i,j=1,2,3;\alpha=1,2,\dots,r_{2}),\\
H_{\alpha 123}&\equiv A_{\alpha 123}+A_{\alpha ij\lambda}\frac{\pd z^{\lambda}}{\pd x^{k}}+A_{\alpha i \lambda\mu}\frac{\pd z^{\lambda}}{\pd x^{j}}\frac{\pd z^{\mu}}{\pd x^{k}}+A_{\alpha\lambda\mu\nu}\frac{\pd z^{\lambda}}{\pd x^{1}}\frac{\pd z^{\mu}}{\pd x^{2}}\frac{\pd z^{\nu}}{\pd x^{3}}=0.
    \end{aligned}
  \right.
\end{equation}

In the expression of $H_{\alpha 123}$, the indices $i,j,k$  appearing in the second and third terms are respectively the three even permutations $123,231,312$ of the indices $1,2,3$.

In a neighbourhood of the origin $M_{0}$ of $(E_{3})_{0}$ we  only need to care about $\rho$ of the equations $f_{\alpha}(x,z)=0$ subject to the single condition that the matrix of the coefficients of $dz^{\lambda}$ in the $\rho$ differentials $df_{\alpha}$ is of rank $\rho$. We suppose that this holds for the first $\rho$ columns \footnote{The integer $\rho$ is the dimension of the analytic manifold defined by the algebraic equations in the system. See \textsection\textbf{53}. --- \textsc{Translator}}.

The equations \eqref{eq:4.8} can be divided into three groups
\begin{align}
\label{eq:4.a}&f_{\alpha}(x,z)=0&(\alpha=1,2,\dots,\rho);\tag{A}\\
\label{eq:4.b}&H_{\alpha1}=0,\quad H_{\alpha 2}=0,\quad H_{\alpha 12}=0;\tag{B}\\
\label{eq:4.c}&H_{\alpha 3}=0,\quad H_{\alpha 13}=0,\quad H_{\alpha23}=0,\quad H_{\alpha 123}=0.\tag{C}
\end{align}

The variety $V_{2}$ satisfies the equations \eqref{eq:4.a} and \eqref{eq:4.b}. The variety $V_{3}$ in addition satisfies the equations \eqref{eq:4.c}.

\vspace{12pt}\fsec If among the coefficients of the equations \eqref{eq:4.c} we give the arguments $x$, $z^{\lambda}$, $\pd z^{\lambda}/\pd x^{1}$, $\pd z^{\lambda}/\pd x^{2}$ respectively the values $0,a^{\lambda},a^{\lambda}_{1},a^{\lambda}_{2}$, we obtain a system of linear equations in $\pd z^{\lambda}/\pd x^{3}$, and it is easy to see that the system \eqref{eq:4.c} becomes the polar system of $(E_{2})_{0}$ after replacing $dx^{1}$ and $dx^{2}$ by $0$, replacing $dx^{3}$ by $1$, and replacing $dz^{\lambda}$ by $\pd z^{\lambda}/\pd x^{3}$. The equations of this polar system contain by hypothesis $s_{0}+s_{1}+s_{2}$ independent relations in $dx^{i}$ and $dz^{\lambda}$, but these equations cannot contain any linear equations between $dx^{1}, dx^{2}, dx^{3}$, otherwise these relations will appear among those that define the element $(E_{3})_{0}$, which we assumed is not the case. The equations \eqref{eq:4.c}, regarded as linear equations in $\pd z^{\lambda}/\pd x^{3}$, therefore contain $s_{0}+s_{1}+s_{2}$ independent relations when, among the coefficients of the equations, we replace respectively $x^{i},z^{\lambda}, \pd z^{\lambda}/\pd x^{i}, \pd z^{\lambda}/\pd x^{2}$ by $0,a^{\lambda},a_{1}^{\lambda},a_{2}^{\lambda}$. Among these equations, we will make a choice of $s_{0}+s_{1}+s_{2}$ independent equations, which will be called \emph{principal}.

If in the principal equations, we give the arguments $x^{i}, z^{\lambda}, \pd z^{\lambda}/\pd x^{1}, \pd z^{\lambda}/\pd x^{2}$  values sufficiently close to $0, a^{\lambda}, a_{1}^{\lambda}, a_{2}^{\lambda}$, they will continue to be linearly independent. There are three possible cases:
\vspace{12pt}

1. \emph{The values given to the arguments define a two dimensional integral element.} The element is necessarily regular if the values of the arguments do not deviate too much from the values $0, a^{\lambda}, a_{1}^{\lambda}, a_{2}^{\lambda}$. In this case the non-principal equations of \eqref{eq:4.c} are consequences of the principal equations of \eqref{eq:4.c}.
\vspace{12pt}

2. \emph{The values given to the arguments $x^{i}, z^{\lambda}$ define an integral point} but the values given to the other arguments do not define a two dimensional integral element. The non-principal equations of \eqref{eq:4.c} are  consequences of the principal equations of \eqref{eq:4.c} and the equations \eqref{eq:4.b}. More precisely, the non-principal equations of $H_{\alpha 3}=0$, depending on neither $\pd z^\lambda/\pd x^{1}$ nor $\pd z^{\lambda}/\pd x^{2}$, are consequences of the principal equations of $H_{\alpha 3}=0$. The non-principal equations of $H_{\alpha 13}$, depending only on $\pd z^\lambda/\pd x^{1}$ but not on $\pd z^{\lambda}/\pd x^{2}$, are consequences of the principal equations of $H_{\alpha 13}=0$ and $H_{\alpha 3}=0$, together with the equations $H_{\alpha 1}=0$  expressing that $\pd z^{\lambda}/\pd x^{1}$ define a linear integral element. Finally the non-principal equations of $H_{\alpha 23}=0, H_{\alpha 123}=0$ are consequences of the principal equations of \eqref{eq:4.c} and the equations \eqref{eq:4.b}. These results may be expressed in the following formulae, where $\alpha'$ is the non-principal index of equation \eqref{eq:4.c}, and $\alpha$ is the principal index of equation \eqref{eq:4.c} or any index of the equation \eqref{eq:4.b}:
\begin{equation}
  \label{eq:4.9}
  \left\{
    \begin{aligned}
      H_{\alpha'3}&=\{H_{\alpha 3}\},\\
      H_{\alpha'13}&=\{H_{\alpha 3}, H_{\alpha 13}, H_{\alpha 1}\},\\
      H_{\alpha'23}&=\{H_{\alpha 3}, H_{\alpha 13}, H_{\alpha 23}, H_{\alpha 123}, H_{\alpha 1}, H_{\alpha 2}, H_{\alpha 12}\},\\
      H_{\alpha'123}&=\{H_{\alpha 3}, H_{\alpha 13}, H_{\alpha 23}, H_{\alpha 123}, H_{\alpha 1}, H_{\alpha 2}, H_{\alpha 12}\},
    \end{aligned}
  \right.
\end{equation}
the curly braces representing linear combinations of the expressions it encloses, the coefficients of which are holomorphic functions of $x^{i}, z^{\lambda}, \pd z^{\lambda}/\pd x^{1}, \pd z^{\lambda}/\pd x^{2}$ in a neighbourhood of the values $0, a^{\lambda}, a^{\lambda}_{1}, a^{\lambda}_{2}$ of the argument.
\vspace{12pt}

3. \emph{The values given to the arguments $x^{i}, z^{\lambda}$ does not define an integral point}. The non-principal equations \eqref{eq:4.c} are then consequences of principal equations of \eqref{eq:4.c}, the equations \eqref{eq:4.b} and the equations \eqref{eq:4.a}.
\vspace{12pt}

We will have two further remarks. The first is that we can assume that the equations $df_{\alpha}=0$ appear among the equations $\theta_{\alpha}=0$, and the equations $d\theta_{\alpha}=0$ appear among the equations $\varphi_{\alpha}=0$ and the equations $d\varphi_{\alpha}=0$ appear among the equations $\psi_{\alpha}=0$. The second is that we can assume that, among the principal equations \eqref{eq:4.c} of the form $H_{\alpha 3}=0$ we have the first $\rho$  equations coming from $df_{\alpha}=0$, that is to say,
\begin{equation}
  \label{eq:4.10}
  \frac{\pd f_{\alpha}}{\pd x^{3}}+\frac{\pd f_{\alpha}}{\pd z^{\lambda}}\frac{\pd z^{\lambda}}{\pd x^{3}}=0\qquad (\alpha=1,2,\dots,\rho).
\end{equation}


\vspace{12pt}\fsec \emph{Proof of the first existence theorem}. Consider the $s_{0}+s_{1}+s_{2}$ principal equations of \eqref{eq:4.c}. As the number of unknown functions $z^{\lambda}$ is $s_{0}+s_{1}+s_{2}=n-3$, these give the derivatives $\pd z^{\lambda}/\pd x^{3}$ in terms of the other derivatives $\pd z^{\lambda}/\pd x^{1}$, $\pd z^{\lambda}/\pd x^{2}$. They constitute a Cauchy-Kowalewski system. Hence they admit one and only one holomorphic solution
\begin{equation}
  \label{eq:4.11'}
  z^{\lambda}=F^{\lambda}(x^{1},x^{2},x^{3})
\end{equation}
where $F^{\lambda}$ reduces to the given functions $\Phi^{\lambda}(x^{1},x^{2})$ when we set $x^{3}=0$. We are now going to prove that the variety $V_{3}$ defined by the equations \eqref{eq:4.11'} is integrable.

\emph{Firstly}, among the equations of the Cauchy-Kowalewski system there are $\rho$ equations \eqref{eq:4.10}, which express the fact that on the variety $V_{3}$ the functions $f_{\alpha}(x,z)$ are independent of $x^{3}$, or, for $x^{3}=0$, they vanish since the variety $V_{2}$ is integrable. Hence these equations vanish identically. \emph{The variety $V_{3}$ therefore satisfies the equations \eqref{eq:4.a}}.

\emph{Secondly}, since the variety $V_{3}$ satisfies the equations \eqref{eq:4.a} whose points are all integral points, all the expressions $H_{\alpha 3}$, even the non-principal ones, are identically zero on the variety $V_{3}$ according to \textsection\textbf{65}.2. As for the expressions $H_{\alpha 13}, H_{\alpha 23}, H_{\alpha 123}$, the principal ones are zero by the solution, and the non-principal ones, according to \eqref{eq:4.9}, satisfy equations of the form
\begin{equation}
  \label{eq:4.11}
  \left\{
    \begin{aligned}
      H_{\alpha'13}&=\{H_{\alpha 1}\},\\
      H_{\alpha'23}&=\{H_{\alpha 1},H_{\alpha 2},H_{\alpha 12}\},\\
      H_{\alpha'123}&=\{H_{\alpha 1},H_{\alpha 2},H_{\alpha 12}\}.
    \end{aligned}
  \right.
\end{equation}
\emph{It therefore suffices that the variety $V_{3}$ satisfies the equations \eqref{eq:4.b} for it to be integrable}.

\emph{Thirdly}, the quantities $H_{\alpha 1}, H_{\alpha 2}, H_{\alpha 12}$ vanish for $x^{3}=0$. Take one of the forms $\theta^{\alpha}$, on the variety $V_{3}$ we have
\[
\theta_{\alpha}=H_{\alpha 1}dx^{1}+H_{\alpha 2}dx^{2},
\]
from which
\begin{equation}
  \label{eq:4.12}
  d\theta_{\alpha}=-\frac{\pd H_{\alpha 1}}{\pd x^{3}}[dx^{1}dx^{3}]-\frac{\pd H_{\alpha 2}}{\pd x^{3}}[dx^{2}dx^{3}]+\left[\frac{\pd H_{\alpha 2}}{\pd x^{1}}-\frac{\pd H_{\alpha 1}}{\pd x^{2}}\right][dx^{1}dx^{2}].
\end{equation}

As the equation $d\theta_{\alpha}=0$ is part of the equations $\varphi_{\alpha}=0$, the coefficient $\pd H_{\alpha 1}/\pd x^{3}$ is a linear combination with holomorphic coefficients of expressions $H_{\alpha 13}$ and hence of the expressions $H_{\alpha 1}$ according to \eqref{eq:4.11}. The $r_{1}$ quantities $H_{\alpha 1}$ therefore satisfy a system of linear differential equations with holomorphic coefficients, with independent variable is $x^{3}$. As the functions $H_{\alpha 1}$ vanish for $x^{3}=0$, they vanish identically. \emph{The variety $V_{3}$ therefore satisfies the equations $H_{\alpha 1}=0$ and hence the equations $H_{\alpha 13}=0$.}

\emph{Fourthly}, the expression $\pd H_{\alpha 2}/\pd x^{3}$ is, according to \eqref{eq:4.12}, a linear combination with holomorphic coefficients of $H_{\alpha 23}$, that is to say of $H_{\alpha 2}$ and $H_{\alpha 12}$ according to \eqref{eq:4.11}.

On the other hand, as $\varphi_{\alpha}=H_{\alpha 12}[dx^{1}dx^{2}]+H_{\alpha 23}[dx^{2}dx^{3}]$, we have
\[
d\varphi_{\alpha}=\left(\frac{\pd H_{\alpha 12}}{\pd x^{3}}+\frac{\pd H_{\alpha 23}}{\pd x^{1}}\right)[dx^{1}dx^{2}dx^{3}],
\]
and as $d\varphi_{\alpha}$ is a linear combination of $\psi_{\alpha}$, the expression $\pd H_{\alpha 12}/\pd x^{3}+\pd H_{\alpha 23}/\pd x^{1}$ is a linear combination of $H_{\alpha 123}$, that is to say, according to \eqref{eq:4.11}, of $H_{\alpha 2}$ and $H_{\alpha 12}$. Finally, as $\pd H_{\alpha 23}/\pd x^{1}$ is a linear combination of $H_{\alpha 2}$, $H_{\alpha 12}$, $\pd H_{\alpha 2}/\pd x^{1}$, $\pd H_{\alpha 12}/\pd x^{1}$ according to \eqref{eq:4.11}, we see that the $\pd H_{\alpha 12}/\pd x^{3}$ are linear combinations with holomorphic coefficients of the functions $H_{\alpha 2}$ and $H_{\alpha 12}$ and of their partial derivatives with respect to $x^{1}$. They therefore satisfy a Cauchy-Kowalewski system and, as they vanish for $x^{3}=0$, they are identically zero, and hence according to \eqref{eq:4.11}, $H_{\alpha 23}$ and $H_{\alpha 123}$ are zero as well.

The variety $V_{3}$ therefore satisfies all the equations \eqref{eq:4.a}, \eqref{eq:4.b} and \eqref{eq:4.c}.\qed


\vspace{12pt}\addtocounter{frenchsec}{1}\index{existence theorem (Cartan-Kaehler)@existence theorem (Cartan-K\"ahler)!second}
\theoremstyle{shape1}
\newtheorem*{thm67}{\hspace{15pt}\textbf{67.} Second existence theorem}
\begin{thm67}
  Given a closed differential system $\Sigma$ for which we have
\[
s_{0}+s_{1}+\dots+s_{p-1}<n-p,
\]
and let $(E_{p})_{0}$ be a $p$ dimensional integral element and $V_{p-1}$ a $p-1$ dimensional integral variety tangent to the regular integral element $(E_{p-1})$ contained in $(E_{p})_{0}$. Then there exists infinitely many integral varieties of $p$ dimensions containing $V_{p-1}$ and tangent to the element $(E_{p})_{0}$. Each of these integral variety is determined uniquely if we choose arbitrarily $n-p-s_{0}-s_{1}-\dots-s_{p-1}$ unknown functions which reduce to the functions $\Phi^{\lambda}(x^{1},x^{2})$ for $x^{3}=0$.
\end{thm67}

The proof is easy and can be reduced to that of the first theorem. Indeed, let us take the hypothesis $p=3$ and consider the principal equations of \eqref{eq:4.c}. They are solvable with respect to $s_{0}+s_{1}+s_{2}$ of the derivatives $\pd z^{\lambda}/\pd x^{3}$. There remains on the right hand side $n-3-s_{0}-s_{1}-s_{2}$ of the derivatives $\pd z^{\lambda}/\pd x^{3}$. Let us assign to $n-3-s_{0}-s_{1}-s_{2}$ of the functions $z^{\lambda}$  values corresponding to holomorphic functions of $x^{1},x^{2},x^{3}$ in a neighbourhood of $x^{i}=0$ subject to the condition that they reduce to the functions $\Phi^{\lambda}(x^{1},x^{2})=0$ of the same index for $x^{3}=0$. We then have a Cauchy-Kowalewski system with unique solution corresponding the the given initial data $z^{\lambda}=\Phi^{\lambda}(x^{1},x^{2})$.

\vspace{12pt}\fsec The integral varieties whose existence has been established by the two existence theorems are \emph{ordinary} integral varieties\index{ordinary integral variety}. The set of ordinary integral varieties constitute  the so-called \emph{general solution}\index{general solution of a differential system} of the given differential system. The integral varieties that are not ordinary do not have ordinary tangent integral elements at any point. The \emph{regular integral varieties}\index{regular integral variety} are those that admit regular tangent integral elements.

We can calculate the \emph{degree of indeterminacy of an ordinary integral variety $V_{p}$}\index{degree of indeterminacy} admitting a given $p$ dimensional ordinary tangent integral element.

Continuing to use the preceding notations, the section $V_{1}$ of $V_{p}$ by the flat variety $x^{p}=x^{p-1}=\dots=x^{2}=0$ depends on $n-p-s_{0}$ arbitrary functions of $x^{1}$ subject to the condition that at $x^{1}=0$ their derivatives take the given values $a_{1}^{\lambda}$. The integral variety $V_{1}$ being chosen, the section of $V_{p}$ by the plane variety $x^{p}=x^{p-1}=\dots=x^{3}=0$ depends on $n-p-s_{0}-s_{1}$ arbitrary functions of $x^{1}$, $x^{2}$ subject to the condition that it reduces at $x^{2}=0$ to the known functions of $x^{1}$. And so on. The variety $V_{p}$ depends on $n-p-s_{0}-s_{1}-\dots-s_{p-1}$ arbitrary functions of $x^{1},x^{2},\dots,x^{p}$ subject to the condition that it reduces at $x^{p}=0$ to the given functions of $x^{1},x^{2},\dots,x^{p-1}$.

Introduce, for symmetry reason, an integer $\sigma_{p}$ (not a character) by the relation
\[
s_{0}+s_{1}+\dots+s_{p-1}+\sigma_{p}=n-p.
\]

We can say, \emph{roughly}, the $p$ dimensional ordinary integral variety tangent to $(E_{p})_{0}$ depend on
\[\begin{array}{rcl}
  s_{1}+s_{2}+\dots+s_{p-1}+\sigma_{p}&\text{arbitrary functions of}& x^{1},\\
  s_{2}+\dots+s_{p-1}+\sigma_{p}&\prime\prime& x^{1},x^{2},\\
  \dots&\dots&\dots\\
  s_{p-1}+\sigma_{p}&\prime\prime& x^{1},x^{2},\dots,x^{p-1},\\
  \sigma_{p}&\prime\prime& x^{1},x^{2},\dots,x^{p-1},x^{p}.\\
\end{array}\]

\vspace{12pt}\addtocounter{frenchsec}{1}
\theoremstyle{shape0}
\newtheorem*{rmk69}{\hspace{15pt}\textbf{69.} Remark}
\begin{rmk69}
  It is inappropriate to give a too absolute interpretation  to the above statement, which specifies only the number of arbitrary functions we may specify to arrive at the most general $p$ dimensional integral variety by successive applications of the Cauchy-Kowalewski theorem. In fact, the only one of these integers that has an absolute meaning is the number of arbitrary functions of maximal numbers of variables ($\sigma_{p}$ if $\sigma_{p}\neq 0$, $s_{p-1}$ if $\sigma_{p}=0, s_{p-1}\neq 0$, {etc}.). Without wanting to justify this assertion, which  makes sense  only for \emph{analytic} differential systems and their \emph{analytic} integral varieties, let us content ourselves by studying a simple example to see why  we should proceed cautiously in such cases.

Consider the equation
\[
\frac{\pd^{2}z}{\pd y^{2}}=\frac{\pd z}{\pd x}.
\]
From our point of view, the general solution of this equation depend on two arbitrary functions of one variable, for example the holomorphic functions of $x$ which reduces to $z$ and $\pd z/\pd y$ for $x=0$. But we may also be tempted to say that the general solution depends on only one arbitrary function, namely the function $\Phi(y)$ which  $z$ reduces to at $x=0$. Unfortunately, for example, if we specify $\Phi(y)$ as a holomorphic function of $y$ in a neighbourhood of $y=0$, the equation may not admit \emph{any} holomorphic solutions in a neighbourhood of the point $x=0, y=0$: it suffices to take $\Phi(y)=1/(1-y)$, which requires $z$ to be the series
\[
z=\frac{1}{1-y}+\frac{2}{1}\frac{x}{(1-y)^{3}}+\frac{4!}{2!}\frac{x^{2}}{(1-y)^{5}}+\dots+\frac{(2n)!}{n!}\frac{x^{n}}{(1-y)^{2n+1}}+\cdots
\]
And this entire series in $x$ converges only for $x=0$. Such negative results occur when the function $\Phi(y)$ is not entire, as well as in certain cases where the function $\Phi(y)$ is entire.
\end{rmk69}



\vspace{12pt}\fsec \emph{Degree of indeterminacy of ordinary integral elements $E_{p}$ having a given regular integral point as the origin}.\index{degree of indeterminacy} If we make the same assumptions as above for the regular integral elements $(E_{p-1})_{0},\dots,(E_{1})_{0}$ contained in the ordinary element $(E_{p})_{0}$, the ordinary elements $(E_{p})$ near $(E_{p})_{0}$ of the same origin as $(E_{p})_{0}$ are obtained in one and only one way by means of $p$ linear integral elements where the $i$th of the first $p$ components has $dx^{k}$ all set to zero with the exception of $dx^{i}=1$, and the rest $\nu$ components are $dz^{\lambda}=t_{i}{}^{\lambda}$. The components $t_{1}^{\lambda}$ of the first linear element are required to satisfy  $s_{0}$ equations. The components $t_{2}^{\lambda}$ of the second linear element which, together with the first linear element form an integral element $(E_{2})$, are required to satisfy $s_{0}+s_{1}$ equations of the polar system of $(E_{1})$. The components $t_{3}^{\lambda}$ of the third linear element which, together with $(E_{2})$ form an integral element $(E_{3})$, are required to satisfy $s_{0}+s_{1}+s_{2}$ equations of the polar system of $(E_{2})$. And we go on in this way. All these equations are independent and we have in total the number of equations
\begin{align*}
s_{0}+(s_{0}+s_{1})+(s_{0}+s_{1}+s_{2})+\dots+(s_{0}+s_{1}+\dots+s_{p-1})\qquad\\
=ps_{0}+(p-1)s_{1}+\dots+s_{p-1}.
\end{align*}

By introducing the number $\sigma_{p}$, we see that the number of arbitrary parameters we are seeking is equal to
\begin{align*}
 &\quad\  p(n-p)-[ps_{0}+(p-1)s_{1}+\dots+2s_{p-2}+s_{p-1}]\\
&=p(s_{0}+s_{1}+\dots+s_{p-1}+\sigma_{p})-[ps_{0}+(p-1)s_{1}+\dots+s_{p-1}]\\
&=s_{1}+2s_{2}+3s_{3}+\dots+(p-1)s_{p-1}+p\sigma_{p}.
\end{align*}

\begin{thm*}
  In a closed system of genre equal or greater than $p$, the $p$ dimensional ordinary integral varieties having a given regular integral point as its origin depends on $s_{1}+2s_{2}+\dots+(p-1)s_{p-1}+p\sigma_{p}$ arbitrary parameters.
\end{thm*}


\vspace{12pt}\fsec \emph{Particular case.} If all the integers except those of ranks $q<p$ are zero and the integer $\sigma_{p}$ is zero, we have the following important theorem:

\begin{thm*}
  If $s_{q}=s_{q+1}=\dots=s_{p-1}=\sigma_{p}=0$, there is one and only one ordinary integral variety $V_{p}$ containing any $q-1$ dimensional regular integral variety.
\end{thm*}
The theorem applies for example to a completely integrable system of $n-p$ equations in terms of total differentials of $n$ variables. For then the closed system contains only the equations $\theta_{\alpha}=0$ and we have
\[
s_{0}=n-p,\qquad s_{1}=s_{2}=\dots=s_{p}=0.
\]
Then effectively there is one and only one integral variety $V_{p}$ for all regular points of the space.

In the general case, given an integral variety $V_{q-1}$, the method indicated above consists of in successively integrating $p-q$ Cauchy-Kowalewski systems. But we can achieve the same result by integrating only once: it suffices, supposing that $V_{q-1}$ is contained in the variety 
\[
x^{q}=x^{q+1}=\dots=x^{p}=0,
\]
to set
\[
x^{q}=a^{q}t,\qquad x^{q+1}=a^{q+1}t,\qquad\dots\qquad x^{p}=a^{p}t,
\]
and regard $a^{q},a^{q+1},\dots,a^{p}$ as arbitrary \emph{parameters} and then integrate the system, regarding  $z^{p}$  as unknown functions of $q$ independent variables $x^{1},x^{2},\dots,x^{q-1},t$. We then have a single Cauchy-Kowalewski system of $n-p$ unknown functions of $q$ independent variables. Once this system is integrated, we replace the unknown functions $t$ by $1$ and $a^{q},a^{q+1},\dots,a^{p}$ by $x^{q},x^{q+1},\dots,x^{p}$ in the expression obtained.

The preceding procedure is fundamentally the same as the procedure indicated for integrating completely integrable system.

\begin{rmk*}
  If the system \eqref{eq:4.1} does not contain exterior differentials of degree more than $2$, the numbers $s_{0},s_{1},s_{2},\dots$ cannot increase, and it suffices that the integer $s_{q}$ vanishes for all the following integers to vanish. As for $\sigma_{p}$, which is not a character, it is enough to suppose that we have
\[
s_{0}+s_{1}+\dots+\dots+s_{q-1}=n-p.
\]
\end{rmk*}


\section{General solution and singular solutions. Characteristics}
\label{sec:gener-solut-sing}


\fsec We say that a $p$ dimensional integral variety is part of the \emph{general solution}\index{general solution of a differential system} of a differential systen with $p$ independent variables if its generic $p$ dimensional tangent element is an ordinary integral element. The existence, at least locally, of the integral variety is established by applying the Cauchy-Kowalewski theorem.

An integral variety which is not part of the general solution is said to be \emph{singular}.\index{singular solution of differential system} A solution may be singular either because none of the points of its integral variety is regular, or none of the integral elements of one dimension, or of two dimensions, {etc.}, or of $p-1$ dimensions, is a regular integral element. Therefore there may be different classes of singular integral varieties, the degree of singularity decreases when we move from one class to the next.



\vspace{12pt}\fsec We have already introduced the notion of \emph{characteristic}\index{characteristic|see{characteristic variety}}\index{characteristic variety} of a differential system, the characteristic generating integral varieties contained in no integral variety of greater dimensions. These characteristics exist only for certain differential system. We will refer to them as the \emph{Cauchy characteristics}\index{Cauchy characteristic}.

There are other spaces of characteristics which exist in general for every integral variety $V_{p}$ of a given dimension $p$ and constitute part of the general solution of the system. These are the $q<p$ dimensional varieties contained in the integral variety $V_{p}$ with the property that their $p$ dimensional tangent elements are not regular.  \emph{Their importance is that the Cauchy-Kowalewski theorem fails if we try to determine the $q+1$ dimensional integral varieties which contain them.} Finding them is related to the preliminary problem of finding the $q$ dimensional integral elements which are not regular. The existence of such elements does not \emph{ipso facto} entails the existence of $q$ dimensional characteristic varieties, unless $q=1$, and due to compatibility conditions we necessarily have $q>1$.

We will clarify these concepts by some examples taken from classical problems.



\vspace{12pt}\fsec \textsc{Example I.} \emph{First order partial differential equations}.\index{partial differential equations!first order} Consider, using classical notations, the second order partial differential equations
\[
F(x,y,z,p,q)=0
\]
of an unknown function of two independent variables. Following the conceptions of S.~Lie, let us extend this problem to the searching of two dimensional solutions of the closed differential system
\begin{equation}
  \label{eq:4.13}
  \left\{
    \begin{aligned}
      F(x,y,z,p,q)&=0,\\
      F_{x}dx+F_{y}dy+F_{z}dz+F_{p}dp+F_{q}dq&=0,\\
      dz-p\,dx-q\,dy&=0,\\
      [dx\,dp]-[dy\,dq]&=0.
    \end{aligned}
\right.
\end{equation}

The character $s_{0}$ is equal to the rank of the system
\begin{equation}
  \label{eq:4.14}
  \left\{
    \begin{aligned}
      (F_{x}+pF_{z})dx+(F_{y}+qF_{z})dy+F_{p}dp+F_{q}dq&=0,\\
      dz-p\,dx-q\,dy&=0.
    \end{aligned}
  \right.
\end{equation}

The rank is equal to $2$. A point of the integral variety is singular ({i.e.}, non-regular) if the rank of the system \eqref{eq:4.14} is less than $2$, and this happens if we have
\begin{equation}
  \label{eq:4.15}
  F_{x}+pF_{z}=0,\qquad F_{y}+qF_{z}=0,\qquad F_{p}=0,\qquad F_{q}=0.
\end{equation}

The \emph{singular solutions} are therefore those that satisfy equations \eqref{eq:4.15}.

There are no others. Indeed suppose a generic point of the integral variety to be regular. The polar element of a linear integral element of components $\delta x,\delta y, \delta z, \delta p, \delta q$ is given by the equations \eqref{eq:4.14} together with the equation
\begin{equation}
  \label{eq:4.16}
  \delta p\,dx+\delta q\,dy-\delta x\,dp-\delta y\,dq=0.
\end{equation}

The character $s_{1}$ is therefore equal to $1$. A singular integral variety can exist in principle if for all tangent linear elements, the rank of the system \eqref{eq:4.14} and \eqref{eq:4.16} is less by one, and this happens if we have
\begin{equation}
  \label{eq:4.17}
  \frac{\delta x}{F_{p}}=\frac{\delta y}{F_{q}}=\frac{-\delta p}{F_{x}+pF_{z}}=\frac{-\delta q}{F_{y}+q F_{z}}=\frac{\delta z}{p F_{p}+qF_{q}},
\end{equation}
but the relations \eqref{eq:4.17} shows that for a given point of the integral variety, there exists only a single singular tangent linear element. \emph{Therefore there exists no other singular solutions than those that satisfy equations \eqref{eq:4.15}, if they exist.}

The characteristics of an integral variety $V_{2}$ are, according to the definition given in \textsection\textbf{73}, the lines whose elements satisfy the equation \eqref{eq:4.17}: these are the characteristics that we have already encountered (\textsection\textbf{50}). They depend on their arbitrary set of constants.

\vspace{12pt}\fsec \textsc{Example II.} \emph{Second order partial differential equations}.\index{partial differential equations!second order} Every second order partial differential equation of an unknown function $z$ of two independent variables $x,y$ can be represented by the closed differential system
\begin{equation}
  \label{eq:4.18}
  \left\{
    \begin{aligned}
      F(x,y,z,p,q,r,s,t)&=0,\\
      (F_{x}+pF_{z}+rF_{p}+sF_{q})dx+
      (F_{y}+qF_{z}+sF_{p}+tF_{q})dy\\
      +F_{r}dr+F_{s}ds+F_{t}dt&=0,\\
      dz-p\,dx-q\,dy&=0,\\
      dp-r\,dx-s\,dy&=0,\\
      dq-s\,dx-t\,dy&=0,\\
      [dx\,dr]+[dy\,ds]&=0,\\
      [dx\,ds]+[dy\,dt]&=0.
    \end{aligned}
  \right.
\end{equation}

The character $s_{0}$ is equal to $4$, the rank of the linear system in $dx,dy,\dots,dt$ formed by the four equations in \eqref{eq:4.18} that follows the first equation. The rank is reduced by one on the points where
\begin{equation}
  \label{eq:4.19}
  F_{x}+pF_{z}+rF_{p}+sF_{q}=0,\quad F_{y}+qF_{z}+sF_{p}+tF_{q}=0,\quad F_{r}=0,\quad F_{s}=0,\quad F_{t}=0.
\end{equation}

We hence have a first class of singular solutions, these that satisfy equations \eqref{eq:4.19}.

Consider now a regular point. The polar system of an ordinary linear integral element of components $(\delta x,\delta y, \delta z, \delta p, \delta q, \delta r, \delta s, \delta t)$ is given by the four equations \eqref{eq:4.18} that follow the first equations, together with the two equations
\begin{equation}
  \label{eq:4.20}
  \left\{
    \begin{aligned}
    \delta x\,dr+\delta y\,ds-\delta r\,dx-\delta s\,dy&=0,\\
    \delta x\,ds+\delta y\,dt-\delta s\,dx-\delta t\,dy&=0.      
    \end{aligned}
  \right.
\end{equation}

We have $s_{0}+s_{1}=6$, from which $s_{1}=2$, and the rank $6$ of the polar system is reduced by one if the rank of the matrix
\begin{equation}
  \label{eq:4.21}
  \begin{pmatrix}
    F_{x}+pF_{z}+rF_{p}+sF_{q}& F_{y}+qF_{z}+sF_{p}+tF_{q}& F_{r}& F_{s}& F_{t}\\
-\delta r&-\delta s&\delta x&\delta y&0\\
-\delta s&-\delta t& 0&\delta x&\delta y
  \end{pmatrix}
\end{equation}
is equal to $2$. This circumstance cannot arise for any  linear element tangent to a two dimensional integral variety, at least if there exists in this variety no relations between $x$ and $y$ \footnote{The cases where one such relation exists are of no interest: either there is only one relation between $x$ and $y$, for example $y$ as a function of $x$: then, according to the three equations in \eqref{eq:4.18} which follow the second equation, $z,p,q$ are also functions of $x$ and the equations of the variety are of the form
  \begin{align*}
    r+2sy'+ty'^{2}&=z''-qy'',\\
    F(x,y,z,p,q,r,s,t)&=0,
  \end{align*}
  or $x$ and $y$ are constants: then $z,p,q$ are constants as well and the equation of the variety becomes $F(x,y,z,p,q,r,s,t)=0$.
}. Indeed, our hypothesis in particular results in the relation
\[
F_{t}\delta x^{2}-F_{s}\delta x\,\delta y+F_{r}\delta y^{2}=0,
\]
from which we have the consequence
\[
F_{r}=F_{s}=F_{t}=0
\]
and, according the the first equation \eqref{eq:4.18}, the relations
\[
F_{x}+pF_{z}+rF_{p}+sF_{q}=0,\qquad F_{y}+q F_{z}+sF_{p}+tF_{q}=0,
\]
and no point of the integral variety is regular. \emph{Therefore there exists no singular integral variety other than those that satisfy equations \eqref{eq:4.19}.}

On an integral variety which is part of the general solution, the characteristics are the one dimensional varieties whose rank of the matrix \eqref{eq:4.21} is equal to $2$. In particular, we have, by moving along such a characteristic, the relation
\begin{equation}
  \label{eq:4.22}
  F_{t}\delta x^{2}-F_{s}\delta x\,\delta y+F_{r}\delta y^{2}=0.
\end{equation}

Through every point of the variety there passes two characteristics, and they are real if $(F_{s})^{2}-F_{r}F_{t}>0$. We easily verify that if we move on the integral variety in a way to satisfy the relation \eqref{eq:4.22}, the rank of the matrix \eqref{eq:4.21} automatically equals $2$.

Unlike what happens for first order partial differential equations, the characteristics of second order partial differential equations depend in general on an infinite number of arbitrary parameters (in fact, of arbitrary functions), and those are not the Cauchy characteristics. 

\vspace{12pt}\fsec \textsc{Example III.} \emph{System of two first order partial differential equations of two unknown functions $z_{1}, z_{2}$ in two independent variables $x,y$}.\index{partial differential equations!first order}

Such a system can be represented by the closed differential system
\begin{equation}
  \label{eq:4.23}
  \left\{
    \begin{aligned}
      F(x,y,z^{1},z^{2},p_{1},q_{1},p_{2},q_{2})&=0,\\
      \Phi(x,y,z^{1},z^{2},p_{1},q_{1},p_{2},q_{2})&=0,\\
      (F_{x}+p_{1}F_{z^{1}}+p_{2}F_{z^{2}})dx+(F_{y}+q_{1}F_{z^{1}}+q_{2}F_{z^{2}})dy\\
      +F_{p_{1}}dp_{1}+F_{q_{1}}dq_{1}+F_{p_{2}}dp_{2}+F_{q_{2}}dq_{2}&=0,\\
      (\Phi_{x}+p_{1}\Phi_{z^{1}}+p_{2}\Phi_{z^{2}})dx+(\Phi_{y}+q_{1}\Phi_{z^{1}}+q_{2}\Phi_{z^{2}})dy\\
      +\Phi_{p_{1}}dp_{1}+\Phi_{q_{1}}dq_{1}+\Phi_{p_{2}}dp_{2}+\Phi_{q_{2}}dq_{2}&=0,\\
      dz^{1}-p_{1}dx-q_{1}dy&=0,\\
      dz^{2}-p_{2}dx-q_{2}dy&=0,\\
      [dx\,dp_{1}]+[dy\,dq_{1}]&=0,\\
      [dx\,dp_{2}]+[dy\,dq_{2}]&=0.
    \end{aligned}
  \right.
\end{equation}

The character $s_{0}$ is equal to $4$ and it is the rank of the linear system consisting of the four equations in \eqref{eq:4.23} that follow the second equation. The rank decreases only if we have either
\begin{equation}
  \label{eq:4.24}
  \left\{
    \begin{aligned}
      F_{x}+p_{1}F_{z^{1}}+p_{2}F_{z^{2}}&=F_{y}+q_{1}F_{z^{1}}+q_{2}F_{z^{2}}=F_{p_{1}}=F_{q_{1}}=F_{p_{2}}=F_{q_{2}}=0,\\
      \Phi_{x}+p_{1}\Phi_{z^{1}}+p_{2}\Phi_{z^{2}}&=\Phi_{y}+q_{1}\Phi_{z^{1}}+q_{2}\Phi_{z^{2}}=\Phi_{p_{1}}=\Phi_{q_{1}}=\Phi_{p_{2}}=\Phi_{q_{2}}=0,
    \end{aligned}
  \right.
\end{equation}
or
\begin{equation}
  \label{eq:4.25}
  \frac{F_{x}+p_{1}F_{z^{1}}+p_{2}F_{z^{2}}}{\Phi_{x}+p_{1}\Phi_{z^{1}}+p_{2}\Phi_{z^{2}}}=\frac{F_{y}+q_{1}F_{z^{1}}+q_{2}F_{z^{2}}}{\Phi_{y}+q_{1}\Phi_{z^{1}}+q_{2}\Phi_{z^{2}}}=\frac{F_{p_{1}}}{\Phi_{p_{1}}}=\frac{F_{q_{1}}}{\Phi_{q_{1}}}=\frac{F_{p_{2}}}{\Phi_{p_{2}}}=\frac{F_{q_{2}}}{\Phi_{q_{2}}}.
\end{equation}

Therefore there are two possible kinds of singular integral varieties, according to wither the equations \eqref{eq:4.24} or \eqref{eq:4.25} are satisfied.

Let us now calculate the character $s_{1}$. To obtain the equations of the polar element of the linear integral elements of components $\delta x,\delta y, \delta z^{1},\delta z^{2},\delta p_{1},\delta q_{1}, \delta p_{2}, \delta q_{2}$, the four linear equations appearing in the system \eqref{eq:4.23} have to be supplemented with the equations
\begin{align*}
  \delta x\,dp_{1}+\delta y\,dq_{1}-\delta p_{1}dx-\delta q_{1}dy&=0,\\
  \delta x\,dp_{2}+\delta y\,dq_{2}-\delta p_{2}dx-\delta q_{2}dy&=0.
\end{align*}

We deduce from this that $s_{1}=2$. A linear integral element is singular if the six equations defining the polar element of this linear integral element reduce to five equations. Now on a two dimensional integral variety not having any relations between $x$ and $y$, these equations can contain no linear relations between $dx$ and $dy$ since the polar element considered contains all the linear elements tangent to the variety. Therefore it is necessary and sufficient that, for the linear element considered to be singular, the equations
\begin{equation}
  \label{eq:4.26}
  \left\{
    \begin{aligned}
      F_{p_{1}}dp_{1}+F_{q_{1}}dq_{1}+F_{p_{2}}dp_{2}+F_{q_{2}}dq_{2}&=0,\\
      \Phi_{p_{1}}dp_{1}+\Phi_{q_{1}}dq_{1}+\Phi_{p_{2}}dp_{2}+\Phi_{q_{2}}dq_{2}&=0,\\
      \delta x\,dp_{1}+\delta y\,dq_{1}&=0,\\
      \delta x\,dp_{2}+\delta y\,dq_{2}&=0,
    \end{aligned}
\right.
\end{equation}
reduce to three equations, which gives immediately
\begin{equation}
  \label{eq:4.27}
  \frac{D(F,\Phi)}{D(q_{1},q_{2})}\delta x^{2}-\left[\frac{D(F,\Phi)}{D(p_{1},q_{2})}-\frac{D(F,\Phi)}{D(p_{2},q_{1})}\right]\delta x\,\delta y+\frac{D(F,\Phi)}{D(p_{1},p_{2})}\delta y^{2}=0.
\end{equation}

We deduce two conclusions from this result:

1. We can have a second class of singular integral varieties, which satisfy the three equations
\begin{equation}
  \label{eq:4.28}
  \frac{D(F,\Phi)}{D(q_{1},q_{2})}=\frac{D(F,\Phi)}{D(p_{1},q_{2})}-\frac{D(F,\Phi)}{D(p_{2},q_{1})}=\frac{D(F,\Phi)}{D(p_{1},p_{2})}=0.
\end{equation}

2. On an ordinary integral variety, i.e., one that belongs to the general solution, there in general exists two families of characteristic lines, defined by the differential equation \eqref{eq:4.27}.
\begin{thm*}
  Giving a system of two first order partial differential equations of two unknown functions $z^{1},z^{2}$ in two independent variables $x,y$, there exist three classes of singular solutions, according to whether the equations \eqref{eq:4.24}, \eqref{eq:4.25} or \eqref{eq:4.28} are satisfied. Moreover, each general integral variety admits two families of characteristic lines defined by the differential equation \eqref{eq:4.27}.
\end{thm*}

\vspace{12pt}\fsec \emph{Second order partial differential equation of one unknown function $z$ in three independent variables $x^{1},x^{2},x^{3}$}\index{partial differential equations!second order}. We denote by $p_{i}$ and $p_{ij}=p_{ji}$ the first and second order partial derivatives of $z$, the indices indicating the variables with respect to which the derivation is taken. If $F(x^{i},z,p_{i},p_{ij})=0$ is the given equation, we can write
\begin{equation}
  \label{eq:4.29}
  \left\{
    \begin{aligned}
      A_{i}&=\frac{\pd F}{\pd x^{i}}+p_{i}\frac{\pd F}{\pd z}+p_{ik}\frac{\pd F}{\pd p_{k}},\\
      A^{ij}&=m\frac{\pd F}{\pd p_{ij}}\qquad \text{($m=1$ if $i=j$, $m=\frac{1}{2}$ if $i\neq j$).}
    \end{aligned}
\right.
\end{equation}

The given equation can be represented by the differential system formed by the $9$ equations
\begin{equation}
  \label{eq:4.30}
  \left\{
    \begin{aligned}
      F&=0,\\
      A_{i}dx^{i}+A^{ij}dp_{ij}&=0,\\
      dz-p_{i}dx^{i}&=0,\\
      dp_{i}-p_{ik}dx^{k}&=0,&(i&=1,2,3),\\
      [dx^{k}dp_{ik}]&=0,&(i&=1,2,3).
    \end{aligned}
\right.
\end{equation}

The character $s_{0}$ is equal to $5$, the rank of the system formed by the $5$ linear equations in \eqref{eq:4.30} following the equation $F=0$. A non-regular integral point is characterised by the relations
\begin{equation}
  \label{eq:4.31}
  A_{i}=0,\qquad A^{ij}=0.
\end{equation}
We have a first possible class of singular integral varieties, by finding the integral varieties satisfying the equations \eqref{eq:4.31}.

The polar system of a regular linear integral element contains $s_{0}+s_{1}=8$ equations, the first $5$ equations define the linear integral elements and the other $3$ equations are
\[
\delta x^{k}dp_{ik}-\delta p_{ik}dx^{k}=0,\qquad (i=1,2,3).
\]

The integral element $(\delta x^{k},\delta z,\dots)$ will be singular if the four equations
\begin{equation}
  \label{eq:4.32}
\left\{\begin{aligned}
  A_{i}dx^{i}+A^{ij}dp_{ij}&=0,\\
  \delta x^{k}dp_{ik}-\delta p_{ik}dx^{k}&=0
\end{aligned}\right.
\end{equation}
reduce to three equations. As on a three dimensional integral variety for which the independent variables are $x^{1},x^{2},x^{3}$ the equations \eqref{eq:4.32} can contain no linear relations between the $dx^{i}$, a singular linear integral element will be characterised by the property that the four equations
\begin{equation}
  \label{eq:4.33}
  \left\{
    \begin{aligned}
      A^{ij}dp_{ij}&=0,\\
      \delta x^{k}dp_{ik}&=0
    \end{aligned}
\right.
\end{equation}
reduce to three equations. Let $\delta x^{i}=a^{i}$ be such a singular integral element, which we assume to exist. If, in the equations \eqref{eq:4.33}, we replace $dp_{ij}$ by the product $\xi_{i}\xi_{j}$ of two new variables $\xi$, we see that the equation $a^{i}\xi_{i}=0$ must imply $A^{ij}\xi_{i}\xi_{j}=0$. Then the quadratic form $A^{ij}\xi_{i}\xi_{j}$ must decompose into a product of two linear factors:
\[
A^{ij}\xi_{i}\xi_{j}=a^{k}b^{h}\xi_{k}\xi_{h}
\]
from which
\begin{equation}
  \label{eq:4.34}
  A^{ij}=\frac{1}{2}(a^{i}b^{j}+b^{i}a^{j}).
\end{equation}

Conversely if the quantities $A^{ij}$ are of the form \eqref{eq:4.33}, we verify easily that there is on each point two singular linear integral elements, of components respectively $\delta x^{i}=a^{i}$ and $\delta x^{i}=b^{i}$.

We see that if an integral element does not annihilate $A^{ij}$, it is impossible for all its tangent linear elements to be singular.

On the other hand, we see that if the discriminant of the quadratic form $A^{ij}\xi_{i}\xi_{j}$ is zero, every non-singular integral variety admit two families of distinct or coinciding characteristic lines, one of them formed by the trajectories of the vector field $a^{i}$, the other formed  by the trajectories of the vector field $b^{i}$. On the contrary, if the discriminant of the quadratic form $A^{ij}\xi_{i}\xi_{j}$ is non-zero, there does not exist any characteristic lines.

Finally let us consider the character $s_{2}$. The polar element of a regular two dimensional integral element is given by the $5$ equations defining the linear integral elements, together with the six other equations which can be written, omitting the terms in $dx^{1},dx^{2},dx^{3}$,
\begin{align*}
  \delta_{1}x^{k}dp_{ik}&=0,\\
  \delta_{2}x^{k}dp_{ik}&=0,
\end{align*}
where we denote by $\delta_{1}x^{k}$ and $\delta_{2}x^{k}$ the components of two linear integral elements determining the two dimensional integral element considered. The equations can be written as
\[
\frac{dp_{i1}}{c_{1}}=\frac{dp_{i2}}{c_{2}}=\frac{dp_{i3}}{c_{3}},\qquad(i=1,2,3),
\]
where we denote by 
\[
c_{i}dx^{i}=0
\]
the equation of the plane integral element considered. From this it follows that the $dp_{ij}$ are proportional to the products $c_{i}c_{j}$. The two dimensional integral element will therefore be singular if we have
\begin{equation}
  \label{eq:4.35}
  A^{ij}c_{i}c_{j}=0.
\end{equation}

This equation expresses that in the three dimensional space constituted by the integral variety under consideration, the singular tangent plane elements based on a point is tangent to a cone of the second class having the point as its apex.

\emph{Therefore there exists two dimensional characteristic varieties: they are the solutions}, in the variety formed by the coordinates $x^{1},x^{2},x^{3}$, \emph{of a first order ordinary partial differential equation}. The \emph{bi-characteristics}\index{bi-characteristic (J.~Hadamard)} of J.~Hadamard are the characteristics of this equation. Strictly speaking it is not the characteristics of the differential system \eqref{eq:4.30}, unless the equation \eqref{eq:4.35} decompose into two linear equations, in which case the partial differential equation of the characteristics decompose into two linear equations whose integral surfaces are the surfaces formed  by the characteristic lines of the first family or by the characteristic lines of the second family.

\begin{thm*}
  A second order partial differential equation of one unknown function $z$ in three independent variables admit only singular solutions that annihilate the partial derivatives occurring in the defining equations that are second order derivatives with respect to $z$. The non-singular solutions always contain two dimensional characteristic varieties given by the integration of a first order partial differential equation, and the enveloping cone of the tangent planes of a point given by the characteristic varieties passing through the point is of the second class. If the cone degenerates into two lines $\Delta_{1},\Delta_{2}$, then there exists two families of two dimensional characteristic varieties: the first family consists of the surfaces formed by the trajectories of the lines $\Delta_{1}$ and the second the surface formed by the trajectories of the lines $\Delta_{2}$, and the trajectories are the characteristic lines of the general integral varieties of the given equation. On the contrary, if the cone is non-degenerate, then there does not exist any characteristic lines, i.e., the lines whose tangent elements are singular. The \textsc{bi-characteristics} of J.~Hadamard, that is to say the characteristics of the first order partial differential equation which gives the characteristic surfaces, are strictly speaking not characteristic lines, since the two dimensional integral varieties of the system \eqref{eq:4.30} passing through a bi-characteristic are provided by the Cauchy-Kowalewski theorem.
\end{thm*}

\chapter{Differential systems in involution with imposed independent variables}
\label{cha:diff-syst-invol}

\section{Generalities. Systems in involution}
\label{sec:gener-syst-invol}

\fsec In many applications the differential systems that we consider contain independent variables $x^{1},x^{2},\dots,x^{p}$ given to us, and after we put the differential system into the form \eqref{eq:4.1} of \textsection\textbf{52}, we are only interested in $p$ dimensional integral varieties and, among these, those that contain no relations between the variables $x^{1},x^{2},\dots,x^{p}$.
\begin{dfn*}
  A differential system $\Sigma$ of $n-p$ unknown functions $z^{\lambda}$ of $p$ variables $x^{i}$ is in involution\index{involution}\index{differential system in involution|see{involution}} if its genre is equal to or greater than $p$ and  the equations defining the $p$ dimensional generic ordinary integral elements do not introduce any linear relations between $dx^{1},dx^{2},\dots,dx^{p}$.
\end{dfn*}
It is then clear that the $p$ dimensional \emph{ordinary} integral varieties, with the independent variables $x^{1},x^{2},\dots,x^{p}$, can be obtained by applying the existence theorems stated and proved in the preceding chapter. 

\vspace{12pt}\fsec \emph{Systems of partial differential equations}.\index{partial differential equations} Every system of exterior differential equations of imposed independent variables can obviously written in the form of a system of partial differential equations of $n-p$ unknown functions in $p$ independent variables. The converse is also true. Indeed, to make things concrete, consider a system with a certain number of relations between  the partial derivatives of the first three orders of $q$ unknown functions $z^{\lambda}$. Denoting by $t_{i}^{\lambda},t_{ij}^{\lambda},t_{ijk}^{\lambda}$ the partial derivatives, the system will be closed by the given conditions between the variables
\[
x^{i},z^{\lambda},t_{i}^{\lambda},t_{ij}^{\lambda},t_{ijk}^{\lambda},\qquad(i,j,k=1,2,\dots,p;\ \lambda=1,2,\dots,q),
\]
together with the Pfaffian equations
\begin{align*}
  dz^{\lambda}-t_{i}^{\lambda}dx^{i}&=0,\\
  dt_{i}^{\lambda}-t_{ij}^{\lambda}dx^{j}&=0,\\
  dt_{ij}^{\lambda}-t_{ijk}^{\lambda}dx^{k}&=0.
\end{align*}

We then adjoin the equations which results from the preceding ones by exterior differentiation. The system $\Sigma$ obtained will in fact contain no exterior differential equations of degrees more than $2$. We will only have to find the $p$ dimensional integral varieties of the system, and among these, those that contain no relations between $x^{1},x^{2},\dots,x^{p}$.

In the theory of first order partial differential equations, S.~Lie has shown that it can be fruitful to relax the last restriction.



\vspace{12pt}\fsec We are going to state immediately the condition for a differential system $\Sigma$ to be in involution.
\begin{thm*}\index{involution criterion}
  For a closed differential system of $n-p$ unknown functions $z^{\lambda}$ in $p$ independent variables $x^{1},x^{2},\dots,x^{p}$ to be in involution, it is necessary and sufficient that the polar systems of  generic integral points and $q\le p-1$ dimensional generic integral elements contain no linear relations between $dx^{1},dx^{2},\dots,dx^{p}$.
\end{thm*}

The condition is obviously necessary. It is sufficient because if it is satisfied, the equations of generic $p$ dimensional ordinary integral elements contain no relations between $dx^{1},dx^{2},\dots,dx^{p}$. The $q<p$ dimensional regular integral elements contained in a $p$ dimensional ordinary integral element then can contain no more than $p-q$ independent relations between $dx^{1},dx^{2},\dots,dx^{p}$.


\section{Reduced characters}
\label{sec:reduced-characters}

\fsec We are going to investigate the criteria for involution by considering what we call the \emph{reduced characters}.

First let us determine the $p$ dimensional integral elements (without linear relations between the $dx^{i}$, which we suppose to always be the case in the following) having a generic integral point as the origin. We exclude the cases where the existence of such an integral element requires new relations between the dependent and independent variables, the cases in which the system will not be in involution. We consider the family $\mathcal{F}$ of integral elements of $1,2,\dots,p-1$ dimensions that may be contained in a $p$ dimensional integral element.
\begin{dfn*}
  A reduced polar system\index{reduced polar system} of an integral point or an integral element is the polar system of the point or the element where  we discard the terms in $dx^{1},dx^{2},\dots,dx^{p}$ and keep only the terms in $dz^{1},dz^{2},\dots,dz^{q}$ in the equations.
\end{dfn*}

We respectively denote by
\[
s'_{0},\quad s'_{0}+s'_{1},\quad s'_{0}+s'_{1}+s'_{2},\quad\dots\quad s'_{0}+s'_{1}+s'_{2}+\dots+s'_{p-1}
\]
the rank of the reduced polar system of a generic integral point, of a one dimensional integral element of the family $\mathcal{F}$, of a two dimensional integral element of the family $\mathcal{F}$ and so on.

\emph{The non-negative integers $s'_{0},s'_{1},\dots,s'_{p-1}$ are  the reduced characters\index{reduced character} of orders $0$, $1$, $\dots$, $p-1$.} It is clear that for the equations of the different reduced polar systems to contain only the variables $dz^{\lambda}$, we must have
\[
s'_{0}+s'_{1}+s'_{2}+\dots+s'_{p-1}\le n-p.
\]

We introduce finally the reduced character $s'_{p}$ by the relation
\[
s'_{0}+s'_{1}+s'_{2}+\dots+s'_{p-1}+s'_{p}=n-p.
\]


\vspace{12pt}\fsec \emph{Remark.} It may happen that the $p$ dimensional integral elements of a generic integral point consist of several distinct continuous families. Each of these families corresponds to a set of reduced characters. The problem of knowing if the given differential system is in involution has to be solved for each of these different families, since the $p$ dimensional integral varieties that we are seeking are not the same in each different case, since their $p$ dimensional tangent elements vary from one family to the other. Therefore it is possible that the given system is in involution for one of the families of $p$ dimensional integral varieties and is not in involution for another.


\section{Necessary and sufficient criterion for involution}
\label{sec:necess-suff-crit}

\fsec We now state a criterion for involution.\index{involution criterion!necessary and sufficient}

\theoremstyle{shape1}
\newtheorem*{cnsi}{\hspace{15pt}Necessary and sufficient criterion for involution}
\begin{cnsi}
  Consider a closed differential system of $n-p$ unknown functions in $p$ independent variables. Let $s'_{0},s'_{1},\dots,s'_{p-1}$ be the reduced characters of the system corresponding to the family [or one of the families] of $p$ dimensional integral elements of the system. For the system to be in involution, it is necessary and sufficient that the number of independent equations linking the parameters $t_{i}^{\lambda}$ of a generic $p$ dimensional integral element of the family is equal to 
\[
ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}.
\]
If the system is not in involution, the number of these equations is greater.
\end{cnsi}


\vspace{12pt}\fsec We begin by an important remark. Suppose, by means of a linear change of variables on $x^{1},x^{2},\dots,x^{p}$ with constant coefficients, that the rank of the reduced polar system of a generic linear integral element of the family $\mathcal{F}$ for which $dx^{2}=dx^{3}=\dots=dx^{p}=0$ is equal to its normal value $s'_{0}+s'_{1}$, that the rank of the reduced polar system of the generic two dimensional integral element of the family $\mathcal{F}$ for which $dx^{3}=\dots=dx^{p}=0$ is equal to $s'_{0}+s'_{1}+s'_{2}$ and so on.

If we denote by
\[
dz^{\lambda}=t_{1}^{\lambda}dx^{1}+t_{2}^{\lambda}dx^{2}+\dots+t_{p}^{\lambda}dx^{p}
\]
the equations of a $p$ dimensional generic element of the given family, we see that $t_{1}^{\lambda}$ must satisfy $s'_{0}$ linear independent equations, and these are the $s'_{0}$ reduced polar equations of a generic integral point where we replace $dz^{\lambda}$ by $t_{1}^{\lambda}$. For $t_{1}^{\lambda}$ satisfying these equations, $t_{2}^{\lambda}$ must satisfy $s'_{0}+s'_{1}$ independent equations, which are the $s'_{0}+s'_{1}$ reduced polar equations of the linear integral elements $(\delta x^{1}=1,\delta x^{2}=\dots=\delta x^{p}=0,\delta z^{\lambda}=t_{1}^{\lambda})$ where we replace $dz^{\lambda}$ by $t_{2}^{\lambda}$. And so on for the rest. This shows that there exists \emph{at least}
\[
ps'_{0}+(p-1)s'_{1}+\dots+2s'_{p-2}+s'_{p-1}
\]
independent equations which the parameters $t_{i}^{\lambda}$ must satisfy. This justifies the last part of the statement of the criterion. We say that these equations, which are well defined whether or not the system is in involution, are the equations that the parameters $t_{i}^{\lambda}$ \emph{normally} satisfy.


\vspace{12pt}\fsec \emph{Proof of the criterion}. We now proceed to prove the main part of the criterion.

First suppose that the system is in involution. Hence every linear integral element for which $dx^{i}$ do not all vanish belong to the family $\mathcal{F}$ and it only needs to satisfy the $s'_{0}$ equations which do not contain any relations between the $dx^{i}$, and therefore we have $s_{0}=s'_{0}$ \footnote{Recall that the $s_{i}$ are the characters defined in \textsection\textbf{56} and \textsection\textbf{57} and the $s'_{i}$ are the reduced characters.}. The polar system of a generic linear regular integral element is then defined by $s'_{0}+s'_{1}$ linear independent equations containing no relations between the $dx^{i}$, we therefore have $s'_{0}+s'_{1}=s_{0}+s_{1}$, from which $s'_{1}=s_{1}$, and all two dimensional ordinary integral elements that contain only $p-2$ relations between the $dx^{i}$ belong to the family $\mathcal{F}$. We can continue the argument to the end and we have $s'_{h}=s_{h}$ for $h=0,1,2,\dots,p-1$. But we know (\textsection\textbf{70}) that the number of independent equations which the parameters $t_{i}^{\lambda}$ satisfy for a $p$ dimensional ordinary integral element is equal to
\[
ps_{0}+(p-1)s_{1}+\dots+s_{p-1}=ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}.
\]
The condition for involution stated is hence necessary.

\emph{Conversely} suppose the system is not in involution. The parameters $t_{i}^{\lambda}$ satisfy
\[
ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}
\]
normal equations. But they are not the only ones. Indeed if all linear integral elements for which there exists only $p-1$ relations between the $dx^{i}$ belong to the family $\mathcal{F}$, we have $s'_{0}=s_{0}$, and if all two dimensional integral elements for which there are only $p-2$ relations between the $dx^{i}$ belong to the family $\mathcal{F}$, we have $s'_{1}=s_{1}$, but we cannot pursue these hypotheses to the end, otherwise the system will be in involution.  Therefore for example suppose that the three dimensional integral elements for which there exists only $p-3$ relations between the $dx^{i}$ do not all belong to the family $\mathcal{F}$. This means that the $t_{1}^{\lambda},t_{2}^{\lambda}$ and $t_{3}^{\lambda}$ satisfy other equations besides the $3s'_{0}+2s'_{1}+s'_{2}$ normal equations which relate them. Then the number of independent equations  which the parameters $t_{i}^{\lambda}$ satisfy is \emph{greater} than $ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}$.\qed

\begin{rmk*} By introducing  the character $s'_{p}$ as above, we can say that \emph{the necessary and sufficient condition for involution is that the most general $p$ dimensional integral element of the family considered depend on}
\[
s'_{1}+2s'_{2}+\dots+(p-1)s'_{p-1}+ps'_{p}
\]
\emph{independent parameters.}
  
\end{rmk*}



\vspace{12pt}\fsec \textsc{Example I.} Consider the system of two independent variables and three unknown functions, defined by the four equations
\begin{align*}
  [dx^{1}dz^{1}]&=0,\\
  [dx^{2}dz^{1}]&=0,\\
  [dz^{1}dz^{2}]&=0,\\
  [dz^{1}dz^{3}]&=0.
\end{align*}

The two dimensional integral elements form only one irreducible family defined by the equations
\begin{align*}
  dz^{1}&=0,\\
  dz^{2}&=a\,dx^{1}+b\,dx^{2},\\
  dz^{3}&=a'dx^{1}+b'dx^{2},
\end{align*}
with four arbitrary parameters: therefore there exists two relations between the $6$ quantities $t_{1}^{\lambda},t_{2}^{\lambda}$. Here we have $s_{0}=0$. On the other hand we can take for a generic linear element of the family $\mathcal{F}$ the element
\[
\delta x^{1}=\alpha,\quad \delta x^{2}=\beta,\quad \delta z^{1}=0,\quad \delta z^{2}=a\alpha+b\beta,\quad \delta z^{3}=a'\alpha+b'\beta,
\]
its reduced polar system is
\[
\alpha\,dz^{1}=0,\quad \beta\, dz^{1}=0,\quad(a\alpha+b\beta)dz^{1}=0,\quad(a'\alpha+b'\beta)dz^{1}=0,
\]
we therefore have $s'_{1}=1$, from which $s'_{2}=2$ (since $s'_{1}+s'_{2}$ is equal to $3$, the number of unknown functions). However, the number $4$ of independent parameters the two dimensional generic integral elements depend on is less than $s'_{1}+2s'_{2}=5$. The system is not in involution. We will arrive at the same conclusion if we suppress the last equation of the system, which gives a system that contains no more than two unknown functions.


\vspace{12pt}\fsec \textsc{Example II.} Consider the following system, of four unknown functions $z^{\lambda}$ of two independent variables $x^{1},x^{2}$,
\begin{align*}
  [dx^{1}dz^{1}]+[dx^{2}dz^{2}]&=0,\\
  [dx^{2}dz^{1}]&=0,\\
  [dx^{2}dz^{3}]&=0,\\
  [dz^{3}dz^{4}]&=0.
\end{align*}

The two dimensional integral elements are given by
\begin{alignat*}{2}
  dz^{1}&=&a\,dx^{2},\\
  dz^{2}&=a\,dx^{1}+&b\,dx^{2},\\
  dz^{3}&=&c\,dx^{2},\\
  c[dx^{2}dz^{4}]&=0.&
\end{alignat*}

Two cases can be distinguished:

1. $c\neq 0$. We then have the irreducible family
\begin{alignat*}{2}
  dz^{1}&=&a\,dx^{2},\\
  dz^{2}&=a\,dx^{1}+&b\,dx^{2},\\
  dz^{3}&=&c\,dx^{2},\\
  dz^{4}&=&h\,dx^{2}
\end{alignat*}
of four parameters $a,b,c,h$. The reduced polar system of a linear integral element $(\delta x^{1}=\alpha,\delta x^{2}=\beta,\delta z^{1}=a\beta,\delta z^{2}=a\alpha+b\beta,\delta z^{3}=c\beta,\delta z^{4}=h\beta)$ is
\begin{align*}
  \alpha\,dz^{1}+\beta\,dz^{2}&=0,\\
  \beta\,dz^{1}&=0,\\
  \beta\,dz^{3}&=0,\\
  c\beta\,dz^{4}-h\beta\,dz^{3}&=0,
\end{align*}
we therefore have $s'_{1}=4,s'_{2}=0$. The number $4$ of independent parameters of the two dimensional integral elements is equal to $s'_{1}+2s'_{2}$, and the system is in involution.

2. $c=0$. We then have the irreducible family
\begin{align*}
  dz^{1}&=a\,dx^{2},\\
  dz^{2}&=a\,dx^{1}+b\,dx^{2},\\
  dz^{3}&=0,\\
  dz^{4}&=h\,dx^{1}+k\,dx^{2}
\end{align*}
with $4$ independent parameters $a,b,h,k$. The reduced polar system of the linear element $(\delta x^{1}=\alpha,\delta x^{2}=\beta,\delta z^{1}=a\beta,\delta z^{2}=a\alpha+b\beta, \delta z^{3}=0,\delta z^{4}=h\alpha+k\beta)$ is
\begin{align*}
  \alpha dz^{1}+\beta dz^{2}&=0,\\
  \beta\,dz^{1}&=0,\\
  \beta\,dz^{3}&=0,\\
  (h\alpha+k\beta)dz^{3}&=0,
\end{align*}
we have $s'_{1}=3,s'_{2}=1$. As $4<s'_{1}+2s'_{2}=5$, the system is not in involution. 

We can remark that if we did not impose the choice of independent variables, we would have a system in involution with a single family of two dimensional ordinary integral elements, i.e., the first family that we considered.


\vspace{12pt}\fsec \emph{Remark}. We may be tempted to extend the criterion in the case where the $s'_{i}$ are defined by means of reduced polar systems of the generic integral elements of the corresponding dimensions belong to \emph{or not belong to} the family $\mathcal{F}$. But then the criterion could fail in the general case.

We can see this from the example in \textsection\textbf{86}. Indeed, if we take the system
\begin{align*}
  [dx^{1}dz^{1}]&=0,\\
  [dx^{2}dz^{1}]&=0,\\
  [dz^{1}dz^{2}]&=0,\\
  [dz^{1}dz^{3}]&=0,
\end{align*}
and we form the reduced polar system of the linear integral element $(\delta x^{1}=\alpha,\delta x^{2}=\beta,\delta z^{1}=t^{1},\delta z^{2}=t^{2},\delta z^{3}=t^{3})$, we obtain
\[
\alpha\,dz^{1}=0,\quad \beta\,dz^{1}=0,\quad t^{1}dz^{2}-t^{2}dz^{1}=0,\quad t^{1}dz^{3}-t^{3}dz^{1}=0,
\]
whose rank is $s'_{1}=3$. On the other hand the number of equations the two dimensional integral elements must satisfy have been shown to equal to $2$, and the number is \emph{less} than $2s'_{0}+s'_{1}=3$. We hence arrive at a result in contradiction with the last part of the criterion.

If we now take the system not in involution
\begin{align*}
  [dx^{1}dz^{1}]&=0,\\
  [dx^{2}dz^{1}]&=0,\\
  [dz^{1}dz^{2}]&=0,  
\end{align*}
the reduced polar system of the linear integral element $(\delta x^{1}=\alpha,\delta x^{2}=\beta,\delta z^{1}=t^{1},\delta z^{2}=t^{2})$ is
\[
\alpha\, dz^{1}=0,\qquad \beta\, dz^{1}=0,\qquad t^{1}dz^{2}-t^{2}dz^{2}=0,
\]
its rank is $s'_{1}=2$. On the other hand the number of equations the parameters of the two dimensional integral elements must satisfy is again equal to $2$, which is in this case equal to $2s'_{0}+s'_{1}$. Nonetheless the system is not in involution.


\section{A sufficient criterion for involution}
\label{sec:suff-crit-invol}

\fsec We are now going to establish a second criterion for involution that is \emph{only sufficient}, often useful in applications.\index{involution criterion!sufficient}

\newtheorem*{inv2cri}{\hspace{15pt}Second criterion, sufficient for involution}
\begin{inv2cri}
  Given a closed differential system of $n-p$ unknown functions in $p$ independent variables, consider an irreducible family of $p$ dimensional integral elements and the family $\mathcal{F}$ of the corresponding integral elements of $q=1,2,\dots,p-1$ dimensions. Denote by $\sigma_{0}=s'_{0}$ the rank of the reduced polar system of a generic point, by $\sigma_{0}+\sigma_{1}$ the rank of the reduced polar system of a generic linear element of the family $\mathcal{F}$ for which $\delta x^{2}=\delta x^{3}=\dots=\delta x^{p}=0$, by $\sigma_{0}+\sigma_{1}+\sigma_{2}$ the rank of the most general two dimensional reduced polar system of the family $\mathcal{F}$ for which $\delta x^{3}=\dots =\delta x^{p}=0$, and so on. The system is in involution if the number of independent equations that the parameters for the family  under consideration of $p$ dimensional integral element must satisfy is equal to $p\sigma_{0}+(p-1)\sigma_{1}+\dots+\sigma_{p-1}$.
\end{inv2cri}

\vspace{12pt}\fsec \emph{Proof}. We know from the previous criterion that the total number of independent equations that the parameters of a $p$ dimensional generic integral element in the family under consideration must satisfy is at least equal to $ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}$. We therefore have
\[
p\sigma_{0}+(p-1)\sigma_{1}+\dots+\sigma_{p-1}\ge ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}.
\]
But we obviously also have the inequalities (where the first is actually an equality)
\begin{align*}
  \sigma_{0}&\le s'_{0},\\
  \sigma_{0}+\sigma_{1}&\le s'_{0}+s'_{1}\\
  \sigma_{0}+\sigma_{1}+\sigma_{2}&\le s'_{0}+s'_{1}+s'_{2},\\
  &\dots\\
  \sigma_{0}+\sigma_{1}+\dots+\sigma_{p-2}+\sigma_{p-1}&\le s'_{0}+s'_{1}+\dots+s'_{p-2}+s'_{p-1},
\end{align*}
which implies, under addition,
\[
p\sigma_{0}+(p-1)\sigma_{1}+\dots+\sigma_{p-1}\le ps'_{0}+(p-1)s'_{1}+\dots+s'_{p-1}.
\]

It then follows:

1. that the two sides of the first inequality are equal and immediately all the preceding inequalities reduce to equalities $(\sigma_{i}=s'_{i})$;

2. that the system is in involution.

We can add that the $q$ dimensional integral elements of the family $\mathcal{F}$ for which $\delta x^{q+1}=\delta x^{q+2}=\dots=\delta x^{p}=0$ are \emph{regular}.


\vspace{12pt}\fsec \emph{Complementary remark}. The second criterion continue to be valid if the integers $\sigma_{i}$ are calculated by leaving aside one or more equations of the given differential system. Indeed, this can only diminish the numerical value of the integers $\sigma_{0},\sigma_{0}+\sigma_{1},\sigma_{0}+\sigma_{1}+\sigma_{2}$, etc., and the proof given for the criterion will continue to hold. Obviously, if we want to profit from this remark, it is essential to take into consideration \emph{all} equations of the system to determine the $p$ dimensional integral elements.

\vspace{12pt}\fsec \textsc{Particular case}. An interesting particular case occurring often in applications is when $dz^{\lambda}$ occur only in first degree in the exterior differential equations of the given closed system. Indeed, in this case the reduced characters can be calculated \emph{without first knowing the $p$ dimensional integral elements}. In this case the reduced polar system can be formed directly without this prior knowledge.

Take for example
\begin{equation}
  \label{eq:5.1}
  \left\{
  \begin{aligned}
    &\quad\ f_{\alpha}(x,z)=0&(\alpha&=1,2,\dots,r_{0}),\\
    \theta_{\alpha}&\equiv A_{\alpha i}dx^{i}+A_{\alpha\lambda}dz^{\lambda}=0&(\alpha&=1,2,\dots,r_{1}),\\
    \varphi_{\alpha}&\equiv \frac{1}{2}A_{\alpha ij}[dx^{i}dx^{j}]+A_{\alpha i \lambda}[dx^{i}dz^{\lambda}]=0&(\alpha&=1,2,\dots,r_{2}),\\
    \psi_{\alpha}&\equiv \frac{1}{6}A_{\alpha ijh}[dx^{i}dx^{j}dx^{h}]+\frac{1}{2}A_{\alpha ij\lambda}[dx^{i}dx^{j}dz^{\lambda}]=0&(\alpha&=1,2,\dots,r_{3}),\\
    &\dots
  \end{aligned}    
  \right.
\end{equation}
as the equations of the system. The reduced character $s_{0}$ is the rank of the system 
\[
A_{\alpha\lambda}dz^{\lambda}=0,
\]
the sum $s'_{0}+s'_{1}$ is the rank of the system
\begin{align*}
  A_{\alpha\lambda}dz^{\lambda}&=0,\\
  A_{\alpha i \lambda}\delta_{1}x^{i}dz^{\lambda}&=0,
\end{align*}
where the $\delta_{1}x^{i}$ are arbitrary parameters.

The sum $s'_{0}+s'_{1}+s'_{2}$ is the rank of the system
\begin{align*}
  A_{\alpha\lambda}dz^{\lambda}&=0,\\
  A_{\alpha i \lambda}\delta_{1}x^{i}dz^{\lambda}&=0,\\
  A_{\alpha i \lambda}\delta_{2}x^{i}dz^{\lambda}&=0,\\
  A_{\alpha ij\lambda}\delta_{1}x^{i}\delta_{2}x^{j}dz^{\lambda}&=0,
\end{align*}
where the $\delta_{1}x^{i}$ and $\delta_{2}x^{i}$ are arbitrary parameters, and so on.

We immediately observe that in this case there is only one irreducible family of $p$ dimensional integral elements, defined by the relations
\begin{align*}
  A_{\alpha i}+A_{\alpha\lambda}t_{i}^{\lambda}&=0,\\
  &(\alpha=1,2,\dots,r_{1};i=1,2,\dots,p);\\
  A_{\alpha ij}+A_{\alpha i\lambda}t_{j}^{\lambda}-A_{\alpha j\lambda}t_{i}^{\lambda}&=0,\\
  &(\alpha=1,2,\dots,r_{1};i,j=1,2,\dots,p);\\
  A_{\alpha ijh}+A_{\alpha ij\lambda}t_{h}^{\lambda}-A_{\alpha ih\lambda}t_{j}^{\lambda}+A_{\alpha jh\lambda}t_{i}^{\lambda}&=0,\\
  &(\alpha=1,2,\dots,r_{1};i,j,h=1,2,\dots,p);\\
\end{align*}

\section{The case of two independent variables}
\label{sec:case-two-independent}
\index{exterior differential system!two independent variables}

\fsec If a closed differential system contains only two independent variables, then this system contains no exterior differential equations of degree more than $2$. Therefore it is not necessary to concern us with equations resulting from exterior differentiation of second order equations that the system may have, since they are always satisfied by all two dimensional plane elements.

We suppose, for simplicity and without loss of generality, that the system contains no algebraic relations. We write the $s_{0}$ linear independent equations under the form
\[
\theta_{\alpha}=0\qquad(\alpha=1,2,\dots,s_{0}).
\]

Introduce, together with the differentials $dx, dy$ of the independent variables, $n-s_{0}-2$ linear differential forms $\varpi^{\lambda}$ $(\lambda=1,2,\dots,n-s_{0}-s)$ independent between themselves and independent with $\theta_{\alpha},dx,dy$. They therefore form, with the $\theta_{\alpha}$, $n-2$ independent forms with respect to the differentials of the unknown functions. Finally we can take, in place of $dx,dy$, two independent combinations $\omega^{1},\omega^{2}$ of these differentials, which may be convenient in applications.

This put the given system under the form
\begin{equation}
  \label{eq:5.2}
  \left\{
    \begin{aligned}
      \theta_{\alpha}&= 0&(\alpha&=1,2,\dots,s_{0}),\\
      \varphi_{\alpha}&\equiv C_{\alpha}[\omega^{1}\omega^{2}]+A_{\alpha\lambda}[\omega^{1}\varpi^{\lambda}]+B_{\alpha\lambda}[\omega^{2}\varpi^{\lambda}]\\
      &+\frac{1}{2}D_{\alpha\lambda\mu}[\varpi^{\lambda}\varpi^{\mu}]=0&(\alpha&=1,2,\dots,r).
    \end{aligned}
  \right.
\end{equation}

We are concerned only with the case where there exists two dimensional integral elements, which would moreover permit us to assume that the coefficients $C_{\alpha}$ are all zero by replacing $\varpi^{\lambda}$ by $\varpi^{\lambda}$ minus a linear combination of $\omega^{1}$ and $\omega^{2}$. We also exclude systems not in involution.

If we know the general equations
\begin{equation}
  \label{eq:5.3}
  \theta_{\alpha}=0,\qquad\varpi^{\lambda}=t_{1}^{\lambda}\omega^{1}+t_{2}^{\lambda}\omega^{2}
\end{equation}
of two dimensional integral elements, we know the family $\mathcal{F}$ of linear integral elements contained in a two dimensional integral element. After this, the reduced polar system of a linear integral element $(\omega^{i}_{\delta},\varpi^{\lambda}_{\delta})$ of the family $\mathcal{F}$ is formed by the equations $\theta_{\alpha}=0$ and the equations
\begin{equation}
  \label{eq:5.4}
  (A_{\alpha\lambda}\omega^{1}_{\delta}+B_{\alpha\lambda}\omega^{2}_{\delta})+D_{\alpha\lambda\mu}\varpi^{\lambda}_{\delta}\varpi^{\mu}=0.
\end{equation}

The matrix of coefficients of the polar system, or the \emph{polar matrix}\index{polar matrix}, is none other than the matrix of partial derivatives $\pd \varphi_{\alpha}/\pd \varpi^{\lambda}_{\delta}$ or, when we replace $\omega^{i}_{\delta},\varpi^{\lambda}_{\delta}$ by $\omega^{i},\varpi^{\lambda}$, the matrix of $r$ rows and $\nu=n-s_{0}$ columns
\begin{equation}
  \label{eq:5.5}
  \left(\frac{\pd \varphi_{\alpha}}{\pd \varpi^{\lambda}}\right).
\end{equation}

The reduced character $s'_{1}$, which we now write as $s_{1}$ as long as no confusion arises, is the rank of the polar matrix. The non-regular, or singular, linear elements are those that annihilate all the determinants formed by $s_{1}$ rows and $s_{1}$ columns of this matrix.

\vspace{12pt}\fsec \emph{Criterion for involution}.\index{involution condition!two independent variables} We immediately obtain a sufficient criterion for involution by observing that if the reduced character $s_{1}$ is equal to the number $r$ of independent linear  forms $\varphi_{\alpha}$, the condition which the coefficients $t_{1}^{\lambda},t_{2}^{\lambda}$ of the general equations \eqref{eq:5.2} of two dimensional integral elements must satisfy reduce to $s_{1}$ conditions
\[
C_{\alpha}+A_{\alpha\lambda}t_{2}^{\lambda}-B_{\alpha\lambda}t_{1}^{\lambda}+D_{\alpha\lambda\mu}t_{1}^{\lambda}t_{2}^{\mu}=0\qquad(\alpha=1,2,\dots,s_{1}).
\]
These conditions are necessarily independent, since the number of independent relations between the $t_{1}^{\lambda}$ and the $t_{2}^{\lambda}$ is \emph{at least} equal to $s_{1}$.

On the other hand there is a case where the sufficient criterion is also necessary, that is if $\varpi^{\lambda}$ enter linearly  in the forms $\varphi_{\alpha}$. Indeed in this case the equations which $t_{1}^{\lambda}$ and $t_{2}^{\lambda}$ must satisfy are
\[
C_{\alpha}+A_{\alpha\lambda}t_{2}^{\lambda}-B_{\alpha\lambda}t_{1}^{\lambda}=0,
\]
and it is clear that there are as many linearly independent equations of this system as there are linearly independent forms $\varphi_{\alpha}$ (taking into account the hypothesis made once and for all that these equations are compatible).

We therefore arrive at the following theorem.
\begin{thm*}
  A sufficient condition for a closed differential system of two independent variables to be in involution is that the reduced character $s_{1}$ is equal to the number of linearly independent quadratic forms $\varphi_{\alpha}$. This condition is also necessary if the forms $\varphi_{\alpha}$ contains only first degree terms of the forms $\varpi^{\lambda}$ (i.e., if the coefficients $D_{\alpha\lambda\mu}$ are all zero).
\end{thm*}

\vspace{12pt}\fsec \emph{Remark}. The following example shows that the condition is not always necessary. Consider the closed differential system of three quadratic exterior equations
\[
[\varpi^{2}\varpi^{3}]=0,\qquad[\varpi^{3}\varpi^{1}]=0,\qquad [\varpi^{1}\varpi^{2}]=0.
\]

The two dimensional integral elements are given by the equations
\begin{align*}
  \varpi^{1}&=a_{1}\omega^{1}+b_{1}\omega^{2},\\
  \varpi^{2}&=a_{2}\omega^{1}+b_{2}\omega^{2},\\
  \varpi^{3}&=a_{3}\omega^{1}+b_{3}\omega^{2},
\end{align*}
with
\[
a_{2}b_{3}-a_{3}b_{2}=0,\qquad a_{3}b_{1}-b_{3}a_{1}=0,\qquad a_{1}b_{2}-a_{2}b_{1}=0,
\]
the number of independent parameters where the equations depend on is equal to $4$ ($a_{1},a_{2},a_{3}$ are arbitrary and $b_{1},b_{2},b_{3}$ are proportional). On the other hand the polar matrix is
\[
\begin{pmatrix}
  0&\varpi^{3}&-\varpi^{2}\\
  -\varpi^{3}&0&\varpi^{1}\\
  \varpi^{2}&-\varpi^{1}&0
\end{pmatrix}
\]
whose rank is equal to $2$: $s_{1}=2$ and $s_{2}=1$. We have $s_{1}+2s_{2}=2+2=4$, the number of independent parameters of a generic two dimensional integral element. Nonetheless the number of linearly independent forms $\varpi_{\alpha}$ is equal to $3>s_{1}$.

\vspace{12pt}\fsec \emph{The case where $s_{2}=0$. Characteristics}. In the case where $s_{2}=0$, i.e., $s_{0}=n-s_{1}-2$, the number of the forms $\varpi^{\lambda}$ is equal to $s_{1}$ if the system is in involution. For all non-characteristic one dimensional integral variety there passes one and only one two dimensional integral variety. The characteristic lines of an ordinary integral variety annihilate all the determinants of $s_{1}$ rows and $s_{1}$ columns of the polar matrix. In the case where $s_{1}$ is equal to the number of linearly independent forms $\varphi_{\alpha}$, the polar matrix has exactly $s_{1}$ rows and $s_{1}$ columns, so that the characteristic lines of a given integral variety are given by a homogeneous equations of degree $s_{1}$ in $\omega^{1},\omega^{2}$, i.e., in $dx,dy$.

Take in particular the case where the coefficients $D_{\alpha\lambda\mu}$ of equations \eqref{eq:5.2} are zero, i.e., the $\varpi^{\lambda}$ enter linearly in the $\varphi_{\alpha}$. In this case the elements of the polar matrix are $A_{\alpha\lambda}\omega^{1}+B_{\alpha\lambda}\omega^{2}$. We can put the quadratic equations $\varphi_{\alpha}=0$ into a remarkable form highlighting the $s_{1}$ families of characteristics, at least when the families are distinct.

Indeed, let $\omega^{2}-m\omega^{1}=0$ be the equation of one of the families. The coefficient $m$ is the root of the equation
\begin{equation}
  \label{eq:5.6}
  |A_{\alpha\lambda}+mB_{\alpha\lambda}|=0,\qquad(\alpha,\lambda=1,2,\dots,s_{1}).
\end{equation}

Let us find a linear combination of the equations $\varphi_{\alpha}$ where the left hand side contains the factor $\omega^{2}-m\omega^{1}$. If $k^{\alpha}\varphi_{\alpha}=0$ is such a combination, that is, when $\lambda=1,2,\dots,s_{1}$ we have
\[
k^{\alpha}(A_{\alpha\lambda}+mB_{\alpha\lambda})=0.
\]
It is possible to find non-zero values of the $k^{\alpha}$ satisfying these $s_{1}$ homogeneous equations since the determinant of the unknown coefficients is zero. A solution of the quadratic exterior equations of the given differential system can therefore be written as the form
\[
[(\omega^{2}-m\omega^{1})\cdot c_{\lambda}\varpi^{\lambda}]=0.
\]

Then, if the $s_{1}$ families of characteristics are distinct and are given by the equations $\omega^{2}-m_{i}\omega^{1}=0$ $(i=1,2,\dots,s_{1})$, the quadratic equations of the system can be put under the form \footnote{We have assumed that the coefficients $C_{\alpha}$ are zero, which can always be realised by adding to $\varpi^{\lambda}$ suitable linear combinations of $\omega^{1},\omega^{2}$.}
\begin{equation}
\label{eq:5.7}
[(\omega^{2}-m_{i}\omega^{1})\cdot c_{i\lambda}\varpi^{\lambda}]=0,\qquad(i=1,2,\dots,s_{1}).
\end{equation}

These equations show a very interesting fact. When we are given, for determining an ordinary integral variety, a one dimensional \emph{characteristic} solution, the problem is in general impossible. This is evident from equations \eqref{eq:5.7}, since if we have $\omega^{2}=m_{i}\omega^{1}$ along the characteristic curve given, it is necessary that along this curve we also have
\[
c_{i\lambda}\varpi^{\lambda}=0,
\]
as on the unknown two dimensional integral variety the form $c_{i \lambda}\varpi^{\lambda}$ must be a multiple of $\omega^{2}-m_{i}\omega^{1}$. The question of whether this necessary condition is also sufficient remains unsolved.  If we suppose that it is sufficient, then the problem has infinitely many solutions. This is an area of the theory that is little studied and we know little about.

\section{System in involution whose general solution depends on one function of one variable}
\label{sec:syst-invol-gener}

\fsec We assume, without loss of generality, that the system does not contain algebraic equations. If it is in $p$ independent variables, we denote by $\omega^{1},\omega^{2},\dots,\omega^{p}$ a system of $p$ independent linear combinations of their differentials. Consider respectively
\begin{equation}
  \label{eq:5.8}
  \left\{
    \begin{aligned}
      \theta_{\alpha}&=0&(\alpha&=1,2,\dots,s_{0}),\\
      \varphi_{\alpha}&\equiv A_{\alpha i\lambda}[\omega^{i}\varpi^{\lambda}]=0&(\alpha&=1,2,\dots,r)
    \end{aligned}
  \right.
\end{equation}
the equations of the system which are of first and second degrees. We make the assumption that in the forms $\varphi_{\alpha}$ the $\varpi^{\lambda}$ appears only as linear terms. We did not include terms in $[\omega^{i}\omega^{j}]$ because the system, being in involution, admits $p$ dimensional integral elements and hence by adding to $\varpi^{\lambda}$ linear combinations of $\omega^{i}$, we can make the coefficients of the products $[\omega^{i}\omega^{j}]$ vanish.

The system may contain equations of degrees higher than $2$, but it is not useful to write them out.

We will indicate a remarkable form of the equations $\varphi_{\alpha}=0$ and deduce from it some relatively important consequences about the characteristics of the given system.

\vspace{12pt}\fsec Let $s_{1}$ be the first order reduced character. The characters of higher degrees are by our hypothesis all zero. The number of  forms $\varpi^{\lambda}$ independent between themselves and with respect to $\theta_{\alpha}$ and $\omega^{i}$ is equal to $s_{1}$. We can then suppose, by means of a suitable linear change of variables of $\omega^{i}$, that the rank of the system
\[
A_{\alpha1\lambda}\varpi^{\lambda}=0
\]
is equal to $s_{1}$ and, by a linear change of variables of the forms $\varphi_{\alpha}$, we have
\begin{equation}
  \label{eq:5.9}
  \left\{
    \begin{aligned}
      A_{\alpha 1 \lambda}\varpi^{\lambda}&\equiv \varpi^{\alpha}&(\alpha&=1,2,\dots,s_{1}),\\
      A_{\beta 1\lambda}\varpi^{\lambda}&=0&(\beta&=s_{1}+1,\dots,r).
    \end{aligned}
  \right.
\end{equation}

This granted, to say that the system is in involution with $s_{2}=s_{3}=\dots=s_{p}=0$ is to say that there exists exactly $(p-1)s_{1}$ relations between the coefficients of the equations
\begin{equation}
  \label{eq:5.10}
  \varpi^{\alpha}=t_{i}^{\alpha}\omega^{i}
\end{equation}
which gives a generic $p$ dimensional integral element. These relations are necessarily
\begin{equation}
  \label{eq:5.11}
  t_{i}^{\alpha}=A_{\alpha i \lambda}t_{1}^{\lambda}\qquad(\alpha=1,2,\dots,s_{1};\ i=2,3,\dots,p).
\end{equation}

In particular, it follows from this that \emph{there are no more than $s_{1}$ linearly independent forms $\varphi_{\alpha}$} ($r=s_{1}$), since the consideration of the form $\varphi_{s_{1}+1}$, in which $\omega^{1}$ does not appear, gives
\[
A_{s_{1}+1,i,\lambda}t_{1}^{\lambda}=0\qquad(t=2,3,\dots,p),
\]
which introduces relations between the $t_{1}^{\lambda}$ that cannot be deduced from \eqref{eq:5.11}.

We now form the determinant of the polar matrix of the linear elements $(\omega^{i}{}_{\delta})$. This is a homogeneous form of degree $s_{1}$ in $\omega^{1},\omega^{2},\dots,\omega^{p}$. Suppose, without loss of generality, that for $\omega^{3}=\dots=\omega^{p}=0$ the determinant decompose into a product of $s_{1}$ distinct linear forms in $\omega^{1},\omega^{2}$. According to the argument in \textsection\textbf{96}, we see that we can, by means of a suitable linear change of variables on $\varphi_{\alpha}$ and $\varpi^{\alpha}$, assume
\[
\varphi_{\alpha}\equiv[(\omega^{1}+m_{\alpha}\omega^{2})\varpi^{\alpha}]+A_{\alpha i \beta}[\omega^{i}\varpi^{\beta}]\qquad(\alpha=1,2,\dots,s_{1};\ i=3,4,\dots,p).
\]

From this we deduce  by requiring that the forms $\varpi^{\alpha}=t_{i}^{\alpha}\omega^{i}$ annihilate the forms $\varphi_{\alpha}$, in particular in the forms $\varphi_{\alpha}$ the coefficients of $[\omega^{2}\omega^{i}]$ are zero,
\[
m_{\alpha}t_{i}^{\alpha}=A_{\alpha i\beta}t_{2}^{\beta},
\]
from which, taking into considerations of the values \eqref{eq:5.11} of $t_{2}^{\beta}$ and $t_{i}^{\alpha}$,
\[
m_{\alpha}A_{\alpha i\beta}t_{1}^{\beta}=A_{\alpha i \beta}m_{\beta}t_{1}^{\beta},
\]
from which
\[
A_{\alpha i \beta}=0\qquad\text{for}\qquad \alpha\neq \beta.
\]

By putting $A_{\alpha i\alpha}=m_{i\alpha}$ and, for symmetry reason, $m_{\alpha}=m_{2\alpha}$, we finally have
\begin{equation}
  \label{eq:5.12}
  \varphi_{\alpha}\equiv[(\omega^{1}+m_{i\alpha}\omega^{i})\varpi^{\alpha}]=0.
\end{equation}

This is the remarkable form that we can bring the quadratic exterior equations of the given differential system into.

\vspace{12pt}\fsec The characteristic lines of the integral varieties are therefore those that are, at each of their point, tangent to the $s_{1}$ plane elements of $(p-1)$ dimensions defined by the equations
\[
\omega^{1}+m_{i\alpha}\omega^{i}=0\qquad(\alpha=1,2,\dots,s_{1}).
\]

These $p-1$ dimensional elements enjoy a remarkable property, which is a consequence of the following theorem:
\begin{thm*}
  On a given integral variety, each of the equations
\[
\omega^{1}+m_{i\alpha}\omega^{i}=0
\]
is completely integrable.
\end{thm*}

Before we proceed with the proof, we will write, for brevity,
\begin{equation}
  \label{eq:5.13}
  \omega^{1}+m_{i\alpha}\omega^{i}=\bar\omega^{\alpha}\qquad(\alpha=1,2,\dots,s_{1}).
\end{equation}
The $\bar\omega^{\alpha}$ are $s_{1}$ distinct forms in $\omega^{1},\omega^{2},\dots,\omega^{p}$ which naturally cannot be linearly independent.

With this notation in place, we first remark that on an integral variety, we have the relations of the form
\begin{equation}
  \label{eq:5.14}
  \varpi^{\alpha}=t^{\alpha}\bar\omega^{\alpha},
\end{equation}
which follows from \eqref{eq:5.12}. On the other hand the equation $d\varphi_{\alpha}=0$ is a consequence of the equations of the given differential system, which is closed. The exterior differential $d\varphi_{\alpha}$ is therefore identically zero when we take into account the equations $\theta_{\beta}=0$ and when we replace $\varpi^{\lambda}$ by $t^{\lambda}\bar\omega^{\lambda}$ (the equations of the system which are of degrees more than $2$ are identically satisfied under the preceding conditions).

\emph{We now fix the index $\alpha$.} We have, by $\varphi_{\alpha}=[\bar\omega^{\alpha}\varpi^{\alpha}]$,
\begin{equation}
  \label{eq:5.15}
  d\varphi_{\alpha}=[d\bar\omega^{\alpha}\varpi^{\alpha}]-[\bar\omega^{\alpha}d\varpi^{\alpha}].
\end{equation}
Suppose we have, taking into account the equations $\theta_{\beta}=0$ \footnote{Naturally the coefficients $a_{ij},a_{i\lambda},\dots$ of the equations \eqref{eq:5.16} vary with the index $\alpha$ remaining fixed.},
\begin{equation}
  \label{eq:5.16}
  \left\{
    \begin{aligned}
      d\bar\omega^{\alpha}&=\frac{1}{2}a_{ij}[\omega^{i}\omega^{j}]+a_{i\lambda}[\omega^{i}\varpi^{\lambda}],\\
      d\varpi^{\alpha}&=\frac{1}{2}c_{ij}[\omega^{i}\omega^{j}]+c_{i\lambda}[\omega^{i}\varpi^{\lambda}]+\frac{1}{2}c_{\lambda\mu}[\varpi^{\lambda}\varpi^{\mu}],
    \end{aligned}
  \right.
\end{equation}
the exterior differential $d\bar\omega^{\alpha}$ does not contain the terms in $[\varpi^{\lambda}\varpi^{\mu}]$ because the equations $\omega^{1}=\omega^{2}=\dots=\omega^{p}=0$ form a completely integrable system \footnote{The forms $\omega^{i}$ are independent linear combinations of $dx^{1},dx^{2},\dots,dx^{p}$.}.

We have, according to \eqref{eq:5.15} and \eqref{eq:5.16},
\begin{align}
  \label{eq:5.17}
  d\varphi_{\alpha}=\frac{1}{2}a_{ij}[\omega^{i}\omega^{j}\varpi^{\alpha}]&+a_{i\lambda}[\omega^{i}\varpi^{\lambda}\varpi^{\alpha}]-\frac{1}{2}c_{ij}[\omega^{i}\omega^{j}\bar\omega^{\alpha}]\\
  &+c_{i\lambda}[\omega^{i}\bar\omega^{\alpha}\varpi^{\lambda}]-\frac{1}{2}c_{\lambda\mu}[\bar\omega^{\alpha}\varpi^{\lambda}\varpi^{\mu}].\notag
\end{align}
We therefore must have, \emph{regardless of the arbitrary parameters $t^{\lambda}$},
\begin{align*}
  \frac{1}{2}t^{\alpha}a_{ij}[\omega^{i}\omega^{j}\bar\omega^{\alpha}]+t^{\alpha}t^{\lambda}a_{i\lambda}[\omega^{i}\bar\omega^{\lambda}\bar\omega^{\alpha}]&-\frac{1}{2}[\omega^{i}\omega^{j}\bar\omega^{\alpha}]+t^{\lambda}c_{i\lambda}[\omega^{i}\bar\omega^{\alpha}\bar\omega^{\lambda}]\\
&-\frac{1}{2}t^{\lambda}t^{\mu}c_{\lambda\mu}[\bar\omega^{\alpha}\bar\omega^{\lambda}\bar\omega^{\mu}]=0.
\end{align*}

Equating to zero the coefficients of $t^{\alpha}$ and of $t^{\alpha}t^{\beta}$, where we suppose $\beta\neq\alpha$, we obtain
\[
a_{ij}[\omega^{i}\omega^{j}\bar\omega^{\alpha}]=0,\qquad a_{i\beta}[\omega^{i}\bar\omega^{\beta}\bar\omega^{\alpha}]=0,
\]
in the second set of equations there is no summation with respect to $\beta$. These equations express that \emph{on every integral variety} the form $d\bar\omega^{\alpha}$ vanishes by taking into consideration $\bar\omega^{\alpha}=0$. The last equation is therefore completely integrable.\qed


\vspace{12pt}\fsec The preceding theorem can be stated in the following form:
\begin{thm*}
  Every differential system in involution in $p$ independent variables whose general solution depends only on $s_{1}$ arbitrary functions of one variable in general admits $s_{1}$ families of $p-1$ dimensional characteristic varieties such that there passes one and only one characteristic variety in each family for a point on the integral variety. Every curve on one of these characteristic varieties is itself a characteristic.
\end{thm*}

\begin{rmk*}
  If the $s_{1}$ families of linear characteristic elements are not distinct, the resulting form \eqref{eq:5.12} of exterior quadratic equations of the differential system will become less simple. We can show that the polar matrix of $s_{1}$ rows and $s_{1}$ columns can be reduced in a manner that all its elements above the principal diagonal is zero. This means that \emph{the determinant of the polar matrix decomposes into a product of $s_{1}$ linear forms in $\omega^{1},\omega^{2},\dots,\omega^{p}$} \footnote{Once we make the forms $A_{\alpha1\lambda}\varpi^{\lambda}$ into $\varpi^{\alpha}$, we can regard the $s_{1}^{2}$ coefficients $A_{\alpha i\beta}$, where $i$ has a fixed value greater than $1$, as the elements of a matrix $S_{i}$. The involution condition for the system, which are expressed by the fact that the relations
\[
A_{\alpha i\lambda}t^{\lambda}_{j}=A_{\alpha j\lambda}t^{\lambda}_{i}
\]
are consequences of the equations \eqref{eq:5.11} where the $t_{1}^{\lambda}$ are not constrained by any relation, is simply equivalent to the property that the $p-1$ matrices $S_{2},\dots,S_{p}$ mutually commute. From the theory of commuting matrices it follows that the characteristic equation of the matrix $u^{2}S_{2}+u^{3}S_{3}+\dots+u^{p}S_{p}$, where the $u^{i}$ are parameters, has all of its roots linear in $u^{2},u^{3},\dots,u^{p}$. As the matrix $S_{1}$ is the unit matrix, this means that the determinant of the matrix $u^{1}S_{1}+u^{2}S_{2}+\dots+u^{p}S_{p}$ is the product of the linear forms in $u^{1},u^{2},\dots,u^{p}$. By replacing $u^{i}$ by $\omega^{i}$, we obtain the result in the text.
}.

We will end this chapter with an application of the theory of systems in involution to Pfaffian systems.

\end{rmk*}


\section{A theorem of J.-A. Schouten and van der Kulk}
\label{sec:theorem-j.-a}\index{Schouten-van der Kulk theorem}

\fsec This theorem is concerned with a generic system of linear equations in total differentials (Pfaffian system). Let
\begin{equation}
  \label{eq:5.18}
  \theta_{\alpha}=0\qquad(\alpha=0,1,2,\dots,q)
\end{equation}
be a system of $q+1$ linearly independent equations in $n$ variables $x^{1},x^{2},\dots,x^{n}$, which contain both dependent and independent variables. \textsc{F.~Engel} has attracted attention by his discovery of a new numeric invariant of Pfaffian system, which is, in the present case, the largest integer $m$ such that the exterior form of degree $2m+q+1$
\begin{equation}
  \label{eq:5.19}
  [\theta_{0}\theta_{1}\dots\theta_{q}(\lambda_{0}d\theta_{0}+\lambda_{1}d\theta_{1}+\dots+\lambda_{q}d\theta_{q})^{m}]
\end{equation}
does not vanish identically, where $\lambda_{0},\lambda_{1},\dots,\lambda_{q}$ are arbitrary parameters.

The theorem in question can be stated in the following manner:
\begin{thm*}
  If $m$ is the Engel invariant of system \eqref{eq:5.18}, it is possible to find an algebraically equivalent system whose left hand sides are all of class $2m+1$.
\end{thm*}

It is clear that if $2m+q$ is equal or greater than $n$, then the form \eqref{eq:5.19} is identically zero. Hence we certainly have $2m\le n-q$.

\vspace{12pt}\fsec To put the problem of the determining a system enjoying the property indicated algebraically equivalent to a given system into equations, suppose, as it is always permissible, that the form
\[
[\theta_{0}\theta_{1}\dots\theta_{q}(d\theta_{0})^{m}]
\]
does not vanish identically, and write
\[
\Theta=\theta_{0}+u^{1}\theta_{1}+u^{2}\theta_{2}+\dots+u^{q}\theta_{q},
\]
where $u^{1},u^{2},\dots,u^{q}$ denote $q$ unknown functions in the variables $x^{1},x^{2},\dots,x^{n}$.

We are going to determine the unknown functions such that the form $\Theta$ is of class $2m+1$. This condition is expressed by the equation of degree $2m+3$
\begin{equation}
  \label{eq:5.20}
  [\Theta(d\Theta)^{m+1}]=0
\end{equation}
to which we should adjoin the equation obtained by exterior differentiation, namely
\begin{equation}
  \label{eq:5.21}
  [(d\Theta)^{m+2}]=0.
\end{equation}

By hypothesis, if we regard $u^{1},u^{2},\dots,u^{p}$ as constants, the exterior differential $d\Theta$ is reducible$\pmod{\theta_{0},\theta_{1},\dots,\theta_{q}}$ to
\[
[\omega^{1}\omega^{2}]+[\omega^{3}\omega^{4}]+\dots+[\omega^{2m-1}\omega^{2m}],
\]
where the $\omega^{i}$ are $2m$ linearly independent differential forms constructed with the variables $x^{i}$ and their differentials, whose the coefficients naturally can depend on $u^{1},u^{2},\dots,u^{q}$.

We then have, by regarding now the $u^{i}$ as variables,
\begin{equation}
  \label{eq:5.22}
  d\Theta\equiv[\omega^{1}\omega^{2}]+\dots+[\omega^{2m-1}\omega^{2m}]+[\theta_{1}\varpi^{1}]+\dots+[\theta_{q}\varpi^{q}]\pmod{\Theta},
\end{equation}
where $\varpi^{i}+du^{i}$ are linear forms in $dx^{1},dx^{2},\dots,dx^{n}$.



\vspace{12pt}\fsec We will first find the $n$ dimensional generic integral elements of the system \eqref{eq:5.20}, \eqref{eq:5.21}. The equation \eqref{eq:5.20} express that $d\Theta$ is$\pmod{\Theta}$ reducible to an exterior quadratic form constructed with $2m$ independent linear forms, which is to say that the right hand side of the congruence \eqref{eq:5.22} is, when we replace the $\varpi^{i}$ by their values, reducible$\pmod{\Theta}$ to
\[
[\bar\omega^{1}\bar\omega^{2}]+[\bar\omega^{3}\bar\omega^{4}]+\dots+[\bar\omega^{2m-1}\bar\omega^{2m}],
\]  
where $\bar\omega^{i}$ is the sum of $\omega^{i}$ and a linear combination of the forms $\theta_{1},\theta_{2},\dots, \theta_{q}$ and of $n-2m-q-1$ other forms independent of $\omega^{i}$ and $\theta_{\alpha}$, which we write as $\omega^{2m+1},\dots,\omega^{n-q-1}$. But we see immediately that the right hand side of \eqref{eq:5.22} is incompatible with the presence of the last $n-q-1$ forms. We therefore finally have a congruence
\begin{align}
  \label{eq:5.23}
  d\Theta\equiv[(\omega^{1}&+a^{1\alpha}\theta_{\alpha})(\omega^{2}+a^{2\alpha}\theta_{\alpha})\\
  &+\dots+[(\omega^{2m-1}+a^{2m-1,\alpha}\theta_{\alpha})(\omega^{2m}+a^{2m,\alpha}\theta_{\alpha})]\pmod{\Theta}.\notag
\end{align}

It follows, by expanding the coefficients of $\theta_{1},\theta_{2},\dots,\theta_{q}$ on the right hand side,
\begin{align}
  \label{eq:5.24}
  \varpi^{\alpha}=a^{1\alpha}\omega^{2}-a^{2\alpha}\omega^{1}+\dots&+a^{2m-1,\alpha}\omega^{2m}-a^{2m,\alpha}\omega^{2m-1}\\
  &+b^{\alpha\lambda}\theta_{\lambda}+c^{\alpha}\Theta\qquad(\alpha=1,2,\dots,q).\notag
\end{align}

The identification with \eqref{eq:5.22} gives
\begin{align}
  \label{eq:5.25}
  b^{\alpha\beta}-b^{\beta\alpha}=a^{1\alpha}a^{2\beta}&-a^{1\beta}a^{2\alpha}+\dots+a^{2m-1,\alpha}a^{2m,\beta}\\
  &-a^{2m,\alpha}a^{2m-1,\beta}\qquad(\alpha,\beta=1,2,\dots,q).
  \notag
\end{align}

The $n$ dimensional element defined by the equations \eqref{eq:5.24}, where the coefficients satisfy the relations \eqref{eq:5.25}, satisfies equation \eqref{eq:5.20}. It then also satisfies automatically equation \eqref{eq:5.21}, since the form $d\Theta$, being the sum of at most $m+1$ independent quadratic monomials according to \eqref{eq:5.20}, has its $(m+2)$-th power identically zero.

The number of independent parameters that the most general $n$ dimensional integral element of the closed system \eqref{eq:5.20}, \eqref{eq:5.21} is then equal to the number $2mq$ of the parameters $a^{i\alpha}$, added to the number $q(q+1)/2$ of independent parameters $b^{\alpha\beta}$ [which are $q^{2}$ parameters constrained by the $q(q-1)/2$ relations \eqref{eq:5.25}], and finally added to with the $q$ parameters $c^{\alpha}$, which gives the total number of independent parameters
\begin{equation}
  \label{eq:5.26}
  2mq+\frac{q(q+3)}{2}.
\end{equation}



\vspace{12pt}\fsec We now come to the determination of the reduced characters of the differential system. The equation \eqref{eq:5.20} being of degree $2m+3$, all $2m+1$ dimensional elements are integral. We therefore have
\[
s_{0}=s_{1}=s_{2}=\dots=s_{2m+1}=0.
\]

We now consider the following chain of integral elements $E_{2m+2},\dots,E_{n-1}$, which successively introduce between the differentials of the $n$ variables the following relations:
\begin{alignat*}{5}
  E_{2m+2}&:\omega^{2m+1}=\dots&=\omega^{n-q-1}&=\theta^{2}&=\theta^{3}=\dots=\theta_{q}&=0;\\
  E_{2m+3}&:\omega^{2m+1}=\dots&=\omega^{n-q-1}&&=\theta^{3}=\dots=\theta_{q}&=0;\\
  &\dots&&&&\dots\\
  E_{2m+q}&:\omega^{2m+1}=\dots&=\omega^{n-q-1}&&&=0;\\
  &\dots&&&&\dots\\
  E_{2m+q}&:&\quad\ \omega^{n-q-1}&&&=0;\\
\end{alignat*}

To form the reduced polar system of each of the integral elements, we need only the equation \eqref{eq:5.20}. We then obtain the ranks $\sigma_{2m+2},\sigma_{2m+2}+\sigma_{2m+3},\dots$, which will be at most equal to the rank $s_{2m+2},s_{2m+2}+s_{2m+3},\dots$. According to the sufficient criterion in \textsection\textbf{29}, the given differential system will be certainly in involution if the number \eqref{eq:5.26} of independent parameters of a generic $n$ dimensional integral element is equal to
\[
(2m+2)\sigma_{2m+2}+(2m+3)\sigma_{2m+3}+\dots+n\sigma_{n}.
\]



\vspace{12pt}\fsec To form the reduced polar system of $E_{2m+2}$, for which we have
\[
\omega^{2m+1}=\omega^{2m+2}=\dots=\omega^{n-q-1}=0,\qquad\theta_{2}=\theta_{3}=\dots=\theta_{q}=0,
\]
it suffices, in calculating $(d\Theta)^{m+1}$, to take into consideration only the terms in $\omega^{1}$, $\omega^{2}$, $\dots$ , $\omega^{2m}$, $\theta^{1}$, which reduce $(d\Theta)^{m+1}$ to the form $[\omega^{1}\omega^{2}\dots\omega^{2m}\theta_{1}\varpi^{1}]\pmod{\Theta}$ up to a numerical factor. The reduce polar system is therefore simply the equation $\varpi^{1}=0$ and we have
\[
\sigma_{2m+2}=1.
\]

The calculation of the reduced polar system of $E_{2m+3}$ is done by reducing $d\Theta\pmod{\Theta}$ to
\[
[\omega^{1}\omega^{2}]+\dots+[\omega^{2m-1}\omega^{2m}]+[\theta_{1}\varpi^{1}]+[\theta_{2}\varpi^{2}],
\]
from which we obtain the equations
\[
\varpi^{1}=0,\qquad\varpi^{2}=0,
\]
we therefore have $\sigma_{2m+2}+\sigma_{2m+3}=2$, from which
\[
\sigma_{2m+3}=1.
\]

We continue in this way step by step to find
\[
\sigma_{2m+4}=\dots=\sigma_{2m+q+1}=1.
\]
As the sum of the already calculated $\sigma$ is equal to $q$, the number of unknown functions, all the following $\sigma$ are zero.

An immediate calculation now gives
\begin{align*}
  (2m+2)\sigma_{2m+2}+\dots+n\sigma&=2mq+(2+3+\dots+q+1)\\
  &=2mq+\frac{q(q+3)}{2}.
\end{align*}

This result shows that \emph{the system is in involution and its general solution depend on an arbitrary function of $2m+q+1$ variables}.

As there always exists one solution (and even an infinite number of solutions) for which the unknown functions $u^{1},u^{2},\dots,u^{q}$ take, for given numerical values $(x^{i})_{0}$ of the variables $x^{i}$, arbitrarily given numerical values, we can find $q+1$ particular solutions such that for $x^{i}=(x^{i})_{0}$, the $q+1$ corresponding forms $\Theta$ are linearly independent in $dx^{1},dx^{2},\dots,dx^{n}$. The theorem is thus proved.

\chapter[Prolongation of differential system]{Prolongations of\\differential system}
\label{cha:prol-diff-syst}

\section{A fundamental problem}
\label{sec:fundamental-problem}

\fsec In the previous chapters we have proved the existence theorems for certain differential systems with imposed independent variables, which are called systems in involutions. The solutions of these system whose existence we have showed by application of the Cauchy-Kowalewski theorem are those that constitute the \emph{general solution} of the system under consideration. However, we know that there can also exist other solutions, which are the \emph{singular solutions}\index{singular solution of differential system}, given by new differential systems each of which is obtained by adjoining new relations between the dependent variables to the given systems of equations, and the new system is in general not in involution. A fundamental problem therefore consists of knowing what information we can obtain for the solutions of a system not in involution, in particular, \emph{given a particular solution of a given differential system, can the solution be obtained as a non-singular solution of a system in involution which may be deducted from the given system by a systematic procedure}?

We will try to answer this question in the present chapter. The systematic procedure which we mentioned involves the notion of \emph{prolongation}\index{prolongation} of a differential system, which we will introduce in the following section.

\vspace{12pt}\fsec \emph{Reduction in the case of a linear Pfaffian system}.\index{Pfaffian problem} We can always, for sake of convenience, suppose that the given system contain only algebraic equations between the dependent and independent variables and linear differential equations with respect to both the dependent and independent variables. Indeed (\textsection\textbf{79}) every differential system can be regarded as a system of first order differential equations of $n-p$ unknown functions $z^{\lambda}$ in $p$ independent variables $x^{1},x^{2},\dots,x^{p}$. From this point of view we can write
\begin{equation}
  \label{eq:6.1}
  \left\{
    \begin{aligned}
      F_{\alpha}(x^{i},z^{\lambda},t_{i}^{\lambda})&=0&(i=1,2,\dots,p;\ \lambda&=1,2,\dots,n-p;\ \alpha=1,2,\dots,r_{0}),\\
      dz^{\lambda}-t_{i}^{\lambda}dx^{i}&=0&(\lambda&=1,2,\dots,n-p).
    \end{aligned}
  \right.
\end{equation}
It will be convient to adjoin to these equations those that are deduced by exterior differentiation, i.e.,
\begin{equation}
  \label{eq:6.2}
  \left\{
    \begin{aligned}
      \frac{\pd F_{\alpha}}{\pd x^{i}}dx^{i}+\frac{\pd F_{\alpha}}{\pd z^{\lambda}}dz^{\lambda}+\frac{\pd F_{\alpha}}{\pd t_{i}^{\lambda}}dt_{i}^{\lambda}&=0&&(\alpha=1,2,\dots,r_{0}),\\
    [dx^{i}dt_{i}^{\lambda}]&=0&&(\lambda=1,2,\dots,n-p).
  \end{aligned}
  \right.
\end{equation}

We now change notation: we denote by $\nu$ the total number of dependent variables $z^{\lambda}, t_{i}^{\lambda}$ which appear in the equations \eqref{eq:6.1} and \eqref{eq:6.2}. The closed system \eqref{eq:6.1}, \eqref{eq:6.2} of $\nu$ unknown functions, which we now denote together with the general notation $z^{\lambda}$ $(\lambda=1,2,\dots,\nu)$ can be written under the form
\begin{equation}
  \label{eq:6.3}
  \left\{
    \begin{aligned}
      F_{\alpha}(x,z)&=0&(\alpha&=1,2,\dots,r_{0}),\\
      \theta_{\alpha}&=0&(\alpha&=1,2,\dots,r_{1}),\\
      \varphi_{\alpha}\equiv C_{\alpha ij}[dx^{i}dx^{j}]+A_{\alpha ij}[dx^{i}\varpi^{\lambda}]&=0&(\alpha&=1,2,\dots,r_{r}),
    \end{aligned}
  \right.
\end{equation}

The $\theta_{\alpha}$ are linear forms in $dx^{1},dx^{2},\dots,dx^{p},dz^{1},dz^{2},\dots,dz^{\nu}$, which are the left hand sides of the last equations of \eqref{eq:6.1} and the first equations of \eqref{eq:6.2}, or independent linear combinations of them. The $\varphi_{\alpha}$ are the left hand sides of the last equations of \eqref{eq:6.2} or independent linear combinations of them. The $\varpi^{\lambda}$ are  linear differential forms which together with the $\theta_{\alpha}$ consist a system of $\nu$ independent Pfaffian forms with respect to $dz^{1},dz^{2},\dots,dz^{\nu}$. It may be useful to replace the differentials $dx^{1},dx^{2},\dots,dx^{p}$ with a system of $p$ independent linear combinations of themselves, which we denote by $\omega^{1},\omega^{2},\dots,\omega^{p}$, the coefficients of $\omega^{i}$ may depend on the dependent as well as the independent variables.



\vspace{12pt}\fsec \emph{Remark}. We can suppose the $r_{1}$  forms $\theta_{\alpha}$ to be independent, i.e., that the rank of the linear system $\theta_{\alpha}=0$ is equal to $r_{1}$. Of course this assumes that we are on a \emph{generic} point $(x,z)$ and the $x^{i}$ and the $z^{\lambda}$ satisfy the algebraic equations of system \eqref{eq:6.3}; such a point is a \emph{regular integral point}. We will be only concerned with the integral varieties of system \eqref{eq:6.3} whose generic points are regular. The other will be solutions of another system which can be deduced from the system \eqref{eq:6.3} by adjoining algebraic relations which express that the rank of the system $\theta_{\alpha}=0$ have a given value less than $r_{1}$.


\section{Prolongations of differential system}
\label{sec:prol-diff-syst}

\fsec The operation of the prolongation\index{prolongation} of a differential system is fundamentally identical to the operation consisting of adjoining to a given system of partial differential equations the equations that can be deduced by applying partial or total derivations with respect to one or more independent variables. Consider the system \eqref{eq:6.3} and form the general equations which give the $p$ dimensional integral elements
\begin{equation}
  \label{eq:6.4}
  \varpi^{\lambda}=t_{i}^{\lambda}dx^{i}\qquad(\lambda=1,2,\dots,\nu-r_{1}),
\end{equation}
these equations are
\begin{equation}
  \label{eq:6.5}
  H_{\alpha ij}\equiv C_{\alpha ij}+A_{\alpha i\lambda}t_{j}^{\lambda}-A_{\alpha j\lambda}t_{i}^{\lambda}=0,\qquad(\alpha=1,2,\dots,r_{2};\ i,j=1,2,\dots,p).
\end{equation}

We can regard $t_{i}^{\lambda}$ as new unknown functions subject to equations \eqref{eq:6.5}. We will have a prolongation of system \eqref{eq:6.3} by adjoining to them the algebraic equations \eqref{eq:6.5}, the linear equations \eqref{eq:6.4} and the equations following from \eqref{eq:6.5} and \eqref{eq:6.4} by exterior differentiation. Observe that in the new system thus obtained we can suppress the exterior quadratic equations $\varphi_{\alpha}=0$ which appear in the initial system \eqref{eq:6.3}, since they are algebraic consequences of equations \eqref{eq:6.4}.

We can also effect a partial prolongation by adjoining only some of the equations \eqref{eq:6.4}.

We can show that if the system \eqref{eq:6.3} is in involution, the system of its total prolongation obtained by the preceding procedure is involutive as well \footnote{\textsc{E.~Cartan}, \emph{Sur la structure des groupes infinis de transformations}, Chapter 1 (Annales \'Ecole normale sup., \textbf{21}, 1904, pp.~154--175, especially pp.~166--171, \textsection{}7--9).}. However we will have no need of this theorem. The theorem can be false if we only apply a partial prolongation.

\vspace{12pt}\fsec Suppose that the given system \eqref{eq:6.3} is not in involution. Several cases appear.

\vspace{12pt}\textsc{First case}: \emph{the equations \eqref{eq:6.5} that furnish the $p$ dimensional integral elements of a generic integral point are incompatible.} In this case, the compatibility of equations \eqref{eq:6.5} contain relations between the coordinates $x^{i},z^{\alpha}$ of the origin of the integral element.

If these equations contain relations between the independent variables, or if they contain as consequences that the rank of the system $\theta_{\alpha}=0$ is less than $r_{1}$, the problem does not admit any solution.

If none of these two kinds of relations results, we must adjoin to equations \eqref{eq:6.3} the algebraic relations between the dependent and independent variables expressing the compatibility of equations \eqref{eq:6.5} as well as the equations that result from exterior differentiation. The numbers $r_{0}$ and $r_{1}$ are hence augmented, but the quadratic equations $\varphi_{\alpha}=0$ do not change. We hence have a new system containing the same dependent and independent variables as the original system, with an increase of the integers $r_{0}$ and $r_{1}$: in particular \emph{the integer $\nu-r_{1}$ has decreased}.

\vspace{12pt}\textsc{Second case}: \emph{the equations \eqref{eq:6.5} are compatible at every regular integral point of the space, but the system is not in involution}. In this case we will prolong this system as explained in \textsection\textbf{109}, and we will obtain a new system containing new dependent variables. With respect to the old system, there will be an increase of the integer $r_{0}$ because we will be adding  the algebraic relations \eqref{eq:6.5} between the independent and dependent variables, relations expressing that the $p$ dimensional element \eqref{eq:6.4} is integral. The integer $r_{1}$ will also increase by the addition of the equations \eqref{eq:6.4} and the equations resulting from differentiation of \eqref{eq:6.5}. As for the quadratic equations of the new system, they will no longer contain the equations $\varphi_{\alpha}=0$ of the old system if we effect a complete prolongation of the system, but we must adjoin the equations resulting from exterior differentiation of the equations \eqref{eq:6.4}. If the prolongation is only partial, certain equations of $\varphi_{\alpha}=0$ may be preserved.


\vspace{12pt}\fsec From the preceding discussion we see that if the given system is not in involution, we have a systematic procedure to deduce new systems admitting the same solutions as the given system from it. We can show that, \emph{under certain conditions which is not easy to make precise at this moment},  we will arrive at a system in involution.\index{prolongation theorem (Cartan-Kuranishi)}

We will not stop to prove the general case and we are just going to show how we can prove the most simple case, the case of two independent variables. The proof that we are going to give does not extend to the case of arbitrary number of independent variables.


\section{The case of two independent variables}
\label{sec:case-two-independent-1}

\fsec We denote by $x$ and $y$ the independent variables. We continue to use our previous notations. We assume, for simplicity, that in the quadratic equations $\varphi_{\alpha}=0$, which we write as
\begin{equation}
  \label{eq:6.6}
  \varphi_{\alpha}\equiv C_{\alpha}[dx\,dy]+A_{\alpha\lambda}[dx\,\varpi^{\lambda}]+B[dy\,\varpi^{\lambda}]=0,
\end{equation}
we got rid of the forms $\theta_{\alpha}$, by means of adding to $\varphi_{\alpha}$ a quadratic form congruent to zero$\pmod{\theta_{1},\theta_{2},\dots,\theta_{r_{1}}}$. We denote by $\rho$ the difference of $\nu-r_{1}$ so that the forms $\varphi_{\alpha}$ contain only $\rho$ forms $\varpi^{1},\varpi^{2},\dots,\varpi^{\rho}$ independent among themselves and with respect to $\theta_{\alpha}$.

We are going to state how we will prolong the system when the equations \eqref{eq:6.5} are compatible and the system is not in involution. As we are going to indicate, we will only effect a partial prolongation.



\vspace{12pt}\fsec Recall the criterion for involution stated in \textsection\textbf{94} in the case where there passes at least a two dimensional integral element on a generic integral point. The necessary and sufficient condition for involution is that the reduced character $s_{1}$ is equal to the number of linearly independent quadratic forms $\varphi_{\alpha}$. In our case we can assume the coefficients $C_{\alpha}$ of the formulae \eqref{eq:6.6} to vanish. By substituting the differentials $dx,dy$ with two independent linear combinations $\omega^{1},\omega^{2}$ and then writing
\[
\varphi_{\alpha}\equiv A_{\alpha\lambda}[\omega^{1}\varpi^{\lambda}]+B_{\alpha\lambda}[\omega^{2}\varpi^{\lambda}],
\]
we can assume that the linear integral element $\omega^{2}=0$ is regular, such that $s_{1}$ is the number of independent forms $A_{\alpha\lambda}\varpi^{\lambda}$. We can, by a change of notation, set $A_{\alpha\lambda}\varpi^{\lambda}\equiv\varpi^{\alpha}$ such that
\begin{equation}
  \label{eq:6.7}
  \varphi_{\alpha}\equiv[\omega^{1}\varpi^{\alpha}]+B_{\alpha\lambda}[\omega^{2}\varpi^{\lambda}]\qquad(\alpha=1,2,\dots,s_{1}).
\end{equation}

If the system is not in involution, then there exists forms $\varphi_{\alpha}$ independent of the $s_{1}$ preceding forms, but the coefficients of $\omega_{1}$ in these forms are linear combinations of $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$. We can therefore suppose, for example, that the form $\varphi_{s_{1}+1}$ does not contain terms in $\omega^{1}$. \emph{We are going to show that the coefficient of $\omega^{2}$ is a linear combination of the $s_{1}$ forms $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$}.

Indeed, suppose for example we have
\[
\varphi_{s+1}\equiv[\omega^{2}\varpi^{s_{1}+1}],
\]
it is then easy to see that the first reduced character is greater that $s_{1}$, since the first $s+1$  reduced equations of the polar element of the linear integral element $(\omega^{1}=1,\omega^{2}=m)$ would be
\begin{align*}
  \varpi^{\alpha}+mB_{\alpha\lambda}\varpi^{\lambda}&=0&(\alpha=1,2,\dots,s_{1}),\\
  m\varpi^{s_{1}+1}&=0,
\end{align*}
but if we give $m$ a sufficiently small value the rank of this system is $s_{1}+1$, so that the first character is at least equal to $s_{1}+1$.



\vspace{12pt}\fsec Following the reasoning above, we can suppose, by a linear change of variables of the $s_{1}$ forms $\varphi_{1},\varphi_{2},\dots,\varphi_{s_{1}}$, that $\varphi_{s_{1}+1}$ is a non-zero multiple of $\varpi^{1}$, so that we can write
\begin{equation}
  \label{eq:6.8}
  \varphi_{s_{1}+1}\equiv[\omega^{2}\varpi^{1}].
\end{equation}

We are going to show that the coefficient of $\omega^{2}$ in $\varphi_{1}$ depends only on $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$. Indeed the reduced polar system of the linear integral element $(\omega^{1}=1,\omega^{2}=m)$ contains the equations
\begin{align*}
  \varpi^{1}+mB_{1\lambda}\varpi^{\lambda}&=0,\\
  \varpi^{\alpha}+mB_{\alpha\lambda}\varpi^{\lambda}&=0,&(\alpha=2,3,\dots,s_{1}),\\
  \varpi^{1}&=0,
\end{align*}
when $m$ tends to zero, the system tends to the system
\[
B_{1\lambda}\varpi^{\lambda}=0,\qquad\varpi^{2}=0,\qquad\dots\qquad\varpi^{s_{1}}=0,\quad\varpi^{1}=0,
\]
as the rank is equal to $s_{1}$, this implies that the form $B_{1\lambda}\varpi^{\lambda}$ can only contain terms in $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$.

We can assume, by means of a linear change of variables for $\varpi^{\alpha}$ $(\alpha\le s_{1})$, that the coefficient of $\omega^{2}$ in $\varphi_{1}$ is equal to $\varpi^{2}$:
\[
\varphi_{1}\equiv[\omega^{1}\varpi^{1}]+[\omega^{2}\varpi^{2}].
\]

We continue the argument. The coefficient of $\omega^{2}$ in $\varphi^{2}$ must be a linear combination of the $s_{1}$ forms $\varpi^{1},\varpi^{2},\dots,\varpi^{s_{1}}$. If this combination is independent of $\varpi^{1}$ and $\varpi^{2}$, we can suppose that it is equal to $\varpi^{3}$ and so on. Ultimately we will have the coefficient of $\omega^{2}$ in the successive forms $\varphi_{1},\varphi_{2},\dots$ depend only on previously encountered forms. We have, for example,
\begin{align*}
  \varphi_{1}&\equiv[\omega^{1}\varpi^{1}]+[\omega^{2}\varpi^{2}],\\
  \varphi_{2}&\equiv[\omega^{1}\varpi^{2}]+[\omega^{2}\varpi^{3}],\\
  &\dots\\
  \varphi_{h-1}&\equiv[\omega^{1}\varpi^{h-1}]+[\omega^{2}\varpi^{h}],\\
  \varphi_{h}&\equiv[\omega^{1}\varpi^{h}]+B_{1}[\omega^{2}\varpi^{1}]+B_{2}[\omega^{2}\varpi^{2}]+\dots+B_{h}[\omega^{2}\varpi^{h}],\\
  \varphi_{s_{1}+1}&\equiv[\omega^{1}\varpi^{1}].
\end{align*}

With this, we deduce that, for all two dimensional integral elements,
\begin{equation}
  \label{eq:6.9}
  \left\{
    \begin{aligned}
      \varpi^{1}&=t_{1}\omega^{2},\\
      \varpi^{2}&=t_{1}\omega^{1}+t_{2}\omega^{2},\\
      \varpi^{3}&=t_{2}\omega^{1}+t_{3}\omega^{2},\\
      &\dots\\
      \varpi^{h}&=t_{h-1}\omega^{1}+t_{h}\omega^{2},
    \end{aligned}
  \right.
\end{equation}
with the relation
\begin{equation}
  \label{eq:6.10}
  t_{h}=B_{2}t_{1}+B_{3}t_{2}+\dots+B_{h}t_{h-1}.
\end{equation}


\vspace{12pt}\fsec After we obtain this first result, let us effect a \emph{partial} prolongation of the given differential system, a prolongation that introduces $h-1$ new unknown functions $t_{1},t_{2},\dots,t_{h-1}$. We then need to adjoin to the linear differential equations $\theta_{\alpha}=0$ of the given system the new independent linear equations
\begin{equation}
  \label{eq:6.11}
  \left\{
    \begin{aligned}
      \varpi^{1}-t_{1}\omega^{2}&=0,\\
      \varpi^{2}-t_{1}\omega^{1}-t_{2}\omega^{2}&=0,\\
      \varpi^{3}-t_{2}\omega^{1}-t_{3}\omega^{2}&=0,\\
      &\dots,\\
      \varpi^{h}-t_{h-1}\omega^{1}-(b_{2}t_{1}+B_{3}t_{2}+\dots+B_{h}t_{h-1})\omega^{2}&=0.\\
    \end{aligned}
  \right.
\end{equation}

To the old quadratic equations, whose number is now reduced to $h$, we adjoin the $h$ quadratic equations resulting from the exterior differentiation of the equations \eqref{eq:6.11}. The fundamental result obtained by this prolongation is that \emph{the integer $\rho$ has decreased}: indeed the integer $r_{1}$ has increased by $h$, whereas the number $\nu$ of dependent variables has only increased by $h-1$, \emph{therefore the integer $\rho=\nu-r_{1}$ has effectively decreased by one.}

\vspace{12pt}\fsec The analysis in the previous section gives a systematic method for obtaining, starting from a differential system not in involution, a set of differential systems admitting the same solutions as the initial system. If at a given moment the system obtained is incompatible, the same can be said about the initial system; if the system obtained does not admit two dimensional integral elements having as its origin a generic point in the space of independent and dependent variables, we deduce from this a new system for which the integer $\rho$ has decreased; if the system obtained admits two dimensional integral elements having as its origin a generic point, but the system is not in involution, we deduce from this a new system for which the integer $\rho$ has again decreased. As the integer $\rho$ cannot decrease indefinitely, \emph{we will therefore arrive at an incompatible system or a system in involution at a certain stage.}
\begin{thm*}
  Every solution of a closed differential system of two independent variables can be regarded as arising from the general solution of a system in involution that can be formed in a systematic manner with finitely many steps of operation.
\end{thm*}



\vspace{12pt}\fsec\textsc{Remark.} In reality we have limited ourselves to the solution of the initial system \eqref{eq:6.3} for which the rank of the  system formed by the first order differential equations have its maximal value, and the same restriction has been implicitly made regarding the successive differential systems obtained. In particular if the initial differential system has singular solutions, these solutions have been left aside. If we want to give the theorem just stated in the previous section validity even in this case, we then need to focus our attention on \emph{a determined solution} of the initial system and, for each successive system, start by adjoining, if necessary, the relations between the dependent and independent variables expressing that the rank of the system of linear differential equations has a value corresponding to the solution considered. Unfortunately \emph{it is not obvious that the rank of the new system of linear differential equations would increase, which is to say that it is not clear that the integer $\rho$ would decrease}, even though the number of independent relations between the dependent and independent variables would increase. So strictly speaking, we can only affirm the theorem as proved above. Nevertheless the considerations of this section provide a practical method for obtaining all solutions of a given system, so that each solution is part of the general solution of a system in involution formed without prior integration.


\setcounter{frenchsec}{1}
\part{Applications to differential geometry}
\label{part:appl-diff-geom}

\chapter{Differential systems in the theory of surfaces}
\label{cha:diff-syst-theory}

\section[{Review of the principles of the theory of moving trihedrals}]{Review of the principles of\\the theory of moving trihedrals}
\label{sec:revi-princ-theory}\index{moving trihedrals}

\fsec Consider in ordinary space a family $\mathcal{F}$ of right trihedrals depending on a certain number of parameters. We denote by $A$ the origin of these trihedrals and by $\vec{e^{1}},\vec{e^{2}},\vec{e^{3}}$ the unit vectors along the axes. The infinitesimal displacement which leads the trihedral $\mathcal{T}$ in the family to an infinitesimally near trihedral $\mathcal{T}'$ is defined if we know the infinitesimal vectors $d\vec{A},d\vec{e^{1}},d\vec{e^{2}},d\vec{e^{3}}$. The decomposition of these vectors by projection on the axes of $\mathcal{T}$ give the relations
\begin{equation}
  \label{eq:7.1}
  dA=\omega^{i}\vec e_{i},\qquad d\vec e_{i}=\omega_{i}{}^{j}\vec e_{j},
\end{equation}
where the $\omega^{i}$ and the $\omega_{i}^{j}$ are linear differential forms with respect to the differentials of the parameters of the family $\mathcal{F}$. They are the \emph{relative components} of infinitesimal displacement of the trihedral. They are in general not independent, since the vectors $\vec e_{i}$ are subject to the relations
\begin{gather*}
  (\vec e_{1})^{2}=(\vec e_{2})^{2}=(\vec e_{3})^{2}=1,\\
  \vec e_{2}\cdot\vec e_{3}=\vec e_{3}\cdot\vec e_{1}=\vec e_{1}\cdot \vec e_{2}=0.
\end{gather*}
These relations, when differentiated, give
\begin{gather*}
  \omega_{1}{}^{1}=\omega_{2}{}^{2}=\omega_{3}{}^{3}=0,\\
  \omega_{2}{}^{3}+\omega_{3}{}^{2}=\omega_{3}{}^{1}+\omega_{1}{}^{3}=\omega_{1}{}^{2}+\omega_{2}{}^{1}=0.
\end{gather*}

Henceforth we will write $\omega_{i}{}^{j}$ or $\omega_{ij}$ indifferently. Besides the three forms $\omega^{1},\omega^{2},\omega^{3}$ which determine the components of the axes of the trihedral $\mathcal{T}$ which make $A$ and $A+d\vec A$ coincident, there exists three other forms
\[
\omega_{23}=-\omega_{32},\qquad \omega_{31}=-\omega_{13},\qquad \omega_{12}=-\omega_{21}
\]
which define the components of rotation which make the trihedral $\mathcal{T}$ to be equipollent to an infinitesimally near one.

 


\vspace{12pt}\fsec The six forms $\omega^{1}, \omega^{2}, \omega^{3},\omega_{23},\omega_{31},\omega_{12}$ satisfy the relations systematically utilised by Darboux in the theory of movement depending on two parameters. These relations results from exterior differentiation of the relations \eqref{eq:7.1}. We have
\begin{align*}
  d\omega^{i}e_{i}-[\omega^{i}d\vec e_{i}]&=0,&\text{or}&&d\omega^{i}-[\omega^{k}\omega_{k}{}^{i}]=0,\\
  d\omega_{i}{}^{j}e_{j}-[\omega_{i}{}^{j}d\vec e_{j}]&=0,&\text{or}&&d\omega_{i}{}^{j}-[\omega_{i}{}^{k}\omega_{k}{}^{j}]=0,
\end{align*}
from which we have the \emph{structural equations}\index{structural equations}
\begin{equation}
  \label{eq:7.2}
  \left\{
    \begin{aligned}
      d\omega^{i}&=[\omega^{k}\omega_{ki}]=[\omega_{ik}\omega^{k}],\\
      d\omega_{i}{}^{j}&=[\omega_{i}{}^{k}\omega_{k}{}^{j}].
    \end{aligned}
  \right.
\end{equation}

The last of equation \eqref{eq:7.2} can again be written
\begin{equation}
  \label{eq:7.2'}\tag{2$'$}
  d\omega_{ij}=-[\omega_{i}{}^{k}\omega_{jk}].
\end{equation}


\vspace{12pt}\fsec Conversely suppose we are given six differential forms $\omega^{i},\omega_{ij}=-\omega_{ji}$ constructed from $q$ variables $u^{k}$ and their differentials which satisfy equations \eqref{eq:7.2}. Then there exists a family of right trihedrals depending on the $q$ parameters $u^{1},u^{2},\dots,u^{q}$ such that the forms $\omega^{i},\omega_{ij}$ are the relative components of their infinitesimal displacement. Indeed consider the most general family possible of right trihedrals, depending on six parameters $\nu^{1},\nu^{2},\dots,\nu^{6}$, and let $\bar \omega^{i}(\nu,d\nu),\bar\omega_{ij}(\nu,d\nu)$ be the relative components corresponding to their infinitesimal displacement. The equations
\begin{equation}
  \label{eq:7.3}
  \left\{
    \begin{aligned}
    \bar\omega^{i}(\nu,d\nu)&=\omega^{i}(u,du)\\
    \bar\omega_{ij}(\nu,d\nu)&=\omega_{ij}(u,du)    
    \end{aligned}
  \right.
\end{equation}
where the $\nu^{i}$ are regarded as unknown functions of the variables $u^{1},u^{2},\dots,u^{q}$, constitute a completely integrable system, since exterior differentiation applied to this equations gives, by virtue of relations \eqref{eq:7.2} which holds in this case by the hypothesis for the forms $\bar\omega^{i},\bar\omega_{ij}$ as well as for the forms $\omega^{i},\omega_{ij}$, relations which are consequences of \eqref{eq:7.3}. We can therefore make a correspondance to each system of values of $u^{i}$ one and only one right trihedral of parameter $\nu^{i}$, if we impose the condition that at some given values $(u^{i})_{0}$ the corresponding trihedral is of the given parameters $(\nu^{i})_{0}$. It is clear that all trihedrals in the family we are searching for are deduced from one of them by arbitrary displacement, with or without a symmetry (a real displacement if we fix the orientation of the trihedrals).

\section[{Fundamental theorems in the theory of surfaces}]{Fundamental theorems\\in the theory of surfaces}
\label{sec:fund-theor-theory}

\fsec To a given surface $S$ we can attach a family of right trihedrals whose origin is on the surface and whose  vector $\vec e_{3}$ is normal to the surface. This family will depend on two parameters if we attach at each point $A$ of the surface a trihedral $\mathcal{T}$ following specific rule, for example by taking $\vec e_{1}$ and $\vec e_{2}$ to be the unit vectors along the principal tangent\index{principal tangent} directions of $A$ \footnote{We call these trihedrals the \emph{Darboux trihedrals}\index{Darboux trihedrals}.},  in which case we must assume that the region of the surface under consideration does not contain umbilical points\index{umbilical point}. The family would depend on three parameters if we attach at each point all the rectangular frames whose vector $\vec e_{3}$ is normal to the surface $A$.

In each of these cases, the vector $d \vec A$ being tangent to the surface, we have
\begin{equation}
  \label{eq:7.4}
  \omega^{3}=0,
\end{equation}
from which, by virtue of the structural equations \eqref{eq:7.2} and the expression of $d\omega^{3}$,
\begin{equation}
  \label{eq:7.5}
  [\omega^{1}\omega_{13}]+[\omega^{2}\omega_{23}]=0.
\end{equation}

Conversely every time when we have a family of right trihedrals such that the form $\omega^{3}$ vanishes identically, the origin $A$ of these trihedrals will describe a surface that the vector $\vec e_{3}$ is normal to. Indeed, the equations $\omega^{1}=\omega^{2}=0$ form a completely integrable system, by virtue of the structural equations
\[
d\omega^{1}=[\omega^{2}\omega_{21}],\qquad d\omega^{2}=[\omega^{1}\omega_{12}].
\]
Let $u$ and $v$ be two independent first integrals of this system. The equation
\[
dA=\omega^{1}\vec e_{1}+\omega^{2}\vec e_{2}
\]
shows that the point $A$ does not depend on $u$ or $v$. The point $A$ therefore describes a surface whose tangent plane contains each of the vectors $\vec e_{1}$ and $\vec e_{2}$ and hence normal to $\vec e_{3}$.

We have implicitly assumed that the forms $\omega^{1}$ and $\omega^{2}$ are linearly independent, otherwise the point $A$ would describe a line and not a surface.




\vspace{12pt}\fsec Consider a curved traced on the surface. Let us define a positive direction on this curve and denote by $\vec T, \vec N, \vec B$ the unit vectors of the Frenet trihedral\index{Frenet trihedrals} attached at a point $A$ of the curve. Let $\theta$ be the angle $(\vec e_{1},\vec T)$, the positive direction in the tangent place being that which rotates $\vec e_{1}$ to $\vec e_{2}$ by a rotation of $+\pi/2$ degrees. Let $\vec\varepsilon$ be the unit vector obtained from $T$ by a rotation of $+\pi/2$ degrees in the tangent plane. Finally let $\varpi$ be the angle $(\vec N,\vec e_{3})$, the positive rotation in the plane perpendicular to $\vec T$ being that which rotates $\vec \varepsilon$ to $\vec e_{3}$ by $+\pi/2$ degrees. We have
\begin{equation}
  \label{eq:7.6}
  \left\{
    \begin{aligned}
      \vec T&=+\vec e_{1}\cos\theta+\vec e_{2}\sin\theta,\\
      \vec\varepsilon&=-\vec e_{1}\sin\theta+\vec e_{2}\cos\theta,\\
      \vec N&=+\vec\varepsilon\sin\varpi+\vec e_{3}\cos\varpi,\\
      \vec B&=-\vec\varepsilon\cos\varpi+\vec e_{3}\sin\varpi.
    \end{aligned}
  \right.
\end{equation}

Finally, recall Frenet's formulae
\begin{equation}
  \label{eq:7.7}
  \left\{
    \begin{aligned}
      d\vec T&=\frac{ds}{\rho}\vec N,\\
      d\vec N&=-\frac{ds}{\rho}\vec T+\frac{ds}{\tau}\vec B,\\
      d\vec B&=-\frac{ds}{\tau}\vec N,
    \end{aligned}
  \right.
\end{equation}
where we have denoted the arc element\index{arc element} as $ds$, the curvature\index{curvature of spatial curve} and torsion\index{torsion of spatial curve} as $1/\rho$ and $1/\tau$.

The differentiation of the first equation of \eqref{eq:7.6} gives
\[
\frac{ds}{\rho}\vec N=(d\theta+\omega_{12})\vec\varepsilon+(\omega_{13}\cos\theta+\omega_{23}\sin\theta)\vec e_{3},
\]
the coefficient of $\vec \varepsilon$ is the projection onto the tangent plane of the vector $ds/\rho$ in the principal normal direction, and the coefficient of $\vec e_{3}$ is the projection onto the normal of the surface. We deduce from these
\begin{equation}
  \label{eq:7.8}
  \left\{
    \begin{aligned}
      d\theta+\omega_{12}&=\frac{ds\sin\varpi}{\rho}=\frac{ds}{R_{g}},\\
      \omega_{13}\cos\theta+\omega_{23}\sin\theta&=\frac{ds\cos\varpi}{\rho}=\frac{ds}{R_{n}},
    \end{aligned}
  \right.
\end{equation}
where $1/R_{g}$ and $1/R_{n}$ and the \emph{geodesic curvature}\index{geodesic curvature} and the \emph{normal curvature}\index{normal curvature}. We have
\[
\frac{ds^{2}}{R_{n}}=\omega_{13}\cos\theta\,ds+\omega_{23}\sin\theta\,ds=\omega^{1}\omega_{13}+\omega^{2}\omega_{23},
\]
the form $\omega^{1}\omega_{13}+\omega^{2}\omega_{23}$ is the \emph{second fundamental form}\index{second fundamental form} $\Phi$ of Gauss, it is also equal to
\begin{align*}
-d\vec e_{3}\cdot d\vec A&=-(\omega_{31}\vec e_{1}+\omega_{32}\vec e_{2})(\omega^{1}\vec e_{1}+\omega^{2}\vec e_{2})\\
&=(\omega_{13}\vec e_{1}+\omega_{23}\vec e_{2})(\omega^{1}\vec e_{1}+\omega^{2}\vec e_{2}).
\end{align*}

We therefore have
\begin{equation}
  \label{eq:7.9}
  \Phi=\omega^{1}\omega_{13}+\omega^{2}\omega_{23}=\frac{ds^{2}}{R_{n}}.
\end{equation}

The first fundamental form\index{first fundamental form} $F$ is the $ds^{2}$ of the surface:
\begin{equation}
  \label{eq:7.10}
  F=(\omega^{1})^{2}+(\omega^{2})^{2}=ds^{2}.
\end{equation}

A third form also plays an important role. Let us derive the equation
\[
\vec e_{3}=\vec N\cos\varpi+\vec B\sin\varpi
\]
which can be easily deduced from the equations \eqref{eq:7.6}. We have, by taking into consideration \eqref{eq:7.6} and \eqref{eq:7.7},
\begin{align*}
  d\vec {e^{3}}&=\omega_{3}{}^{1}\vec e_{1}+\omega_{3}{}^{2}\vec e_{2}=-d\varpi\,\vec \varepsilon-ds\frac{\cos\varpi}{\rho}\vec T-\frac{ds}{\tau}\vec\varepsilon\\
  &=-\frac{ds}{R_{n}}\vec T-\left(d\varpi+\frac{ds}{\tau}\right)\vec \varepsilon,
\end{align*}
from which, by projecting to $\vec T$ and $\vec \varepsilon$,
\begin{align*}
  -\vec T\cdot d\vec e_{3}&=\frac{ds}{R_{n}}=\omega_{13}\cos\theta+\omega_{23}\sin\theta,\\
  -\vec\varepsilon\cdot d\vec e_{3}&=d\varpi+\frac{ds}{\tau}=-\omega_{13}\sin\theta+\omega_{23}\cos\theta.
\end{align*}

We find that the expression of $ds/R_{n}$ is already provided by the second formula of \eqref{eq:7.8}. As for the quantity $d\varpi/ds+1/\tau$, it is the \emph{geodesic torsion}\index{geodesic torsion} $1/T_{g}$ and we have, by replacing $\cos\theta$ by $\omega^{1}/ds$ and $\sin\theta$ by $\omega^{2}/ds$, the relation
\begin{equation}
  \label{eq:7.11}
  \frac{ds^{2}}{T_{g}}=\left(\frac{d\varpi}{ds}+\frac{1}{\tau}\right)ds^{2}=\omega^{1}\omega_{23}-\omega^{2}\omega_{13}.
\end{equation}

The form
\begin{equation}
  \label{eq:24}
  \Psi=\omega^{1}\omega_{23}-\omega^{2}\omega_{13}
\end{equation}
is the \emph{third fundamental form}\index{third fundamental form} of the surface.

Observe that the second fundamental form depends on a positive direction chosen on the normal of the surface, but not on the orientation of the trihedral. The third fundamental form on the contrary changes sign depending on the orientation of the trihedral, but it does not depend on the positive direction chosen for the normal.

\vspace{12pt}\fsec The formulae of the previous section are valid even if we attach at each point of the surface an infinity of right trihedrals subject to the condition that the vector $\vec e_{3}$ is normal to the surface. Now suppose we attach at each point $A$ of $S$ a determined trihedral. The relation \eqref{eq:7.5} can be written
\begin{equation}
  \label{eq:7.13}
  \left\{
    \begin{aligned}
      \omega_{13}&=a\omega^{1}+b\omega^{2},\\
      \omega_{23}&=b\omega^{1}+c\omega^{2}.
    \end{aligned}
  \right.
\end{equation}

\label{pageref1}
The three fundamental forms are then
\begin{equation}
  \label{eq:7.14}
  \left\{
    \begin{aligned}
      F&=(\omega^{1})^{2}+(\omega^{2})^{2},\\
      \Phi&=a(\omega^{1})^{2}+2b\omega^{1}\omega^{2}+c(\omega^{2})^{2},\\
      \Psi&=b(\omega^{1})^{2}+(c-a)\omega^{1}\omega^{2}-b(\omega^{2})^{2}.
    \end{aligned}
  \right.
\end{equation}

We see that the form $\Psi$ is the Jacobian of the forms $F$ and $\Phi$, i.e., the determinant of half of the partial derivatives of these forms with respect to $\omega^{1}$ and $\omega^{2}$.

The curvature lines\index{curvature line} are given by the equation $\Psi=0$, the asymptotic lines\index{asymptotic line} by the equation $\Phi=0$. As for the principal curvatures, they are given by the system of equations
\[
\frac{a\omega^{1}+b\omega^{2}}{\omega^{1}}=\frac{b\omega^{1}+c\omega^{2}}{\omega^{2}}=\frac{1}{R},
\]
from which we extract the second degree equation in $1/R$
\begin{equation}
  \label{eq:7.15}
  \left(a-\frac{1}{R}\right)
  \left(c-\frac{1}{R}\right)
  -b^{2}=0,
\end{equation}
we therefore have
\begin{equation}
  \label{eq:7.15'}
  \tag{15$'$}
  \frac{1}{R_{1}}+\frac{1}{R_{2}}=a+c,\qquad \frac{1}{R_{1}R_{2}}=ac-b^{2}.
\end{equation}

In the case where the vectors $\vec e_{1}$ and $\vec e_{2}$ are given by the principal tangents, we have $1/R_{1}=a$, $1/R_{2}=c$, $b=0$, and
\begin{equation}
  \label{eq:7.16}
  \left\{
    \begin{aligned}
      \Phi&=\frac{1}{R_{1}}(\omega^{1})^{2}+\frac{1}{R_{2}}(\omega^{2})^{2},\\
      \Psi&=\left(\frac{1}{R_{2}}-\frac{1}{R_{1}}\right)\omega^{1}\omega^{2}.
    \end{aligned}
  \right.
\end{equation}

If $\theta$ is the angle between the first principal tangent and the positive tangent of a oriented curve, we have for this curve
\begin{equation}
  \label{eq:7.17}
  \frac{1}{R_{n}}=\frac{\cos^{2}\theta}{R_{1}}+\frac{\sin^{2}\theta}{R_{2}},\qquad\frac{1}{T_{g}}=\left(\frac{1}{R_{2}}-\frac{1}{R_{1}}\right)\sin\theta\cos\theta,\qquad\frac{1}{R_{g}}=\frac{d\theta+\omega_{12}}{ds}.
\end{equation}

Finally, note that if we take the vector $\vec e_{1}$ to be the unit vector tangent to a given curve $C$ traced on the surface, we have for this curve at each point
\begin{equation}
  \label{eq:7.18}
  \frac{1}{R_{n}}=a,\qquad\frac{1}{T_{g}}=b,\qquad\frac{1}{R_{g}}=\frac{d\theta+\omega_{12}}{ds}.
\end{equation}
It follows in particular that if $C$ is an asymptotic line of the surface we have at each point $a=0$, from which, according to equation \eqref{eq:7.15'},
\[
b^{2}=-\frac{1}{R_{1}R_{2}},
\]
it follows that the torsion of the curve is equal to $\pm\sqrt{-1/R_{1}R_{2}}$ (Enneper's theorem).


\vspace{12pt}\fsec Now we are going to discuss different problems in the classical theory of surfaces. These problems mainly focus on finding surfaces having certain properties or finding a couple of surfaces admitting a point correspondance and having given properties. We will reduce the problems in the first category to finding a family of right trihedrals attached to different points on the surface we are searching for whose vector $\vec e_{3}$ is normal to the surface. In many cases this will amount to attach to each point the \emph{Darboux trihedral}\index{Darboux trihedrals} where the vectors $\vec e_{2},\vec e_{3}$ are given by the principal tangents. It is true that this method has the disadvantage that we have to restrict ourselves to regions of the surface where there are no umbilical points, since at umbilical points the Darboux trihedrals are not determined. However, it will often be of interest to attach at each point of the surface all right trihedrals whose vector $\vec e_{3}$ is normal to the surface, and this would in turn suppress the restriction we would have in the questions. It is true that this method of proceeding looks as if we are introducing unknown parameters, but, as we will see, this is only an illusion. For the problems in the second category, we will also reduce the problems to finding a family of right trihedrals attached to the two surfaces that we are searching for, each trihedral attached to a point $A$ in the first surface corresponding to a determined trihedral attached to a corresponding point $A'$ on the second surface. The point correspondance  between the two surfaces establishes a determined correspondance between the tangents $AT$ and $A'T'$ of the two corresponding points $A$ and $A'$.

\subsection{Surfaces containing only umbilical points}
\label{sec:surfaces-where-all}

\fsec The second fundamental form being proportional to the first, we have, in the equations \eqref{eq:7.13} (\textsection\textbf{6}),
\[
a=c,\qquad b=0.
\]

The family of right trihedrals attached at different points on the surface can therefore be regarded as constituting an integral variety of the equations
\[
\omega^{3}=0,\qquad \omega_{13}=a\omega^{1},\qquad \omega_{23}=a\omega^{2},
\]
which, with its closure by exterior differentiation, furnish the system
\[
\omega^{3}=0,\qquad \omega_{13}=a\omega^{1},\qquad \omega_{23}=a \omega^{2},\qquad [\omega^{1}da]=0,\qquad [\omega^{2}da]=0.
\]

The system is not in involution. It entails $da=0$, from which we obtain the new system
\begin{equation}
  \label{eq:7.I.1}
  \tag{I, 1}
  \omega^{3}=0,\qquad \omega_{13}= a\omega^{1},\qquad \omega_{23}=a\omega^{2},\qquad da=0,
\end{equation}
which we easily check to be completely integrable, since exterior differentiation introduces no new relations.

If the constant $a$ is zero, we see that $d\vec e_{3}=0$, and the normal of the surface is in a fixed direction: the surface is a plane. If the constant $a$ is non-zero, the point $P=A+(1/a)\vec e_{3}$ is fixed, since $d\vec P=(\omega^{1}+(1/a)\omega_{31})\vec e_{1}+(\omega^{2}+(1/a)\omega_{32})\vec e_{2}=0$: we have a sphere of centre $P$ and radius $1/a$. All the surfaces we are looking for are obtained in these ways and they depend on four arbitrary constants.

But the family of trihedrals that we have taken as unknown auxiliary variables depends not only on arbitrary constants, since at each point on the sphere which constitute the family of the surfaces we are looking for, we can arbitrarily choose right trihedrals subject to the condition that the vector $\vec e_{3}$ is normal to the surface. These trihedrals therefore depend on one arbitrary function of two variables. But this function is insignificant and does not affect the initial problem. \emph{This is evident from the differential system \eqref{eq:7.I.1}, which expresses the conditions of the problem.} Indeed the system does not involve six forms $\omega^{1},\omega^{2},\omega^{3},\omega_{13},\omega_{23},\omega_{12}$: it involves only the five forms $\omega^{3},\omega^{1},\omega^{2},\omega_{13},\omega_{23}$. By equating to zero these five forms we obtain a completely integrable system [\emph{it is the characteristic system of \eqref{eq:7.I.1}}] whose solution depends on $5$ arbitrary constants $u_{1},u_{2},\dots, u_{5}$, each particular solution defining a family of parameters of right trihedrals. The geometrical significance of one such family is easy to obtain, since if we are inside such a family, the forms $\omega^{3},\omega^{1},\omega^{2},\omega_{13},\omega_{23}$ stay zero, the origin $A$ of the trihedral stays fixed $(\omega^{1}=\omega^{2}=\omega^{3}=0)$ and the vector $\vec e_{3}$ stays fixed as well $(\omega_{31}=\omega_{32}=0)$. The family is therefore formed by the trihedrals having a given origin $A$ and a given $\vec e_{3}$. It is geometrically equivalent to a \emph{contact element}\index{contact element} of Lie (a point and a plane passing though the point). The closed differential system \eqref{eq:7.I.1} which involves the dependent and independent variables $u_{1},u_{2},u_{3},u_{4},u_{5},a$, therefore simply expresses a property of the contact elements of the surface we are looking for, a property that characterises precisely contact elements of a surface whose points are all umbilical, and the differential system we arrived at \emph{involves only the components $\omega^{i},\omega_{ij}$ that play an effective role in the problem stated.}



\subsection[{Establishing a point correspondence between two given surfaces}]{Establishing a point correspondence\\between two given surfaces}
\label{sec:establ-point-corr}

\fsec Let $S$ and $\bar S$ be two given surfaces: we establish a point correspondance between these two surfaces such that between their linear elements $ds$ and $d\bar s^{2}$ there exists a relation of the form
\[
d\bar s^{2}=u^{2}ds^{2}.
\]

Let us attach at each point of these two surfaces the most general right trihedrals having the point as its origin and the vector $\vec e_{3}$ normal to the surface. We have the relation
\[
(\bar \omega^{1})^{2}+(\bar \omega^{2})^{2}=u^{2}[(\omega^{1})^{2}+(\omega^{2})^{2}],
\]
from which
\begin{align*}
  \bar\omega^{1}&=u(\omega^{1}\cos\theta+\omega^{2}\sin\theta),\\
  \bar\omega^{2}&=u(-\omega^{1}\sin\theta+\omega^{2}\cos\theta),
\end{align*}
or
\begin{align*}
  \bar\omega^{1}&=u(\omega^{1}\cos\theta+\omega^{2}\sin\theta),\\
  \bar\omega^{2}&=u(\omega^{1}\sin\theta-\omega^{2}\cos\theta).
\end{align*}

We can always assume that we are in the first case, since if we are in the second case, we could replace the vectors $\vec e_{2}$ and $\vec e_{3}$ of the trihedral attached at the second surface by $-\vec e_{2}$ and $-\vec e_{3}$, which will give a new right trihedral which satisfies the first class of formulae.

By turning the trihedral by the angle $\theta$ around its third axis, we will then finally have corresponding to each trihedral attach at the first surface a determined trihedral attached as the second surface such that there we have the relation
\begin{equation}
  \label{eq:7.II.1}
  \tag{II, 1}
  \bar\omega^{1}=u\omega^{1},\qquad\bar\omega^{2}=u\omega^{2}.
\end{equation}

The system is closed by exterior differentiation and gives the new system
\begin{equation}
  \label{eq:7.II.2}
  \tag{II, 2}
  \left\{
    \begin{aligned}
      \bar\omega^{1}=u\omega^{1},\quad\bar\omega^{2}=u\omega^{2},\\
      [\omega^{1}du]-u[\omega^{2}(\bar\omega_{12}-\omega_{12})]&=0,\\
      [\omega^{2}du]+u[\omega^{1}(\bar\omega_{12}-\omega_{12})]&=0.
    \end{aligned}
  \right.
\end{equation}

The forms $\omega^{1}$ and $\omega^{2}$ are linear combinations of the differentials of the parameters that the position of the present point on the surface $S$ depends on. The number of unknown functions is equal to four, i.e., the two parameters that depend on the position of the point $\bar A$ of $\bar S$ which correspond to the point $A$ of $S$, the ratio of local similitude $u$ of $\bar S$ with respect to $S$ in a neighbourhood of $A$, and finally the position in the tangent plane of $\bar A$ in $\bar S$ which corresponds to a given tangent at $A$ of $S$. In the system \eqref{eq:7.II.2} there appears effectively $4$ distinct forms in $\omega^{1},\omega^{2}$, i.e., $\bar\omega^{1},\bar\omega^{2},du,\bar\omega_{12}-\omega_{12}$. We can see that to annihilate these four forms, we need to fix the point $\bar A$ and the function $u$, and also require that the trihedral corresponding to the origin $A$ and $\bar A$ form the same angle with respect to their third axis, that is to say that the vectors $\vec e_{1}$ of the two trihedrals correspond to each other constantly.

The polar matrix (\textsection\textbf{93}) of the system \eqref{eq:7.II.2} is, supposing that the columns corresponding respectively to the forms $du$ and $\bar\omega_{12}-\omega_{12}$,
\begin{equation}
  \label{eq:7.II.3}
  \tag{II, 3}
  \begin{bmatrix}
    \omega^{1}&-u\omega^{2}\\
    \omega^{2}&u\omega^{1}
  \end{bmatrix}=u[(\omega^{1})^{2}+(\omega^{2})^{2}].
\end{equation}

Its rank $s_{1}=2$ is equal to the number of quadratic forms that appear in the equations \eqref{eq:7.II.2}. The system is therefore in involution (\textsection\textbf{94}) and its general solution depend on two arbitrary functions of one argument. The function $u$ being essentially non-zero, there is no singular solution. There is no real characteristic either.



\vspace{12pt}\fsec\emph{The Cauchy problem} Every solution is completely determined by a one dimensional solution to the equations
\begin{equation}
  \tag{II, 1}
  \bar\omega^{1}=u\omega^{1},\qquad\bar\omega^{2}=u\omega^{2}.
\end{equation}
We specify an arbitrary curve $C$ on the surface $S$ and an arbitrary curve $\bar C$ on the surface $\bar S$. Given our arbitrary choice of trihedrals attached at each point of $S$, we take on every point of $C$ the vector $\vec e_{1}$ tangent to $C$ so that we have $\omega^{2}=0$, and we also take the vector $\vec e_{1}$ tangent at each point of $\bar C$, but the vector $\vec e_{2}$ can be chosen in two different manners, each of which corresponds to a determined choice of $\vec e_{3}$. After we do this we establish an arbitrary (analytic) point correspondance between $C$ and $\bar C$, which furnish the function $u=d\bar s/ds$. Each of these one dimensional solutions of the system \eqref{eq:7.II.2} will therefore furnish, according to the general theory, a determined conformal representation of $\bar S$ on $S$.

Analytically, the problem is easy to solve. As the two surfaces are assumed to be analytic, we can find on each of them a system of curvilinear coordinates: $x,y$ for $S$, $\bar x,\bar y$ for $\bar S$,  such that we have
\[
ds^{2}=A^{2}(dx^{2}+dy^{2}),\qquad d\bar s^{2}=\bar A^{2}(d\bar x^{2}+d\bar y^{2}).
\]
Every conformal representation is obtained by taking $\bar x+i\bar y$ to be an analytic function of $x+iy$, or an analytic function of $x-iy$. The analytic curve $C$ is on the other hand defined by taking $x+iy$ to be an analytic function $f(t)$ of one real parameter $t$. If we attribute to each point of $\bar C$ the same parameter as on the corresponding point of $t$, we will have
\[
\bar x+i\bar y=\bar f(t),
\]
$\bar f(t)$ being a certain analytic function of $t$. By eliminating $t$ we will have an analytic relation between the two complex variables $x+iy, \bar x+i\bar y$, and this analytic relation will define one of the conformal representation we are looking for. The other is obtained by eliminating $t$ from the relations $\bar x+i\bar y=\bar f(t)$ and the equation expressing $x-iy$ as a function of $t$.





\subsection{Weingarten surfaces}
\label{sec:weingarten-surfaces}

\fsec Weingarten surfaces are those that have a given relation between the principal curvatures. More symmetrically we can define such a surface by an (analytic) relation between the mean curvature $1/R_{1}+1/R_{2}$ and the total curvature $1/R_{1}R_{2}$.

If we attach at each point of a surface $S$ an arbitrary right trihedral subject only to the condition that the vector $\vec e_{3}$ is normal to the surface, the differential system that describes the problem is furnished by the relations
\begin{equation}
  \label{eq:7.III.1}
  \tag{III, 1}
  \left\{
    \begin{aligned}
      \omega^{3}=0,\quad\omega_{13}=a\omega^{1}+b\omega^{2},&\quad\omega_{23}=b\omega^{1}+c\omega^{2},\\
      F(a+c,ac-b^{2})&=0,\\
      F_{a}da+F_{b}db+F_{c}dc&=0.
    \end{aligned}
  \right.
\end{equation}
The system is closed by adjoining the exterior quadratic equations
\begin{equation}
  \label{eq:7.III.2}
  \tag{III, 2}
  \left\{
    \begin{aligned}{}
      [\omega^{1}(da-2b\omega_{12})]+[\omega^{2}(db+\overline{a-c}\,\omega_{12})]&=0\\
      [\omega^{1}(db+\overline{a-c}\,\omega_{12})]+[\omega^{2}(dc+2b\omega_{12})]&=0.
    \end{aligned}
  \right.
\end{equation}

We can easily verify that the last equation of \eqref{eq:7.III.1} can be written as
\begin{equation}
  \label{eq:7.III.3}
  \tag{III, 3}
  F_{a}(da-2b\omega_{12})+F_{b}(db+\overline{a-c}\,\omega_{12})+F_{c}(dc+2b\omega_{12})=0.
\end{equation}

The governing equations of the system hence involve the forms $\omega^{1},\omega^{2}$, which are independent linear combinations of  differentials of the curvilinear coordinates on point on the surface, and the six independent forms $\omega^{3},\omega_{13},\omega_{23},da-2b\omega_{12},db+(a-c)\omega_{12},dc+2b\omega_{12}$, among the last three the relation \eqref{eq:7.III.3} holds. There are then $5$ unknown functions defining the contact element of $S$ corresponding to a system of given values of the curvilinear coordinates.

Here  the polar matrix, whose columns correspond to the three forms $da-2b\omega_{12},db+\overline{a-c}\,\omega_{12},dc+2b\omega_{12}$, is
\[
\begin{pmatrix}
  \omega^{1}&\omega^{2}&0\\
  0&\omega^{1}&\omega^{2}\\
  F_{a}&F_{b}&F_{c}
\end{pmatrix}
\]
we have $s_{1}=2$, the characteristic being defined by the equation
\[
F_{c}(\omega^{1})^{2}-F_{b}\omega^{1}\omega^{2}+F_{a}(\omega^{2})^{2}=0,
\]
or again, by setting $a+c=u, ac-b^{2}=v$,
\begin{equation}
  \label{eq:7.III.4}
  \tag{III, 4}
  F_{u}[(\omega^{1})^{2}+(\omega^{2})^{2}]+F_{v}[a(\omega^{1})^{2}+2b\omega^{1}\omega^{2}+c(\omega^{2})^{2}]=0.
\end{equation}

We see that the characteristic tangents on each integral surface belong to the involution defined by the asymptotic tangents and the minimal tangents.

The generic two dimensional integral element is given by the relations
\begin{equation}
  \label{eq:7.III.5}\tag{III, 5}
  \left\{
    \begin{aligned}
      da-2b\omega_{12}&=\alpha\omega^{1}+\beta\omega^{2},\\
      db+(a-c)\omega_{12}&=\beta\omega^{1}+\gamma\omega^{2},\\
      dc+2b\omega_{12}&=\gamma\omega^{1}+\delta\omega^{2},
    \end{aligned}
  \right.
\end{equation}
with
\begin{align*}
  \alpha F_{a}+\beta F_{b}+\gamma F_{c}&=0,\\
  \beta F_{a}+\gamma F_{b}+\delta F_{c}&=0.
\end{align*}



\vspace{12pt}\fsec\emph{The Cauchy problem}. Every non characteristic one dimensional solution of the equations \eqref{eq:7.III.1} furnishes a unique Weingarten surface in a given class. Such a solution is defined by a one parameter family of right trihedrals, each of which is associated with the three numbers $a, b, c$. We obtain them by taking a curve $C$, at each point of which we attach, following an arbitrary analytic rule, a right trihedral whose vector $\vec e_{1}$ is tangent to $C$ (this is possible because of the indetermination of the trihedral attached at each  point of the surface we are looking for). We will then have, according to the formulae \eqref{eq:7.18} of \textsection\textbf{6},
\[
a=\frac{1}{R_{n}}=\frac{\cos\varpi}{\rho},\qquad b=\frac{1}{T_{g}}=\frac{d\varpi}{ds}+\frac{1}{\tau},
\]
$\varpi$ has the meaning given to it before (\textsection\textbf{5}). As for $c$, it is given by the equation
\[
F(a+c,ac-b^{2})=0,
\]
and to each solution of this equation there will be a corresponding one dimensional solution of equations \eqref{eq:7.1} and hence a Weingarten surface containing the curve $(C)$ and normal to the vector $\vec e_{3}$ attached to each point of the curve.

The Cauchy problem may be unsolvable or indeterministic if we start with a one dimensional characteristic solution, i.e., according to \eqref{eq:7.III.4}, if $F_{u}+aF_{v}=F_{c}=0$, or if the value chosen for $c$ at each point of $C$ is a double root of the equation $F=0$ determining its value. It is easy to see that if the curve $C$ is chosen arbitrarily, the problem is unsolvable. Indeed we have according to \eqref{eq:7.III.3}, along $C$,
\begin{equation}
  \label{eq:7.III.6}\tag{III, 6}
  F_{a}(da-2b\omega_{12})+F_{b}(db+\overline{a-c}\,\omega_{12})=0,
\end{equation}
from which, according to \eqref{eq:7.III.5},
\[
  \alpha F_{a}+\beta F_{b}=0,\qquad \beta F_{a}+\gamma F_{b}=0,
\]
and hence, along $C$ $(\omega^{2}=0)$,
\begin{equation}
  \label{eq:7.III.7}\tag{III, 7}
  F_{a}(db+\overline{a-c}\,\omega_{12})+F_{b}(dc+2b\omega_{12})=0.
\end{equation}

We see that, $a, b, c$ being functions of the curvature $1/\rho$, the torsion $1/\tau$ of the curve $C$ and the angle $\varpi$ between the normal of $S$ and the principal normal of $C$, the two equations $F=0,F_{c}=0$ and the equation \eqref{eq:7.III.7} furnish three relations between $1/\rho,1/\tau,\varpi,c$ and their derivatives with respect to $s$. The elimination of $c$ and of $\varpi$ will give therefore a relation between the curvature $1/\rho$, the torsion $1/\tau$ and their derivatives with respect to the curvilinear abscissa, which constrains the possible choice of the curve $C$.

We are now going to study some particular cases.




\vspace{12pt}\fsec \textsc{First particular case:} \emph{Surfaces where one of the principal curvatures has a given constant value $\alpha$.} The relation $F=0$ is here, according to formula \eqref{eq:7.15},
\[
F\equiv (\alpha-a)(\alpha-c)-b^{2}=0.
\]

The characteristics are given by
\[
a(\omega^{1})^{2}+2b\omega^{1}\omega^{2}+c(\omega^{2})^{2}=\alpha[(\omega^{1})^{2}+(\omega^{2})^{2}].
\]

The relations for the elements of a characteristic curve are
\[
a=\alpha,\qquad b=0,\qquad(c-\alpha)^{2}\omega_{12}=0.
\]

Two cases are possible. If the value of $c$ along $C$ is not always equal to $\alpha$, we will have $\omega_{12}=0$, and hence the line $C$ must be a geodesic of the surface, with $\varpi=0$, from which
\[
\alpha=\frac{1}{\rho},\qquad\frac{1}{\tau}=0.
\]
The characteristic $C$ is a circumference of radius $1/\alpha$, the surface tangent along $C$ being a cylinder of revlotion with the same radius.


On the other hand, if $c=\alpha$ at all points on $C$, then the line $C$ is an umbilical line, a line traced on a sphere of radius $\alpha$, since the equations
\[
\frac{\cos\varpi}{\rho}=\alpha,\qquad\frac{d\varpi}{ds}+\frac{1}{\tau}=0
\]
give us
\[
\rho^{2}+\tau^{2}\left(\frac{d\rho}{ds}\right)^{2}=\frac{1}{\alpha^{2}},
\]
the sphere then constitutes a solution surface of the question.

The Weingarten surfaces of the class considered here are just the \emph{canal surfaces}\index{canal surface}: envelopes of a family of spheres of radius $1/\alpha$ depending on one parameter. We now easily see that if we have a curve $C$ traced on a sphere $\Sigma$ of radius $1/\alpha$ which is not a great circle of the sphere, the only canal surface that can contain $C$ and is tangent along $C$ to the sphere $\Sigma$ is the sphere itself: the Cauchy problem has one and only one solution in this case. If on the contrary $C$ is a circumference of radius $1/\alpha$, then there exists an infinity of canal surfaces containing $C$ as the geodesic: the Cauchy problem is indeterministic \footnote{The problem has singular solutions in this case since the characteristic equations become an identity if $a=c=\alpha,b=0$. They are the spheres of radius $1/\alpha$.}.




\vspace{12pt}\fsec \textsc{Second particular case:} \emph{Surfaces where the difference of the principal curvatures has a given value $2\alpha$.} The relation between $a,b,c$ is here
\[
F\equiv(a-c)^{2}+4(b^{2}-\alpha^{2})=0.
\]

The characteristics are given by
\[
(a-c)(\omega^{1})^{2}+4b\omega^{1}\omega^{2}-(a-c)(\omega^{2})^{2}=0.
\]

The three relations that a characteristic satisfies are
\[
a-c=0,\qquad b=\pm\alpha,\qquad dc+2b\omega_{12}=0,
\]
from which, supposing $b=\alpha$,
\[
a=c=\frac{\cos\varpi}{\rho},\qquad\frac{d\varpi}{ds}+\frac{1}{\tau}=\alpha,\qquad \cos\varpi\frac{d(1/\rho)}{ds}+\frac{\sin\varpi}{\rho}\left(\frac{1}{\tau}+\alpha\right)=0.
\]

If $d(1/\rho)/ds=0$ with $1/\rho\neq 0$, we have $1/\tau=+\alpha$ with $\varpi =0$, or $1/\tau=-\alpha$ with $d\varpi/ds=2\alpha$. In these two cases the curve $C$ is a circular helix. Otherwise, the curve $C$ is subject to the relation
\[
\frac{d(1/\rho)}{ds}\frac{d(1/\tau)}{ds}-\left(\frac{1}{\tau}+\alpha\right)\frac{d^{2}(1/\rho)}{ds^{2}}+2\frac{\rho}{\tau}\left(\frac{d(1/\rho)}{ds}\right)^{2}+\frac{1}{\rho}\left(\frac{1}{\tau^{2}}-\alpha\right)\left(\frac{1}{\tau}+\alpha\right)=0.
\]
Once the relation is satisfied, the angle $\varpi$ is determined at each point up to multiples of $2\pi$.

The problem does not have singular solutions, since the characteristic equations reduce to identities only if $a=c,b=0$, which is incompatible with the equation $F=0$.



\vspace{12pt}\fsec \textsc{Third particular case:} \emph{Surfaces with constant mean curvature.} The relation between $a, b, c$ is
\[
F=a+c-\alpha=0,
\]
the characteristics, which are imaginary, are the minimal lines of the integral surface. The Cauchy problem has one and only one solution if we specify a curve $C$ and at each point on the curve a trihedral whose vector $\vec e_{1}$ is tangent to the curve: we then have along $C$
\[
a=\frac{\cos\varpi}{\rho},\qquad c=\alpha-\frac{\cos\varpi}{\rho},\qquad b=\frac{d\varpi}{ds}+\frac{1}{\tau}.
\]



\vspace{12pt}\fsec \textsc{Fourth particular case:} \emph{Surfaces with constant total curvature $K$.} Now we have
\[
F=ac-b^{2}-K=0.
\]

The characteristics are given by
\[
a(\omega^{1})^{2}+2b\omega^{1}\omega^{2}+c(\omega^{2})^{2}=0,
\]
these are the asymptotic lines of the integral surface, real if $K<0$. The three relations that a characteristic must satisfy, assuming that at each point being tangent to the vector $\vec e_{1}$, are
\[
a=0,\qquad b^{2}=-K,\qquad c^{2}\omega_{12}+2b(dc+2b\omega_{12})=0,
\]
or
\[
\frac{\cos\varpi}{\rho}=0,\qquad\frac{d\varpi}{ds}+\frac{1}{\tau}=\sqrt{-K},\qquad \frac{dc}{ds}+\frac{c^{2}-4K}{2\sqrt{-K}}\frac{\sin\varpi}{\rho}=0.
\]

If $C$ is not a straight line, we have $\cos\varpi=0, \sin\varpi=\pm 1, 1/\tau=\sqrt{-K}$, and the curve is of constant torsion equal to $\sqrt{-K}$ (Enneper's theorem). We also have
\[
c=\pm\sqrt{-K}\tan\int\frac{ds}{\rho}.
\]

If $C$ is straight, the speed of rotation of the normal of the surface along the line is equal to $\sqrt{-K}$ and we must take $c$ to be a constant value along the line.

As in the preceding problems, we leave aside the question of knowing whether given data satisfying the preceding necessary conditions, the Cauchy problem has one solution or an infinity of solutions. There are certainly cases where we have an infinity of solutions, otherwise the surfaces we are seeking can depend on no more than one arbitrary function of one variable, as the curves of constant torsions given above.


\subsection{Isothermal surfaces}
\label{sec:isothermal-surfaces}\index{isothermal surface}

\fsec An isothermal surface is defined by the property that $ds^{2}$ is reducible to the form $A(d\xi^{2}+d\eta^{2})$, where $\xi$ and $\eta$ are parameters of the curvature lines. Let us attach at each point of such a surface a Darboux trihedral whose vectors $\vec e_{1}$ and $\vec e_{2}$ are along the principal tangents, which restricts us to portions of the surface without umbilical points.

Denoting $a$ and $c$ the principal curvatures, we first have the equations
\begin{equation}
  \label{eq:7.IV.1}\tag{IV, 1}
  \omega^{3}=0,\qquad \omega_{13}=a\omega^{1},\qquad \omega_{23}=c\omega^{2}.
\end{equation}

We then need to express that there exists a function $u$ such that the two forms $u\omega^{1}$ and $u\omega^{2}$ are exact differentials, which gives
\begin{equation}
  \label{eq:7.IV.2}\tag{IV, 2}
  \left\{
    \begin{aligned}
      \left[\omega^{1}\frac{du}{u}\right]+[\omega^{2}\omega_{12}]&=0,\\
      \left[\omega^{2}\frac{du}{u}\right]-[\omega^{1}\omega_{12}]&=0.
    \end{aligned}
  \right.
\end{equation}

Finally by exterior differentiating the equations \eqref{eq:7.IV.1} and taking into account the structural equations, we obtain
\begin{equation}
  \label{eq:7.IV.3}\tag{IV, 3}
  \left\{
    \begin{aligned}{}
      [\omega^{1}da]+(a-c)[\omega^{2}\omega_{12}]&=0,\\
      [\omega^{2}dc]+(a-c)[\omega^{1}\omega_{12}]&=0.
    \end{aligned}
  \right.
\end{equation}

The equations \eqref{eq:7.IV.1}, \eqref{eq:7.IV.2}, \eqref{eq:7.IV.3} constitute the closed differential system of the problem.

The generic two dimensional integral element is given by the equations
\begin{align*}
  \omega_{12}&=h\omega^{1}+k\omega^{2},\\
  \frac{du}{u}&=-k\omega^{1}+h\omega^{2},\\
  da&=a_{1}\omega^{1}+(a-c)h\omega^{2},\\
  dc&=(a-c)k\omega^{1}+c_{2}\omega^{2},
\end{align*}
it depend on four arbitrary parameters $h, k, a_{1}, c_{2}$. As there is four linearly independent quadratic equations \eqref{eq:7.IV.2} and \eqref{eq:7.IV.3}, the system is in involution and its general solution depend on four arbitrary functions of one variable. The determinant of the polar system is $\omega^{1}\omega^{2}[(\omega^{1})^{2}+(\omega^{2})^{2}]$, and the real characteristics are the curvature lines of the integral surfaces.




\vspace{12pt}\fsec\emph{Cauchy problem}. Every one dimensional solution of the equations \eqref{eq:7.IV.1} may be obtained by an oriented curved $C$ at each point of which we attach a right trihedral whose vector $\vec e_{3}$ is normal to $C$. Calling $\theta$ the angle which turns $\vec e_{1}$ around $\vec e_{3}$ in the positive direction to obtain the positive tangent of $C$, we have
\begin{equation}
  \label{eq:7.IV.4}\tag{IV, 4}
  a\cos^{2}\theta+c\sin^{2}\theta=\frac{\cos\varpi}{\rho},\qquad(c-a)\sin\theta\cos\theta=\frac{d\varpi}{ds}+\frac{1}{\tau},
\end{equation}
where $\varpi$ denote the angle between the normal of $S$ with the principal normal of $C$. We can specify the angle $\varpi$ and the angle $\theta$ as arbitrary functions of $s$, and the two preceding equations will give $a$ and $c$ if $\sin\theta\cos\theta$ does not vanish. Finally the function $u$ can be chosen arbitrarily along $C$. With these data there will correspond one and only one isothermal surface containing $C$ and admitting Darboux trihedrals at each point of $C$ corresponding to the trihedral that we have specified at that point.

These data depend on five arbitrary functions of one variable. This is in accord with that we have obtained, since we can determine the surface $S$ by taking for $C$ the section of the surface by a given plane, which reduces the data of the Cauchy problem to four arbitrary functions.

Suppose now we take $\theta=0$, the curve $C$ becoming a curvature line of the unknown integral surface. The equations \eqref{eq:7.IV.4} reduce to
\[
a=\frac{\cos\varpi}{\rho},\qquad\frac{d\varpi}{ds}+\frac{1}{\tau}=0,
\]
but there is a supplementary condition, that the equations \eqref{eq:7.IV.2} and \eqref{eq:7.IV.3} entail
\[
\left[\omega^{2}\left(\frac{du}{u}+\frac{dc}{a-c}\right)\right]=0,
\]
we therefore must have along $C$
\[
\frac{du}{u}+\frac{dc}{a-c}=0.
\]

We can specify the curve $C$ and the function $c$, and the angle $\varpi$ will be given by a constant according to the expression  $(-\int ds/\tau)$. We will then have $a=\cos\varpi/\rho$, and finally the function $u$ will be known up to a factor (\emph{which does not play any role in the question}). This time the data involve only three arbitrary functions of one variable. We leave aside the question of knowing if there exists compatible solutions with the given data and what is their degree of indeterminacy. There are certainly cases where there corresponds to it an infinity of integral surfaces.

\emph{Particular case}. The minimal surfaces\index{minimal surface} are special isothermal surfaces, since for one such sufrace $(c=-a)$, the equations \eqref{eq:7.IV.2} and \eqref{eq:7.IV.3} show that $du/u=da/2a$, which permit suppressing the equations \eqref{eq:7.IV.2}. More generally the surfaces with constant mean curvature are isothermal surfaces since $du/u=da/(a-c)$, which is an exact differential.


\subsection{Two isometric surfaces}
\label{sec:some-isoth-surf}

\fsec Let $S$ and $\bar S$ be two isometric surfaces, that is to say of the same $ds^{2}$. Let us attach at each point of $S$ the most general right trihedral whose vector $\vec e_{3}$ is normal to $S$. By hypothesis there exists a point correspondence between $S$ and $\bar S$ conserving $ds^{2}$. According to the argument in the solution of Problem II, we can assign to each right trihedral attached as $S$ a right trihedral attached at $\bar S$ in a way such that we have, by the correspondence, the relations $\bar\omega^{1}=\omega^{1},\bar\omega^{2}=\omega^{2}$. These two relations give, by exterior differentiation,
\[
[\omega^{1}(\bar\omega_{12}-\omega_{12})]=0,\qquad[\omega^{2}(\bar\omega_{12}-\omega_{12})]=0,
\]
from which we deduce $\bar\omega_{12}-\omega_{12}=0$. The differential system for the problem is formed by the first degree equations
\begin{equation}
  \label{eq:7.V.1}\tag{V, 1}
  \omega^{3}=0,\qquad\bar\omega^{3}=0,\qquad\bar\omega^{1}=\omega^{1},\qquad\bar\omega^{2}=\omega^{2},\qquad\bar\omega_{12}=\omega_{12}.
\end{equation}

The system is completed by exterior differentiation of the these equations, which give three exterior quadratic equations
\begin{equation}
  \label{eq:7.V.2}\tag{V, 2}
  \left\{
    \begin{aligned}[]
      [\omega^{1}\omega_{13}]+[\omega^{2}\omega_{23}]&=0,\\
      [\omega^{1}\bar\omega_{13}]+[\omega^{2}\bar\omega_{23}]&=0,\\
      [\bar\omega_{13}\bar\omega_{23}]-[\omega_{13}\omega_{23}]&=0.\\
    \end{aligned}
  \right.
\end{equation}

These equations contain the eleven forms $\omega^{1},\omega^{2},\omega^{3}$, $\bar\omega^{1},\bar\omega^{2},\bar\omega^{3}$, $\omega_{13},\omega_{23}$, $\bar\omega_{13},\bar\omega_{23}$, $\bar\omega_{12}-\omega_{12}$.
There then exists eleven variables, both dependent and independent: ten of which determining the contact elements of the two surfaces, and the eleventh determining the correspondance between the tangent spaces of the two surfaces at two corresponding points.

The generic two dimensional integral element is given by the relations
\begin{equation}
  \label{eq:7.V.3}\tag{V, 3}
  \left\{
    \begin{aligned}
      \omega_{13}&=a\omega^{1}+b\omega^{2},\\
      \omega_{23}&=b\omega^{1}+c\omega^{2},\\
      \bar\omega_{13}&=\bar a\omega^{1}+\bar b\omega^{2},\\
      \bar\omega_{23}&=\bar b\omega^{1}+\bar c\omega^{2},
    \end{aligned}
  \right.
\end{equation}
with 
\begin{equation}
  \label{eq:7.V.4}\tag{V, 4}
  \bar a\bar c-\bar b^{2}=ac-b^{2}.
\end{equation}

Observe immediately that the relation \eqref{eq:7.V.4} expresses the equality of the total curvature at two corresponding points of the two surfaces, and that the equation $\bar\omega_{12}=\omega_{12}$ expresses the equality of the geodesic curvatures of two corresponding curves (Gauss' theorem).

The polar matrix, whose columns corresponding respectively to the forms $\omega_{13},\omega_{23},\bar\omega_{13},\bar\omega_{23}$ is
\[
\begin{pmatrix}
  \omega^{1}&\omega^{2}&0&0\\
  0&0&\omega^{1}&\omega^{2}\\
  \omega_{23}&-\omega_{13}&-\bar\omega_{23}&\bar\omega_{13}
\end{pmatrix},
\]
its rank is equal to $3$, the number of quadratic equations \eqref{eq:7.V.2}, the system is in involution and its general solution depend on $s_{2}=1$ arbitrary function of two variables.

The one dimensional characteristic solutions, which reduce the rank of the polar matrix to $2$, is given by the equations
\[
\omega^{1}\omega_{13}+\omega^{2}\omega_{23}=0,\qquad\omega^{1}\bar\omega_{13}+\omega^{2}\bar\omega_{23}=0,
\]
and they exist only when the two surfaces $S$ and $\bar S$ admit two asymptotic lines which correspond to each other.

The \emph{singular solutions} of the system are those where the two preceding equations are identities: they are formed by a pair of planes and they are trivial.


\vspace{12pt}\fsec\textsc{Finding surfaces $\bar S$ isometric to a given surface.} Given a surface $S$, the closed differential system that gives $\bar S$ reduces to the equations\label{s20}
\begin{equation}
  \label{eq:7.V.5}\tag{V, 5}
  \left\{
    \begin{aligned}
      \bar\omega^{1}=\omega^{1},\quad\bar\omega^{2}=\omega^{2},\quad\bar\omega^{3}&=0,\quad\bar\omega_{12}=\omega_{12},\\
      [\omega^{1}\bar\omega_{13}]+[\omega^{2}\bar\omega_{23}]&=0,\\
      [\bar\omega_{13}\bar\omega_{23}]-K[\omega^{1}\omega^{2}]&=0,
    \end{aligned}
  \right.
\end{equation}
$K$ denoting the total curvature $ac-b^{2}$ of $S$.

The generic two dimensional integral elements is given by
\begin{equation}
  \label{eq:7.V.6}\tag{V, 6}
  \left\{
    \begin{aligned}
      \bar\omega_{13}&=\bar a\omega^{1}+\bar b\omega^{2},\\
      \bar\omega_{23}&=\bar b\omega^{1}+\bar c\omega^{2},
    \end{aligned}
  \right.
\end{equation}
with
\begin{equation}
  \label{eq:7.V.7}\tag{V, 7}
  \bar{a}\bar{c}-\bar b^{2}=K.
\end{equation}

The polar matrix has its determinant as
\[
\begin{vmatrix}
  \omega^{1}&\omega^{2}\\
  -\bar\omega_{23}&\bar\omega_{13}
\end{vmatrix}
=\omega^{1}\bar\omega_{13}+\omega^{2}\bar\omega_{23}.
\]
Its rank $2$ is equal to the number of quadratic equations \eqref{eq:7.V.5}, and the system is in involution and its general solution depends on two arbitrary functions of one variable.






\vspace{12pt}\fsec\emph{The Cauchy problem}. Given a curve $C$ on the surface $S$, we can take advantage of the indeterminacy of the trihedrals attached at different points of $S$ by keeping at each point of $C$ only the trihedral whose vector $\vec e_{1}$ is tangent to $C$ in a chosen positive direction on the curve. Now given an oriented curve $\bar C$ and we try to attach at each point of $\bar C$ a right trihedral in a way to obtain a one dimensional solution of equation \eqref{eq:7.V.5}. There will exist between $C$ and $\bar C$ a correspondence which conserves the arcs $d\bar s=ds$, the vector $\vec e_{1}$ of the trihedral will be tangent to $\bar C$ in the positive direction, and the vector $\vec e_{3}$ will be normal to $\bar C$, and finally the condition $\bar \omega_{12}=\omega_{12}$ gives
\[
\frac{\sin\bar\varpi}{\rho}=\frac{\sin\varpi}{\rho},
\]
given the curve $\bar C$, this relation gives $\sin\bar\varpi=\sin\varpi({\bar\rho}/{\rho})$. For the problem to be solvable, the curvature $1/\bar\rho$ of $\bar C$ has to be less than the geodesic curvature of $C$ on the surface $S$. If this condition is fulfilled, there are two values for $\bar\varpi$, which give two possible positions for the vector $\vec e_{3}$ of the trihedral attached to $\bar C$. One of these positions being chosen, there will exist one and only one surface $\bar S$ isometric to $S$ containing the curve $\bar C$, whose normal is the vector $\vec e_{3}$.

The case is exceptional if $\bar a=0$, i.e., if $\cos\bar\varpi=0$. If the problem is solvable, the curve $\bar C$ will then be an asymptotic line of $\bar S$. This case happens if at each point of $\bar C$ the absolute value of the curvature is equal to the geodesic curvature of $C$ at a corresponding point. In general, this case has no solutions. The relation \eqref{eq:7.V.7} gives $\bar b^{2}=-K$, and $\bar b$ is the geodesic torsion, i.e., here the ordinary torsion of $\bar C$. It is therefore necessary that the torsion of $\bar C$ is equal in absolute value to the  square root of the negative of the total curvature of the surface at the corresponding point of $C$ (Enneper's theorem). The characteristics of the integral surface $\bar S$ are therefore not arbitrary curves.

Observe that the quadratic equations \eqref{eq:7.V.5} can be written
\begin{align*}
  [(\bar\omega_{13}+\sqrt{-K}\omega_{2})(\bar\omega_{23}-\sqrt{-K}\omega^{1})]&=0,\\
  [(\bar\omega_{13}-\sqrt{-K}\omega_{2})(\bar\omega_{23}+\sqrt{-K}\omega^{1})]&=0,
\end{align*}
they obviously give two families of characteristics
\begin{align*}
  \bar\omega_{13}+\sqrt{-K}\omega^{2}&=0,&\bar\omega_{23}-\sqrt{-K}\omega^{1}&=0,\\
  \bar\omega_{13}-\sqrt{-K}\omega^{2}&=0,&\bar\omega_{23}+\sqrt{-K}\omega^{1}&=0,
\end{align*}
the curves in each of these families satisfy the asymptotic equations. On the other hand we have the following for the first family\label{s21}
\begin{align*}
\omega^{1}\bar\omega_{23}-\omega^{2}\bar\omega_{13}&=\sqrt{-K}[(\omega^{1})^{2}+(\omega^{2})^{2}],&\text{where}&&\frac{1}{\bar\tau}&=\sqrt{-K},\\
\intertext{and the following for the second family}
\omega^{1}\bar\omega_{23}-\omega^{2}\bar\omega_{13}&=-\sqrt{-K}[(\omega^{1})^{2}+(\omega^{2})^{2}],&\text{where}&&\frac{1}{\bar\tau}&=-\sqrt{-K}.
\end{align*}



\subsection{Two isometric surfaces with the same families of asymptotic lines}
\label{sec:some-isoth-surf-1}

\fsec We attach  at corresponding points of two surfaces right trihedrals whose vector $\vec e_{1}$ is tangent to the asymptotic of the family conserved by the correspondence and whose vector $\vec e_{3}$ is normal to the surface such that we have $\bar\omega^{1}=\omega^{1}, \bar\omega^{2}=\omega^{2}$ (the trihedral being chosen on $S$, the one attached at $\bar S$ is then determined). The consideration of relations \eqref{eq:7.V.3} and \eqref{eq:7.V.4} in the previous section, where $a=\bar a=0$, gives the equaitons
\begin{equation}
  \label{eq:7.VI.1}\tag{VI, 1}
  \left\{
    \begin{gathered}
      \bar\omega^{1}=\omega^{1},\quad\bar\omega^{2}=\omega^{2},\quad\omega^{3}=0,\quad\bar\omega^{3}=0,\quad\bar\omega_{12}=\omega_{12},\\
      \bar\omega_{13}=\varepsilon\omega_{13},\quad(\varepsilon=\pm1),\\
      [\omega^{1}\omega_{13}]+[\omega^{2}\omega_{23}]=0,\quad[\omega^{1}\bar\omega_{13}]+[\omega^{2}\bar\omega_{23}]=0,\\
      [\omega^{2}\omega_{13}]=0,\quad[\omega_{12}(\bar\omega_{23}-\varepsilon\omega_{23})]=0,
    \end{gathered}
  \right.
\end{equation}
the last equation comes from exterior differentiation of the equation $\bar\omega_{13}=\varepsilon\omega_{13}$.

The two dimensional generic integral element is given by the relations
\begin{align*}
  \omega_{13}=b\omega^{2},\qquad\omega_{23}&=b\omega^{1}+c\omega^{2},\\
  \bar\omega_{13}=\varepsilon b\omega^{2},\qquad\bar\omega_{23}&=\varepsilon b\omega^{1}+\bar c\omega^{2},\\
  (\bar c-\varepsilon c)[\omega_{12}\omega^{2}]&=0.
\end{align*}

Two cases can be distinguished:

1. $\bar c=\varepsilon c$. In this case $\bar \omega_{13}=\varepsilon\omega_{13},\bar\omega_{23}=\varepsilon\omega_{23},\bar\omega_{12}=\omega_{12}$. If $\varepsilon=1$ the two families of trihedrals are equal, and hence the two surfaces are equal as well; if $\varepsilon=-1$ the two surfaces are symmetric. This solution is trivial.

2. $\bar c-\varepsilon c\neq 0$. In this case we have
\[
\omega_{12}=h\omega^{2},
\]
the two dimensional integral element depends on four arbitrary parameters $b,c,\bar c, h$.

The determinant of the polar matrix, where the  columns  correspond to $\omega_{13},\omega_{23},\bar\omega_{23},\omega_{12}$, is
\[
\begin{vmatrix}
  \omega^{1}&\omega^{2}&0&0\\
  \varepsilon\omega^{1}&0&\omega^{2}&0\\
  \omega^{2}&0&0&0\\
  0&-\varepsilon\omega_{12}&\omega_{12}&-\bar\omega_{23}+\varepsilon\omega^{23}
\end{vmatrix}=(\varepsilon c-\bar c)(\omega^{2})^{4},
\]
its rank $4$ is equal to the number of quadratic equations \eqref{eq:7.VI.1}, the system is in involution and its general solution depend on four arbitrary functions of one variable. The characteristics are the asymptotic lines which correspond to each other on the two surfaces.

The surfaces $S$ and $\bar S$ enjoy a simple geometrical property, i.e., they are ruled surfaces\index{ruled surface}. Indeed we displace along the asymptotic line $\omega^{2}=0$, we have
\[
d\vec A=ds\,\vec e_{1},\qquad d\vec e_{1}=\omega_{12}\vec e_{2}+\omega_{13}\vec e_{3}=0,
\]
the point $A$ describes a straight line, the vector $\vec e_{1}$ being constant.

\vspace{12pt}\fsec\emph{The Cauchy problem}. We will have one one dimensional solution of the linear equations \eqref{eq:7.VI.1} when we specify two oriented curves $C, \bar C$ corresponding to each other with their curvilinear abscissas conserved. We attach at each point of $C$ a right trihedral whose vector $\vec e_{3}$ is normal to $C$, and let $\theta$ be the angle between the vector $\vec e_{1}$ with the positive tangent of $C$. At the corresponding point of $\bar C$ we need to attach a right trihedral whose vector $\vec e_{3}$ is normal to $\bar C$ and whose vector $\vec e_{1}$ makes the same angle $\theta$ with the positive tangent of $\bar C$. To satisfy the last two linear equations of \eqref{eq:7.VI.1}, it is necessary to have \footnote{According to the equations \eqref{eq:7.14} of \textsection{\textbf{6}}, p.~\pageref{pageref1}, we have the identity
\[
\omega_{1}\Phi-\omega_{2}\Psi=\omega_{13}F,
\]
where $F,\Phi,\Psi$ denote the three fundamental forms. We deduce from this
\[
\omega_{13}=\frac{1}{R_{n}}\omega_{1}-\frac{1}{T_{g}}\omega_{2},
\]
from this we get the second equation of \eqref{eq:7.VI.2}.
}
\begin{equation}
  \label{eq:7.VI.2}\tag{VI, 2}
  \left\{
    \begin{aligned}
      \frac{\sin\bar\varpi}{\bar\rho}&=\frac{\sin\varpi}{\rho},\\
      \cos\theta\frac{\cos\bar\varpi}{\bar\rho}-\sin\theta\left(\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}\right)&=\varepsilon\cos\theta\frac{\cos\varpi}{\rho}-\varepsilon\sin\theta\left(\frac{d\varpi}{ds}+\frac{1}{\tau}\right).
    \end{aligned}
  \right.
\end{equation}

Given the two oriented curves $C, \bar C$ together with the angle $\theta$, the two preceding equations determine $\varpi$ and $\bar \varpi$. If we choose for example $\theta=\pi / 2$, we have
\begin{align*}
  \frac{d(\bar \varpi-\varepsilon\varpi)}{ds}&=\frac{\varepsilon}{\tau}-\frac{1}{\bar\tau},\\
  \tan\frac{\bar\varpi+\varepsilon\varpi}{2}&=\frac{\bar\rho+\varepsilon\rho}{\bar\rho-\varepsilon\rho}\tan\frac{\bar\varpi-\varepsilon\varpi}{2},
\end{align*}
which give $\bar\varpi-\varepsilon\varpi$ up to an additive constant, and hence $\tan((\bar\varpi+\varepsilon\varpi)/2)$ where $\bar\varpi+\varepsilon\varpi$ is solved up to multiples of $2\pi$. The angles $\varpi$ and $\bar\varpi$ being fixed, the two surfaces $S$ and $\bar S$ are well determined. Through each point of $C$ there is a straight line tangent to $S$ along the vector $\vec e_{1}$, and this line generate the surface $S$. The surface $\bar S$ is generated by an analogous manner.

Alternatively,  we could proceed by specifying the curves $C$ and $\bar C$ arbitrarily and at each point of the curves the vector $\vec e_{3}$ normal to the curve subject to the condition that we have $\sin\varpi/\rho=\sin\bar\varpi/\bar\rho$, and now the angle $\theta$ would be determined, according to the equations \eqref{eq:7.VI.2}, by its tangent. The condition that this angle is non-zero is expressed by the inequality
\[
\frac{\cos\bar\varpi}{\bar\rho}\neq\varepsilon\frac{\cos\varpi}{\rho}.
\]
The Cauchy-Kowalewski theorem does not apply if the normal curvatures of $C$ and $\bar C$ are zero. In this case we know that the two lines must be straight such that the angle which turns the vector $\vec e_{3}$ when we displace by a certain segment of $\bar C$ is equal to the analogous angle relative to $C$ multiplied by $\varepsilon$ $(\bar b=\varepsilon b)$. It is clear that the problem then has an infinite number of solutions.

\emph{Remark I}. If the surface $S$ is given as well as the curve $C$, the corresponding curve $\bar C$ as well as the angle $\bar \varpi$ are given by two equations in $1/\bar\rho,1/\bar\tau,\bar\varpi,d\bar\varpi/d\bar s$. By eliminating $\bar\varpi$, we see that the curve $\bar C$ must satisfy a certain relation relating its curvature, its torsion and their derivatives with respect to the segment.

\emph{Remark II}. Two ruled surfaces can be isometric without their generating curves corresponding to each other.


\subsection{Two isometric surfaces with the same curvature lines}
\label{sec:some-isoth-surf-2}

\fsec To the equations
\begin{equation}
  \label{eq:7.VII.1}\tag{VII, 1}
  \bar\omega^{1}=\omega^{1},\quad\bar\omega^{2}=\omega^{2},\quad\omega^{3}=0,\quad\bar\omega=0,\quad\bar\omega_{12}=\omega_{12}
\end{equation}
which hold for any two isometric surfaces, we must adjoin a new equation of the form
\[
\omega^{1}\bar\omega_{23}-\omega^{2}\bar\omega_{13}=u(\omega^{1}\omega_{23}-\omega^{2}\omega_{13}),
\]
$u$ being an essential non-zero unknown function. This equation can be written
\begin{equation}
  \label{eq:7.VII.2}\tag{VII, 2}
  \left\{
    \begin{aligned}
      \bar\omega_{13}&=u\omega_{13}+v\omega^{1},\\
      \bar\omega_{23}&=u\omega_{23}+v\omega^{2}.
    \end{aligned}
  \right.
\end{equation}

Finally exterior differentiation of the equations \eqref{eq:7.VII.1} and \eqref{eq:7.VII.2} result in the quadratic equations
\begin{equation}
  \label{eq:7.VII.3}\tag{VII, 3}
  \left\{
    \begin{aligned}[]
      [\omega^{1}\omega_{13}]+[\omega^{2}\omega_{23}]&=0,\\
      [\bar\omega_{13}\bar\omega_{23}]-[\omega_{13}\omega_{23}]&=0,\\
      [\omega_{13}du]+[\omega^{1}dv]&=0,\\
      [\omega_{23}du]+[\omega^{2}dv]&=0.
    \end{aligned}
  \right.
\end{equation}

The equations \eqref{eq:7.VII.1}, \eqref{eq:7.VII.2}, \eqref{eq:7.VII.3} constitute the closed differential equation for the problem.

The generic two dimensional integral element is given by
\begin{equation}
  \label{eq:7.VII.4}\tag{VII, 4}
  \left\{
    \begin{aligned}
      \omega_{13}&=a\omega^{1}+b\omega^{2},\\
      \omega_{23}&=b\omega^{1}+c\omega^{2},\\
      du&=u_{1}\omega^{1}+u_{2}\omega^{2},\\
      dv&=(bu_{2}-cu_{1})\omega^{1}+(bu_{1}-au_{2})\omega^{2},
    \end{aligned}
  \right.
\end{equation}
with
\begin{equation}
  \label{eq:7.VII.5}\tag{VII, 5}
  (au+v)(cu+v)-b^{2}u^{2}=ac-b^{2},
\end{equation}
it depends therefore on four arbitrary parameters.

The determinant of the polar matrix, where the columns correspond to the forms $\omega_{13},\omega_{23},du,dv$ is
\begin{align*}
&\quad
\begin{vmatrix}
  \omega^{1}&\omega^{2}&0&0\\
  \omega_{23}-u\bar\omega_{23}&-\omega_{13}+u\bar\omega_{13}&0&0\\
  -du&0&\omega_{13}&\omega^{1}\\
  0&-du&\omega_{23}&\omega^{2}
\end{vmatrix}\\
&=-(\omega^{1}\omega_{23}-\omega^{2}\omega_{13})[u(\omega^{1}\bar\omega_{13}+\omega^{2}\bar\omega_{23})-(\omega^{1}\omega_{13}+\omega^{2}\omega_{23})].
\end{align*}

The rank is equal to $4$, the number of quadratic equations \eqref{eq:7.VII.3}, the system is in involution and its generic solution depend on four arbitrary functions of one variable.



\vspace{12pt}\fsec \emph{The singular solutions} are those for which the determinant of the polar matrix is identically zero, which gives $u=\pm 1, v=0$. The two surfaces are then equal or symmetric: these are the trivial solutions. We need to add to these the singular solutions where the curvature lines are indeterminate: two planes or spheres, which are also trivial solutions.

Hence it follows that \emph{surfaces $S$ enjoying the property that there exists a surface $\bar S$ isometric to $S$ with the same the curvature lines without either being equal or symmetric to $S$ are exceptional.}

The characteristics are:

1. The two families of curvature lines which correspond to each other on the two surfaces;

2. The two families defined by
\[
u(\omega^{1}\bar\omega_{13}+\omega^{2}\bar\omega_{23})=\omega^{1}\omega_{13}+\omega^{2}\omega_{23},
\]
these two families are not in general real. Indeed the discriminant of the quadratic form
\begin{align*}
  &\quad u(\omega^{1}\bar\omega_{13}+\omega^{2}\bar\omega_{23})-(\omega^{1}\omega_{13}+\omega^{2}\omega_{23})\\
  &=(u^{2}-1)[a(\omega^{1})^{2}+2b\omega^{1}\omega^{2}+c(\omega^{2})^{2}]+uv[(\omega^{1})^{2}+(\omega^{2})^{2}]
\end{align*}
is
\begin{align*}
  &\quad (u^{2}-1)^{2}b^{2}-[(u^{2}-1)a+uv][(u^{2}-1)c+uv]\\
  &=(u^{2}-1)^{2}(b^{2}-ac)-(u^{2}-1)uv(a+c)-u^{2}v^{2},
\end{align*}
taking into account the relation  \eqref{eq:7.VII.5}, this discriminant reduces to $-v^{2}<0$.

\vspace{12pt}\fsec \emph{The Cauchy problem}. Let us find a one dimensional solution of the system of equations \eqref{eq:7.VII.1} and \eqref{eq:7.VII.2}. We can suppose, because of the indeterminacy of the trihedrals attached at the surfaces $S$ and $\bar S$, that we have $\omega_{2}=0$ for this situation. We therefore take two oriented curves $C$ and $\bar C$ which correspond to each other with the same arc elements $(d\bar s=ds)$. At each point on each curve we attach a trihedral whose vector $\vec e_{1}$ is tangent to the curve in the positive direction. We then have
\[
\frac{\sin\bar\varpi}{\bar\rho}=\frac{\sin\varpi}{\rho},\qquad\frac{\cos\bar\varpi}{\bar\rho}=u\frac{\cos\varpi}{\rho}+v,\qquad\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}=u\left(\frac{d\varpi}{ds}+\frac{1}{\tau}\right).
\]
We see that the choice of the vectors $\vec e_{3}$ is subject to the single condition
\[
\frac{\sin\bar\varpi}{\bar\rho}=\frac{\sin\varpi}{\rho},
\]
the functions $u$ and $v$ are then determined along $C$ and $\bar C$ by the two other equations which essentially exclude the curvature lines. These data determine the surfaces $S$ and $\bar S$ unambiguously. We see that they involve five arbitrary functions of one variable (two arbitrary functions for each curve and one arbitrary function for the angle $\varpi$), but if we are forced to take the curve $C$ in a given plane, there is then only four arbitrary functions, conforming to the result obtained above.

Now take the case where the Cauchy-Kowalewski theorem does not hold. In this case we have the necessary conditions
\begin{gather*}
  \frac{\sin\bar\varpi}{\bar\rho}=\frac{\sin\varpi}{\rho},\qquad\frac{d\varpi}{ds}+\frac{1}{\tau}=0,\qquad\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}=0,\\
  \frac{\cos\bar\varpi}{\bar\rho}=u\frac{\cos\varpi}{\rho}+v.
\end{gather*}

The equations \eqref{eq:7.VII.4} show further that in the present case $(\omega_{2}=0,b=0)$ we must have $dv+c\,du=0$ along the lines $C$ and $\bar C$. We have
\[
a=\frac{\cos\varpi}{\rho},\qquad\bar a=\frac{\cos\bar\varpi}{\bar\rho},\qquad v=\bar a-ua,
\]
then, according to \eqref{eq:7.VII.5},
\[
\bar a(cu+v)=ac,
\]
we deduce from it easily
\[
c=\frac{\bar a(\bar a -au)}{a-\bar a u},
\]
by replacing $v$ and $c$ by their values in the equation $dv+c\,du=0$, we obtain the Riccati equation for the function $u$
\[
(\bar a^{2}-a^{2})\frac{du}{ds}+\bar a\frac{da}{ds}u^{2}-\left(a\frac{da}{ds}+\bar a\frac{d\bar a}{d\bar s}\right)u+a\frac{d\bar a}{d\bar s}=0.
\]

Observe that the two curves $C$ and $\bar C$ cannot be chosen arbitrarily. They as a whole depend on only two arbitrary functions of one variable. We can specify arbitrarily the angles $\varpi$ and $\bar \varpi$ as well as the common geodesic curvature $1/R_{g}$ as functions of $s$, but then the curvatures and torsions are determined as functions of $s$.


\vspace{12pt}\fsec\textsc{Another method}. The calculations become simpler if we attach at each point of $S$ the Darboux trihedral whose vectors $\vec e_{1}$ and $\vec e_{2}$ are along the principal tangents, and by doing this we naturally limit ourselves to regions without umbilical points. We then have the equations
\begin{equation}
  \label{eq:7.VII.6}\tag{VII, 6}
  \left\{
    \begin{gathered}
      \bar\omega^{1}=\omega^{1},\qquad\bar\omega^{2}=\omega^{2},\qquad\omega^{3}=0,\qquad\bar\omega_{12}=\omega_{12},\\
      \omega_{13}=a\omega^{1},\qquad\omega_{23}=c\omega^{2},\\
      \bar\omega_{13}=ta\omega^{1},\qquad\bar\omega_{23}=\frac{c}{t}\omega^{2},
    \end{gathered}
  \right.
\end{equation}
where the last two express the equality of the total curvatures at two corresponding points of the two surfaces.

We close the system by exterior differentiation, and obtain the new equations
\begin{equation}
  \label{eq:7.VII.7}\tag{VII, 7}
  \left\{
    \begin{aligned}{}
      [\omega^{1}da]+(a-c)[\omega^{2}\omega_{12}]&=0,\\
      [\omega^{2}dc]+(a-c)[\omega^{1}\omega_{12}]&=0,\\
      [\omega^{1}d(ta)]+\left(ta-\frac{c}{t}\right)[\omega^{2}\omega_{12}]&=0,\\
      \left[\omega^{2}d\left(\frac{c}{t}\right)\right]+\left(ta-\frac{c}{t}\right)[\omega^{1}\omega_{12}]&=0.
    \end{aligned}
  \right.
\end{equation}

We can modify the last two equations by using the first two equations, which gives the equivalent equations
\begin{equation}
  \label{eq:7.VII.7'}\tag{VII, 7$'$}
  \left\{
    \begin{aligned}{}
      [\omega^{1}da]+(a-c)[\omega^{2}\omega_{12}]&=0,\\
      [\omega^{2}dc]+(a-c)[\omega^{1}\omega_{12}]&=0,\\
      \left[\omega^{1}\left(\frac{t\,dt}{1-t^{2}}+\frac{c}{a}\frac{da}{a-c}\right)\right]&=0,\\
      \left[\omega^{2}\left(\frac{dt}{t(1-t^{2})}-\frac{a}{c}\frac{da}{a-c}\right)\right]&=0.
    \end{aligned}
  \right.
\end{equation}

The characteristics are, as can be shown by an easy calculation, given by the equation
\[
\omega^{1}\omega^{2}[a^{2}t^{2}(\omega^{1})^{2}+c^{2}(\omega^{2})^{2}]=0,
\]
we find the two families of curvature lines and two imaginary families of characteristics.

\vspace{12pt}\fsec This method has the advantage of rapidly arriving at the differential system which gives the surfaces $\bar S$ isometric to a given surface $S$ with corresponding lines of curvature, which are neither equal nor symmetric to $S$. Given the surface $S$, effectively we simply have to determine the unknown $t$ with the condition $t^{2}\neq 1$. The last two equations \eqref{eq:7.VII.7'} represent the differential system we are interested in, in which $a$ and $c$ are the given data. If we set, on the surface $S$,
\[
\omega_{12}=h\omega^{1}+k\omega^{2},
\]
the first two equations of \eqref{eq:7.VII.7} give us
\begin{align*}
  a_{2}&=(a-c)h,\\
  c_{1}&=(a-c)k,
\end{align*}
where we denote by $a_{2}$ the second covariant derivative of $a$ ($da=a_{1}\omega_{1}+a_{2}\omega_{2})$ and by $c_{1}$ the first covariant derivative of $c$. The last two equations \eqref{eq:7.VII.7'} then give us
\begin{equation}
  \label{eq:7.VII.8}\tag{VII, 8}
  \frac{t\,dt}{1-t^{2}}=t\frac{ta}{c}k\omega^{1}-\frac{c}{a}h\omega^{2}=t^{2}\varpi^{1}-\varpi^{2},
\end{equation}
by using the abbreviations
\[
\frac{a}{c}k\omega^{1}=\varpi^{1},\qquad\frac{c}{a}h\omega^{2}=\varpi^{2}.
\]

The exterior differentiation of \eqref{eq:7.VII.8} gives
\[
t^{2}\{d\varpi^{1}-2[\varpi^{1}\varpi^{2}]\}=d\varpi^{2}-2[\varpi^{1}\varpi^{2}].
\]

The coefficient of $t^{2}$ on the left hand side is known, and we will write it as $A[\omega^{1}\omega^{2}]$. The right hand side is also known, let us write it as $B[\omega^{1}\omega^{2}]$.

After this is done, if $A$ and $B$ are not both zero, the differential system admit only the solution $t^{2}=B/A$. In general this will not be a solution, since we know that the problem is solvable only for one narrow class of surfaces $S$. It might happen that $t^{2}=B/A$ is actually a solution and then it will be the unique solution of the problem, if  $B\neq A$ \footnote{In reality there will be two solutions corresponding to two surfaces $S$ symmetric to each other.}. If $A=B=0$, the equation \eqref{eq:7.VII.8} is completely integrable, and in this case there is an infinite number of surfaces $\bar S$ depending on essentially one arbitrary constant (essentially means: up to translation).

The surfaces $S$ for which this particular case arises can be characterised by the property that the two forms
\[
\varpi^{1}=\frac{a}{c}k\omega^{1},\qquad\varpi^{2}=\frac{c}{a}h\omega^{2}
\]
satisfy the two relations
\[
d\varpi^{1}=d\varpi^{2}=2[\varpi^{1}\varpi^{2}],
\]
in particular the form $\varpi^{1}-\varpi^{2}$ is an exact differential.

These surfaces can be determined. Using the relations
\[
d\omega^{1}=h[\omega^{1}\omega^{2}],\qquad d\omega^{2}=k[\omega^{1}\omega^{2}],
\]
we find
\[
\left(\frac{h}{a}\right)_{1}=\frac{hk}{c},\qquad
\left(\frac{k}{c}\right)_{2}=-\frac{hk}{a}.
\]

Continuing the calculation \footnote{The differential system which gives the surfaces we are seeking just by using the results we have just obtained is not in involution. We find the relation $hk=0$ by applying the method of prolongation indicated in chapter VI.}, we find $hk=0$.

Let, for example, $k=0$. we have $d(h/a)=\lambda\omega^{2}$. The exterior differentiation of the equation $\omega_{12}=h\omega^{1}$ gives $h_{2}=ac+h^{2}$, and as $a_{2}=(a-c)h$, it follows
\[
\lambda=c\left(1+\frac{h^{2}}{a^{2}}\right).
\]

The closed differential system which defines the class of surfaces  $S$ under consideration is
\begin{equation}
  \label{eq:7.VII.9}\tag{VII, 9}
  \left\{
    \begin{gathered}
      \omega^{3}=0,\quad\omega_{13}=a\omega^{1},\quad\omega_{23}=c\omega^{2},\quad\omega_{12}=h\omega^{1},\\
      d\left(\frac{h}{a}\right)=c\left(1+\frac{h^{2}}{a^{2}}\right)\omega^{2},\\
      [\omega^{1}da]-h(a-c)[\omega^{1}\omega^{2}]=0,\\
      [\omega^{2}dc]=0.
    \end{gathered}
  \right.
\end{equation}

It is in involution and its general solution depends on two arbitrary functions of one variable. The characteristics are the two families of curvature lines.


\vspace{12pt}\fsec We can characterise these surfaces geometrically. First observe that the curvature lines of the second family $(\omega^{1}=0)$ are geodesics $(\omega_{12}=0)$. They are planer, since by moving along one of them we have
\[
\frac{d\vec A}{ds}=\vec e_{2},\qquad\frac{d\vec e_{2}}{ds}=c\vec e_{3},\qquad\frac{d\vec e_{3}}{ds}=-c\vec e_{2},
\]
its curvature is $c$. As $dc$ is a multiple of $\omega^{2}$, all these geodesic planes are equal. The planes of these different curvature lines bound a developable surface, and the surface is therefore generated by a planer curve $\Gamma$ whose plane rolls without slipping on a fixed developable surface (each point of $\Gamma$ moves normally to the plane of the curve). As the vector $\vec e_{1}$ normal to the plane of the curve has its differential as $(h\vec e_{2}+a\vec e_{3})\omega^{1}$, it follows that the characteristic of the plane of $\Gamma$ is, in the plane $A\vec e_{2}\vec e_{3}$,  perpendicular to the vector $h\vec e_{2}+a\vec e_{3}$. The generating line of the developable surface bounding the plane of $\Gamma$ is therefore a straight line parallel to the vector $\vec e_{2}-({h}/{a})\vec e_{3}$. As the differential of the vector is $c(h/a)(\vec e_{2}-(h/a)\vec e_{3})\omega^{2}$, it follows that the generating line of the developable surface has a fixed direction. The plane of $\Gamma$ therefore bounds a cylinder, such that the surfaces sought after are the \emph{moulding surfaces}\index{moulding surface}, which effectively depend on two arbitrary functions of one variable.

We therefore arrive at the following conclusion. \emph{The surfaces $S$ for which there exists a surface $\bar S$ isometric to $S$ with the same the curvature lines without $\bar S$ being equal or symmetric to $S$ are exceptional, and they form a class depending on four arbitrary functions of one variable. The corresponding surface  $\bar S$ is in general unique, up to a translation or a symmetry. Exceptions are the moulding surfaces, for which the surface $\bar S$ depends on one arbitrary constant up to a translation or a symmetry.}


\subsection{Two isometric surfaces with the same principal curvatures}
\label{sec:some-isoth-surf-3}

\fsec It suffices that the mean curvature is the same for the two surfaces. If we use the same right trihedrals as in the preceding problems, we have the equations
\begin{equation}
  \label{eq:7.VIII.1}\tag{VIII, 1}
  \bar\omega^{1}=\omega^{1},\qquad\bar\omega^{2}=\omega^{2},\qquad\omega^{3}=0,\qquad\bar\omega^{3}=0,\qquad\bar\omega_{12}=\omega_{12}.
\end{equation}
As on the other hand
\[
[\omega^{1}\omega_{23}]-[\omega^{2}\omega_{13}]=(a+c)[\omega^{1}\omega^{2}]=\left(\frac{1}{R_{1}}+\frac{1}{R_{2}}\right)[\omega^{1}\omega^{2}],
\]
the closed differential system which express these conditions of the problem is obtained by adjoining to the equations \eqref{eq:7.VIII.1} the quadratic equations
\begin{equation}
  \label{eq:7.VIII.2}\tag{VIII, 2}
  \left\{
    \begin{aligned}{}
      [\omega^{1}\omega_{13}]+[\omega^{2}\omega_{23}]&=0,\\
      [\omega^{1}\bar\omega_{13}]+[\omega^{2}\bar\omega_{23}]&=0,\\
      [\bar\omega_{13}\bar\omega_{23}]-[\omega_{13}\omega_{23}]&=0,\\
      [\omega^{1}(\bar\omega_{23}-\varepsilon\omega_{23})]-[\omega^{2}(\bar\omega_{13}-\varepsilon\omega_{13})]&=0,\qquad(\varepsilon=\pm 1).
    \end{aligned}
  \right.
\end{equation}

The double sign is due to that the rays of the principal curvatures of the two surfaces can be along different directions with respect to the vectors $\vec e_{3}$ attached at the surfaces $S$ and $\bar S$. We can take the case $\varepsilon=1$, and the solution in the case of $\varepsilon=-1$ is deduced from the solutions of the case $\varepsilon=+1$ by symmetry of the surface $\bar S$ with respect to a plane.

The two dimensional generic integral element is given by
\begin{align*}
  \omega_{13}&=a\omega^{1}+b\omega^{2},&\omega_{23}&=b\omega^{1}+c\omega^{2},\\
  \bar\omega_{13}&=\bar a\omega^{1}+\bar b\omega^{2},&\bar\omega_{23}&=\bar b\omega^{1}+\bar c\omega^{2},
\end{align*}
with
\[
\bar a\bar c-\bar b^{2}=ac-b^{2},\qquad \bar a+\bar c=a+c,
\]
it depends on four arbitrary parameters.

The determinant of the polar matrix, whose columns correspond to the forms $\omega_{13}$, $\omega_{23}$, $\bar\omega_{13}$, $\bar\omega_{23}$, is
\begin{equation}
  \label{eq:7.VIII.3}\tag{VIII, 3}
      \begin{vmatrix}
        \omega^{1}&\omega^{2}&0&0\\
        0&0&\omega^{1}&\omega^{2}\\
        \omega_{23}&-\omega_{13}&-\bar\omega_{23}&\bar\omega_{13}\\
        \omega^{2}&-\omega^{1}&-\omega^{2}&\omega^{1}
      \end{vmatrix}
      =[(\omega^{1})^{2}+(\omega^{2})^{2}][\omega^{1}(\omega_{13}-\bar\omega_{13})+\omega^{2}(\omega_{23}-\bar\omega_{23})].
\end{equation}
The rank of the matrix is equal to $4$, the number of quadratic equations \eqref{eq:7.VIII.2}. The system is in involution and its general solution depends on four arbitrary functions of one variable.

\emph{Singular solutions}. They correspond to $\bar a = a,\bar b=b, \bar c=c$, that is to say $\bar \omega_{13}=\omega_{13},\bar\omega_{23}=\omega_{23}$. They are furnished by two equal or symmetric $(\varepsilon=-1)$ surfaces. They are trivial solutions.

The real \emph{characteristics} are furnished by the pair of corresponding curves having the same normal curvature.


\vspace{12pt}\fsec\emph{The Cauchy problem}. Every one dimensional solution of system \eqref{eq:7.VIII.1} is obtained by taking two corresponding oriented curves $C$ and $\bar C$ with the same arc elements and attaching to these curves the right trihedrals whose vector $\vec e_{1}$ is along the positive tangent of the curve, the vector $\vec e_{3}$ chosen in a way to satisfy the relation
\begin{equation}
  \label{eq:7.VIII.4}\tag{VIII, 4}
  \frac{\sin\bar\varpi}{\bar\rho}=\frac{\sin\varpi}{\rho},
\end{equation}
one such solution depends on $5$ arbitrary functions of one variable. It is characteristic if we have at the same time
\begin{equation}
  \label{eq:7.VIII.5}\tag{VIII, 5}
  \frac{\cos\bar\varpi}{\bar\rho}=\frac{\cos\varpi}{\rho},
\end{equation}
which is to say if the curvature is the same function as the curvilinear abscissa for the two curves. 
We can then take $\bar\varpi=\varpi$. On the other hand, the equalities
\[
\bar a=a,\qquad\bar a+\bar c=a+c,\qquad\bar a\bar c-\bar b^{2}=ac-b^{2}
\]
entail
\[
\bar c=c,\qquad\bar b=\pm b,
\]
the two curves therefore have equal or opposite geodesic torsion. This is a new condition which they two curves must satisfy for the Cauchy problem to be well-defined \footnote{On problem VIII, posed by \textsc{O.~Bonnet}, see an article by \textsc{E.~Cartan}, \emph{Sur les couples de surfaces applicables avec conservation des courbures principales} (Bull.~Sc.~Math., \textbf{66}, 1942, pp.~55--85).}.

\subsection[{Two surfaces in point correspondence with the same curvature lines and principal curvatures}]{Two surfaces in point correspondence\\with the same curvature lines and principal curvatures}
\label{sec:some-surfaces-point}

\fsec The problem is defined by requiring the correspondance between the two surfaces to conserve the mean curvature and the total curvature.

We attach at each point of the surface $S$ a right Darboux trihedral \footnote{This assumes that we are considering only parts of the surface without umbilical points.}. Then we can attach at corresponding points of the surface $\bar S$ in one and only one way such that
\begin{equation}
  \label{eq:7.IX.1}\tag{IX, 1}
  \omega^{3}=0,\qquad\bar\omega^{3}=0,\qquad\bar\omega^{1}=u\omega^{1},\qquad\bar\omega^{2}=v\omega^{2}\qquad(u>0,\ v>0).
\end{equation}

Granted this, we can now distinguish two essentially distinct cases according wo whether the principal curvatures are the same for the curvature lines which correspond to each other on the two surfaces, or whether the principal curvatures exchanges between the two families of curvature lines when we pass from one surface to the other.


\vspace{12pt}\fsec\textsc{First problem}. We will adjoin to the equations \eqref{eq:7.IX.1} the equations
\begin{equation}
  \label{eq:7.IX.2}\tag{IX, 2}
  \omega_{13}=a\omega^{1},\qquad\omega_{23}=c\omega^{2},\qquad\bar\omega_{13}=\varepsilon a\bar\omega^{1},\qquad\bar\omega_{23}=\varepsilon c\bar\omega^{2}\qquad(\varepsilon=\pm1),
\end{equation}
and the equations that are deduced from \eqref{eq:7.IX.1} and \eqref{eq:7.IX.2} by exterior differentiation. The new equations are
\begin{align*}
  [\omega^{1}du]+[\omega^{2}(u\omega_{12}-v\bar\omega_{12})]&=0,\\
  [\omega^{2}dv]+[\omega^{1}(u\bar\omega_{12}-v\omega_{12})]&=0,\\
  [\omega^{1}da]+(a-c)[\omega^{2}\omega_{12}]&=0,\\
  [\omega^{2}dc]+(a-c)[\omega^{1}\omega_{12}]&=0,\\
  [\bar\omega^{1}da]+(a-c)[\bar\omega^{2}\bar\omega_{12}]&=0,\\
  [\bar\omega^{2}dc]+(a-c)[\bar\omega^{1}\bar\omega_{12}]&=0.
\end{align*}

By subtracting from the fifth equation the third equation multiplied by $u$, and the sixth equation the fourth equation multiplied by $v$, we obtain the equations
\[
[\omega^{2}(u\omega_{12}-v\bar\omega_{12})]=0,\qquad\text{and}\qquad [\omega^{1}(u\bar\omega_{12}-v\omega_{12})]=0.
\]
This permits us to write the six quadratic equations under the form
\begin{equation}
  \label{eq:7.IX.3}\tag{IX, 3}
  \left\{
  \begin{aligned}{}
    [\omega^{1}du]&=0,&[\omega^{2}dv]&=0,\\
    [\omega^{1}(u\bar\omega_{12}-v\omega_{12})]&=0,&[\omega^{2}(u\omega_{12}-v\bar\omega_{12})]&=0,\\
    [\omega^{1}da]+(a-c)[\omega^{2}\omega_{12}]&=0,\\
    [\omega^{2}dc]+(a-c)[\omega^{1}\omega_{12}]&=0.
  \end{aligned}
  \right.
\end{equation}

The equations \eqref{eq:7.IX.1}, \eqref{eq:7.IX.2}, \eqref{eq:7.IX.3} constitute the closed differential system of the first problem.

The determinant $\Delta$ of the polar matrix whose columns correspond to the forms $du,dv,\omega_{12},\bar\omega_{12},da,dc$ is, after calculation, given by
\begin{equation}
  \label{eq:7.IX.4}\tag{IX, 4}
  \Delta=(a-c)^{2}(u^{2}-v^{2})(\omega^{1})^{3}(\omega^{2})^{3}.
\end{equation}

The rank is equal to $6$, the number of quadratic equations \eqref{eq:7.IX.3}, the system is in involution and \emph{its general solution depends on six arbitrary functions of one variable.}

\emph{Singular solutions}. As $a-c$ is by hypothesis essentially non-zero, the singular solutions are those for which $u^{2}-v^{2}=0$, or $u=v$, since $u$ and $v$ are two essentially positive functions. The first two equations of \eqref{eq:7.IX.3} then give $du=0$ and the following two give $\bar\omega_{12}=\omega_{12}$, from which, by exterior differentiation, 
\[
[\bar\omega_{13}\bar\omega_{23}]=[\omega_{13}\omega_{23}]
\]
or
\[
(u^{2}-1)ac=0.
\]
If we leave aside the developable surfaces, we fine $u=1$, the equalities
\[
\bar\omega^{1}=\omega^{1},\qquad\bar\omega^{2}=\omega^{2},\qquad\bar\omega_{12}=\omega_{12},\qquad\bar\omega_{13}=\varepsilon\omega_{13},\qquad\bar\omega_{23}=\varepsilon\omega_{23}
\]
then show that the two surfaces are equal or symmetric: they are trivial solutions.

Hence we see that for the surfaces $S$ such that there exists one surface $\bar S$ susceptible to be in point correspondence with $S$ with the same the curvature lines without being either equal or symmetric to $S$, their mean curvature and the total curvature are exceptional. They form a class depending on six arbitrary functions of one variable.

The characteristics, according to \eqref{eq:7.IX.4}, are the curvature lines.

\vspace{12pt}\fsec \emph{The Cauchy problem}. Every one dimensional solution of equations \eqref{eq:7.IX.1} and \eqref{eq:7.IX.2} is constituted by two families of right trihedrals attached at two oriented curves $C$ and $\bar C$ and four functions $u,v,a,c$. Each trihedral is determined by the vector $\vec e_{3}$ normal to the corresponding curve and the angle $\theta$ between the positive tangent of the curve and the vector $\vec e_{1}$. We then have, according to \eqref{eq:7.14} and \eqref{eq:7.16} of \textsection\textbf{6}, pp.~\pageref{eq:7.14}, the relations
\begin{align*}
  d\bar s\cos\bar\theta&=u\,ds\cos\theta,&d\bar s\sin\bar\theta&=v\,ds\sin\theta,\\
  \frac{\cos\varpi}{\rho}&=a\cos^{2}\theta+c\sin^{2}\theta,&\frac{d\varpi}{ds}+\frac{1}{\tau}&=(c-a)\sin\theta\cos\theta,\\
  \frac{\cos\bar\varpi}{\bar\rho}&=\varepsilon(a\cos^{2}\bar\theta+c\sin^{2}\bar\theta),&\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}&=\varepsilon(c-a)\bar\sin\theta\cos\bar\theta.
\end{align*}

We will arbitrarily match the points between $C$ and $\bar C$, i.e., $d\bar s/ds=w$. We then have
\[
u=\frac{w\cos\bar\theta}{\cos\theta},\qquad v=\frac{w\sin\bar\theta}{\sin\theta},
\]
the functions $a,c,\theta,\bar\theta$ are determined by the last four equations, where the angles $\varpi$ and $\bar\varpi$ are known by the position of the vectors $\vec e_{3}$ of the trihedrals attached at the two curves. We see that the data depend on $7$ arbitrary functions of one variable (the two curves, the angles $\varpi$ and $\bar\varpi$ and the function $w$). This is in agreement with the degree of arbitrariness of the general solution of the problem, since given one solution of the problem, we can take the curve $C$ as the section of $S$ by a given fixed plane.

The Cauchy-Kowalewski theorem does not apply if the curve $C$ is a curvature line of $S$, i.e., if $\theta=0$ or $\pi/2$ ($\bar\theta$ then has the same value). Suppose for example $\theta=0$, $\omega^{2}=\bar\omega^{2}=0$. The equations \eqref{eq:7.IX.3} show that we must have, for the problem to be solvable, 
\[
dv=0,\qquad u\omega_{12}-v\bar\omega_{12}=0.
\]

The data in this case are the following. We must have
\begin{gather*}
  u=\frac{d\bar s}{ds},\qquad a=\frac{\cos\varpi}{\rho}=\varepsilon\frac{\cos\bar\varpi}{\bar\rho},\qquad\frac{d\varpi}{ds}+\frac{1}{\tau}=0,\qquad\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}=0,\\
  \frac{\sin\varpi}{\rho}=v\frac{\sin\bar\varpi}{\bar\rho},\qquad dv=0.
\end{gather*}

Specify, for example, $\varpi$ and $1/\rho$ as functions of $s$, the torsion $1/\tau$ of $C$ will be given by $1/\tau=-d\varpi/ds$. Specify $u$ as function of $s$ and $v$ the constant value $m$, we will have
\[
\tan\bar\varpi=\frac{\varepsilon}{m}\tan\varpi,\qquad\frac{1}{\bar\rho}=\varepsilon\frac{\cos\varpi}{\cos\bar\varpi}\frac{1}{\rho},\qquad\frac{1}{\bar\tau}=-\frac{1}{u}\frac{d\bar\varpi}{ds},
\]
which give us $\bar\varpi, 1/\bar\rho$ and $1/\bar\tau$ as functions of $\bar s$. Finally let us specify $c$ as arbitrary function of $s$. The characteristic solution thus obtained depends on four arbitrary functions of $s$, that is to say $\varpi,1/\rho,u$ and $c$. It is clear that there will exist one dimensional characteristic solutions which correspond to an infinite number of solutions of the given problem.


\vspace{12pt}\fsec \textsc{Particular case}: \emph{surfaces admitting a family of curvature lines formed by geodesics.} If the second family of curvature lines of a surface $S$ are by geodesics, we have a relation of the form
\begin{equation}
  \label{eq:7.IX.5}\tag{IX, 5}
  \omega_{12}=h\omega^{1},
\end{equation}
as $d\omega^{2}=[\omega^{1}\omega_{12}]=0$, the form $\omega^{2}$ is an exact differential $d\beta$. As we have already seen (\textsection\textbf{29}), the curvature lines of this family are planar and they are all equal. The surface $S$ is generated by a planar curve $\Gamma$ where the plane rolls without slipping on one fixed developable surface.

If one such surface belongs to the class we consider, the surface $\bar S$ will be given by the system
\begin{equation}
  \label{eq:7.IX.6}\tag{IX, 6}
  \left\{
    \begin{gathered}{}
      \bar\omega^{3}=0,\quad\bar\omega^{1}=u\omega^{1},\quad\bar\omega^{2}=v\omega^{2},\quad\bar\omega_{13}=\varepsilon a\bar\omega^{1},\quad\bar\omega_{23}=\varepsilon c\bar\omega_{2},\\
      [\omega^{1}du]=0,\quad[\omega^{2}dv]=0,\\
      [\omega^{1}(u\bar\omega_{12}-v\omega_{12})]=0,\quad[\omega^{1}(u\omega_{12}-v\bar\omega_{12})]=0.
    \end{gathered}
  \right.
\end{equation}

From the penultimate of these equations and the equation \eqref{eq:7.IX.5}, it follows that $\bar\omega_{12}$ is proportional to $\omega^{1}$ and the last equation \eqref{eq:7.IX.6} results in
\[
\bar\omega_{12}=\frac{u}{v}\omega_{12}=\frac{h}{v}\bar\omega^{1}.
\]

Therefore the surface $\bar S$, if it exists, also has its second family of curvature lines formed by geodesics. The functions $u$ and $v$ are then given by the system
\begin{equation}
  \label{eq:7.IX.7}\tag{IX, 7}
  [\omega^{1}du]=0,\qquad[\omega^{2}dv]=0,\qquad\bar\omega_{12}=\frac{u}{v}\omega_{12},
\end{equation}
from which, by exterior differentiation,
\begin{equation}
  \label{eq:7.IX.8}\tag{IX, 8}
  h[\omega^{1}dv]+acv(v^{2}-1)[\omega^{1}\omega^{2}]=0.
\end{equation}

We deduce the equations
\begin{equation}
  \label{eq:7.IX.9}\tag{IX, 9}
  dv=ac\frac{v(1-v^{2})}{h}\omega^{2}.
\end{equation}

As $[\omega^{2}dc]=0$, we see that there are two possible cases:

1. $[\omega^{2}d(a/h)]=0$. In this case the equation \eqref{eq:7.IX.9} is completely integrable and gives $v$ as a function dependent on one arbitrary constant. The equation $[\omega^{1}du]=0$ gives $u$ as an arbitrary function of the parameter of the curvature lines in the first family of the surface $S$. There is then an infinite number of surfaces $\bar S$ forming with $S$ a pair of surfaces satisfying the contions of the problem.

2. $[\omega^{2}d(a/h)]\neq 0$. In this case the equation \eqref{eq:7.IX.9} admit the unique solution $v=1$, and we can again take $u$ to be an arbitrary function of the parameter of the curvature lines in the first family of $S$, from which again there exists an infinite number of surfaces $\bar S$.

The first case is the case of \emph{moulding surfaces.}\index{moulding surface}



\vspace{12pt}\fsec\textsc{Second problem}. We start from equations \eqref{eq:7.IX.1}, to which we must add the equations
\begin{equation}
  \label{eq:7.IX.10}\tag{IX, 10}
  \omega_{13}=a\omega^{1},\quad\omega_{23}=c\omega^{2},\quad\bar\omega_{13}=\varepsilon c\bar\omega^{1},\quad\bar\omega_{23}=\varepsilon a\bar\omega^{2}\quad(\varepsilon=\pm 1)
\end{equation}
and the quadratic equations
\begin{equation}
  \label{eq:7.IX.11}\tag{IX, 11}
  \left\{
    \begin{aligned}{}
      [\omega^{1}du]+[\omega^{2}(u\omega_{12}-v\bar\omega_{12})]&=0,\\
      [\omega^{2}dv]+[\omega^{1}(u\bar\omega_{12}-v\omega_{12})]&=0,\\
      [\omega^{1}da]+(a-c)[\omega^{2}\omega_{12}]&=0,\\
      [\omega^{2}dc]+(a-c)[\omega^{1}\omega_{12}]&=0,\\
      [\bar\omega^{1}dc]-(a-c)[\bar\omega^{2}\bar\omega_{12}]&=0,\\
      [\bar\omega^{2}da]-(a-c)[\bar\omega^{1}\bar\omega_{12}]&=0.
    \end{aligned}
  \right.
\end{equation}

The determinant of the polar matrix whose columns correspond to the forms $du$, $dv$, $\omega_{12}$, $\bar\omega_{12}$, $da$, $dc$ respectively is
\[
\begin{vmatrix}
  \omega^{1}&0&u\omega^{2}&-v\omega^{2}&0&0\\
  0&\omega^{2}&-v\omega^{1}&u\omega^{1}&0&0\\
  0&0&(a-c)\omega^{2}&0&\omega^{1}&0\\
  0&0&(a-c)\omega^{1}&0&0&\omega^{2}\\
  0&0&0&-(a-c)\bar\omega^{2}&0&\bar\omega^{1}\\
  0&0&0&-(a-c)\bar\omega^{1}&\bar\omega^{2}&0
\end{vmatrix}
=\omega^{1}\omega^{2}(a-c)^{2}[(\omega^{1})^{2}(\bar\omega^{1})^{2}-(\omega^{2})^{2}(\bar\omega^{2})^{2}].
\]

The rank of the matrix is equal to $6$, the number of quadratic equations \eqref{eq:7.IX.11}, the system is in involution and \emph{its general solution depends on six arbitrary functions of one variable}. As $a-c$ as well as the functions $u$ and $v$ are essentially different from zero, there does not exist any singular solution. The \emph{characteristics} are the curvature lines and the two families of lines given by
\[
\omega^{1}\bar\omega^{1}-\omega^{2}\bar\omega^{2}=0\qquad\text{or}\qquad u(\omega^{1})^{2}-v(\omega^{2})^{2}=0.
\]
There are two other families of characteristics, but they are imaginary.

\vspace{12pt}\fsec \emph{The Cauchy problem}. The formulae obtained in the first problem for the formulation of the Cauchy problem are here replaced by the following
\begin{equation}
  \label{eq:7.IX.12}\tag{IX, 12}
  \left\{
    \begin{aligned}
      d\bar s\cos\bar\theta&=u\,ds\cos\theta,&d\bar s\sin\bar\theta&=v\,ds\sin\theta,\\
      \frac{\cos\varpi}{\rho}&=a\cos^{2}\theta+c\sin^{2}\theta,&\frac{d\varpi}{ds}+\frac{1}{\tau}&=(c-a)\sin\theta\cos\theta,\\
      \frac{\cos\bar\varpi}{\bar\rho}&=\varepsilon(c\cos^{2}\bar\theta+a\sin^{2}\bar\theta),&\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}&=\varepsilon(a-c)\sin\bar\theta\cos\bar\theta.\\
    \end{aligned}
  \right.
\end{equation}

We specify arbitrarily two curves $C$ and $\bar C$, the angles $\varpi$ and $\bar\varpi$ and the point correspondance between them, i.e., the function $d\bar s/d s=w$. We then have
\[
u=w\frac{\cos\bar\theta}{\cos\theta},\qquad v=w\frac{\sin\bar\theta}{\sin\theta},
\]
the functions $a,c,\theta,\bar\theta$ are then determined by the last four relations.

The Cauchy-Kowalewski theorem does not apply if either the curve $C$ is the curvature line of $S$, or if for example $\sqrt{u}\cos\theta=\sqrt{v}\sin\theta$.

Let us first examine the first case and suppose, to fix the ideas, $\theta=0$, from which $\bar\theta=0$. To the equations
\[
d\bar s=u\,ds,\qquad a=\frac{\cos\varpi}{\rho},\qquad c=\varepsilon\frac{\cos\bar\varpi}{\bar\rho},\qquad \frac{d\varpi}{ds}+\frac{1}{\tau}=0,\qquad\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar \tau}=0,
\]
we should add new equations necessary for the problem to be solvable. From the equations \eqref{eq:7.IX.11} we deduce
\[
\left[\omega^{2}\left(da+dc+\overline{a-c}\frac{dv}{v}\right)\right]=0,
\]
we must therefore have, along $C$,
\begin{equation}
  \label{eq:7.IX.13}\tag{IX, 13}
  \frac{dv}{v}+\frac{da+dc}{a-c}=0.
\end{equation}

Let us specify, for example, $u, \varpi$ and $1/\rho$ as functions of $s$, the curve $C$ will then be determined by its curvature and its torsion equal to $-d\varpi/ds$. The function $\bar s$ known, if we specify $\bar\varpi$ and $1/\bar\rho$ as functions of $\bar s$, the torsion $1/\bar\tau$ will be determined. As for the function $v$, it is determined up to a constant factor. The data hence depend on five arbitrary functions of one variable.

Let us now examine the second case, where we suppose $\sqrt{u}\cos\theta=\sqrt{v}\sin\theta$, or $\sqrt{u}\omega^{1}=\sqrt{v}\omega^{2}$. From equations \eqref{eq:7.IX.11} we get the equation
\[
[(\sqrt{u}\omega^{1}-\sqrt{v}\omega^{2})(v\,da+u\,dc+\overline{a-c}\sqrt{uv}(\bar\omega_{12}-\omega_{12}))]=0.
\]

To the equations \eqref{eq:7.IX.12} we should add the equation
\begin{equation}
  \label{eq:7.IX.14}\tag{IX, 14}
  v\,da+u\,dc+(a-c)\sqrt{uv}(\bar\omega_{12}-\omega_{12})=0.
\end{equation}
We can then specify the curve $C$ and the functions $\varpi,u,v$ as functions of $s$. We have
\[
\tan\theta=\sqrt{\frac{u}{v}},\qquad\tan\bar\theta=\sqrt{\frac{v}{u}},\qquad d\bar s=\sqrt{uv}\,ds,
\]
from which we obtain $\theta,\bar\theta,\bar s$. As for the functions $a$ and $c$, they are given by the equations
\[
\frac{av+cu}{u+v}=\frac{\cos\varpi}{\rho},\qquad(c-a)\frac{\sqrt{uv}}{u+v}=\frac{d\varpi}{ds}+\frac{1}{\tau},
\]
finally the curve $\bar C$ and the angle $\bar\varpi$ are given by the equations
\[
\frac{\cos\bar\varpi}{\bar\rho}=\varepsilon\frac{cu+av}{u+v},\qquad\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}=\varepsilon(a-c)\frac{\sqrt{uv}}{u+v}
\]
and the value of $(\sin\bar\varpi)/\bar\rho$ will be given by the equation \eqref{eq:7.IX.14}. The data hence depend on five arbitrary functions of one variable.

We leave aside the determination of the surface $\bar S$ when we know the surface $S$.


\subsection[{Two surfaces in point correspondence with the same geodesic torsion of curves}]{Two surfaces in point correspondence\\with the same geodesic torsion of curves}
\label{sec:some-surfaces-point-1}

\fsec We exclude the trivial cases  of the planes and the spheres. The geodesic torsion being the expression $(\omega^{1}\omega_{23}-\omega^{2}\omega_{13})/((\omega^{1})^{2}+(\omega^{2})^{2})$, each of the forms in the numerator and denominator must, on passing from the first surface $S$ to the second surface $\bar S$, remain proportional to itself multiplied by the same factor (up to sign). The two surfaces are therefore in conformal correspondence. Let us attach at different points of $S$ right trihedrals subject to the condition that the vector $\vec e_{3}$ is normal to the surface. We attach at corresponding points of $\bar S$, without ambiguity, the corresponding trihedrals in a such way to have
\[
\bar\omega^{1}=u\omega^{1},\qquad\bar\omega^{2}=u\omega^{2},\qquad(y>0),
\]
we then must have
\[
\omega^{1}\bar\omega_{23}-\omega^{2}\bar\omega_{13}=\varepsilon u(\omega^{1}\omega_{23}+\omega^{2}\omega_{13}).
\]

Finally the closed differential system of the problem asked is formed by the equations
\begin{equation}
  \label{eq:7.X.1}\tag{X, 1}
  \left\{
    \begin{gathered}
      \omega^{3}=0,\qquad\bar\omega^{3}=0,\qquad\bar\omega^{1}=u\omega^{1},\qquad\bar\omega^{2}=u\omega^{2},\\
      \bar\omega_{13}=\varepsilon u(\omega_{13}+v\omega^{1}),\qquad\bar\omega_{23}=\varepsilon u(\omega_{23}+v\omega^{2}),
    \end{gathered}
  \right.
\end{equation}
and the quadratic exterior equations
\begin{equation}
  \label{eq:7.X.2}\tag{X, 2}
  \left\{
    \begin{aligned}{}
      [\omega^{1}\omega_{13}]+[\omega^{2}\omega_{23}]&=0,\\
      \left[\omega^{1}\frac{du}{u}\right]+[\omega^{2}(\omega_{12}-\bar\omega_{12})]&=0,\\
      \left[\omega^{2}\frac{du}{u}\right]-[\omega^{2}(\omega_{12}-\bar\omega_{12})]&=0,\\
      [\omega^{1}dv]+\left[\omega_{13}\frac{du}{u}\right]+[\omega_{23}(\omega_{12}-\bar\omega_{12})]&=0,\\
      [\omega^{2}dv]+\left[\omega_{23}\frac{du}{u}\right]-[\omega_{13}(\omega_{12}-\bar\omega_{12})]&=0.
    \end{aligned}
  \right.
\end{equation}

The two dimensional generic integral element is given by the equations
\begin{equation}
  \label{eq:7.X.3}\tag{X, 3}
  \left\{
    \begin{aligned}
      \omega_{13}&=a\omega^{1}+b\omega^{2},\\
      \omega_{23}&=b\omega^{1}+c\omega^{2},\\
      \frac{du}{u}&=\alpha\omega^{1}+\beta\omega^{2},\\
      \omega_{12}-\bar\omega_{12}&=\beta\omega^{1}-\alpha\omega^{2},\\
      dv&=[(a-c)\alpha+2b\beta]\omega^{1}+[2b\alpha+(c-a)\beta]\omega^{2},
    \end{aligned}
  \right.
\end{equation}
it depends on five arbitrary parameters $a,b,c,\alpha,\beta$. The system is in involution and the general solution depends on five arbitrary functions of one variable if the determinant of the polar matrix is not identically zero. For this determinant, whose columns correspond to the forms $\omega_{13},\omega_{23},du/u,\omega_{12}-\bar\omega_{12},dv$, is
\[
\begin{vmatrix}
  \omega^{1}&\omega^{2}&0&0&0\\
  0&0&\omega^{1}&\omega^{2}&0\\
  0&0&\omega^{2}&-\omega^{1}&0\\
  -\dfrac{du}{u}&\bar\omega_{12}-\omega_{12}&\omega_{13}&\omega_{23}&\omega^{1}\\
  \omega_{12}-\bar\omega_{12}&-\dfrac{du}{u}&\omega_{23}&-\omega_{13}&\omega^{2}
\end{vmatrix}
=\frac{du}{u}[(\omega^{1})^{2}+(\omega^{2})^{2}]^{2}.
\]
It is not identically zero. The real characteristics of the general solutions form a single family characterised by the constant  function $u$.

\vspace{12pt}\fsec\emph{The Cauchy problem}. Let us study the Cauchy problem of the general solutions, leaving aside for the moment the singular solutions of the current problem. We can assume that for the one dimensional solutions (Cauchy data) of the system \eqref{eq:7.X.1}, we have $\omega^{2}=0$. We then have for the pair of corresponding curves $C,\bar C$, and the pair of circumscribed developable surfaces $\Sigma, \bar\Sigma$,
\[
d\bar s=u\,ds,\qquad\frac{\cos\bar\varpi}{\bar\rho}=\varepsilon\left(\cos\frac{\varpi}{\rho}+v\right),\qquad\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar \tau}=\varepsilon\left(\frac{d\varpi}{ds}+\frac{1}{\tau}\right).
\]

More simply we can specify arbitrarily the two curves and the two developable surfaces. The point correspondance between the two curves are that which realises the equality, up to the sign $\varepsilon$, of the geodesic torsion. We hence have the function $u=d\bar s/ds$ along the curve, and the function $v$ will be the difference $(\varepsilon=1)$ or the sum $(\varepsilon=-1)$ of the normal curvatures at two correspondant points of the two curves. The data depend on six arbitrary functions, which reduce to five if we take $C$ as the section of the surface $S$ by a given plane.

The Cauchy-Kowalewski theorem does not apply if the data are characteristics, i.e., if the function $u$ is constant, equal to $m$ for example. In this case there is a supplementary condition for the data. The coefficient $\alpha$ of the formulae \eqref{eq:7.X.3} being zero, we must have along the curve $C$
\[
dv=2b(\omega_{12}-\bar\omega_{12}),
\]
or
\[
\frac{dv}{ds}=2\left(\frac{d\varpi}{ds}+\frac{1}{\tau}\right)\left(\frac{\sin\varpi}{\rho}-m\frac{\sin\bar\varpi}{\bar\rho}\right).
\]

The data then depend on no more than four arbitrary functions of one variable. Given the curve $C$ and the developable surface $\Sigma$ (three arbitrary functions), the functions $1/\bar\rho,1/\bar\tau,\bar\varpi$ of $\bar s$ are linked by one algebraic relation and one differential equation.

If the surface $S$ is given, the surface $\bar S$ is determined by the system formed by the last five equations of \eqref{eq:7.X.1} and the last four equations of \eqref{eq:7.X.2}. The system is not in involution. In the most favourable case the surface $\bar S$ depends on five arbitrary constant up to translation. Again we have to check in this case that the solutions actually exist.

\vspace{12pt}\fsec\textsc{Singular solutions}. The singular solutions of the system \eqref{eq:7.X.1}, \eqref{eq:7.X.2} are those for which the function $u$ is a constant $m$. The equations \eqref{eq:7.X.2} then give
\[
\bar\omega_{12}-\omega_{12}=0,\qquad dv=0,\qquad(v=n).
\]
and, by exterior differentiation,
\[
[\bar\omega_{13}\bar\omega_{23}]-[\omega_{13}\omega_{23}]=0.
\]

The closed differential system which gives the singular solutions is then
\begin{equation}
  \label{eq:7.X.4}\tag{X, 4}
  \left\{
    \begin{gathered}
      \omega^{3}=0,\qquad\bar\omega^{3}=0,\qquad\bar\omega^{1}=m\omega^{1},\qquad\bar\omega^{2}=m\omega^{2},\\
      \bar\omega_{12}=\bar\omega_{12},\qquad\bar\omega_{13}=\varepsilon m(\omega_{13}+n\omega^{1}),\qquad\bar\omega_{23}=\varepsilon m(\omega_{23}+n\omega^{2}),\\
      [\omega^{1}\omega_{13}]+[\omega^{2}\omega_{23}]=0,\\
      [\bar\omega_{13}\bar\omega_{23}]-[\omega_{13}\omega_{23}]=0.
    \end{gathered}
  \right.
\end{equation}

The system is in involution and its general solution depends on two arbitrary functions of one argument. The characteristics are given by the equation
\[
\bar\omega^{1}\bar\omega_{13}+\bar\omega^{2}\bar\omega_{23}=\omega^{1}\omega_{13}+\omega^{2}\omega_{23},
\]
or
\[
(m^{2}-1)(\omega^{1}\omega_{13}+\omega^{2}\omega_{23})+m^{2}n[(\omega^{1})^{2}+(\omega^{2})^{2}]=0.
\]

The last equation of \eqref{eq:7.X.4} shows that the surfaces $S$ are the Weingarten surfaces satisfying the relation
\[
\frac{(m^{2}-1)}{R_{1}R_{2}}+m^{2}n\left(\frac{1}{R_{1}}+\frac{1}{R_{2}}\right)+m^{2}n^{2}=0.
\]
The corresponding surfaces $\bar S$ satisfy one analogous relation where $m$ is replaced by $1/m$ and $n$ by $-\varepsilon n$. If the surface $S$ is given, the constants $m$ and $n$ are given, and the corresponding surfaces $\bar S$ are given by the completely integrable system
\begin{gather}
  \label{eq:7.X.5}\tag{X, 5}
  \bar\omega{^3}=0,\qquad\bar\omega^{1}=m\omega^{1},\qquad\bar\omega^{2}=m\omega^{2},\qquad\bar\omega_{12}=\omega_{12},\\
  \bar\omega_{13}=\varepsilon m(\omega_{13}+n\omega^{1}),\qquad\bar\omega_{23}=\varepsilon m(\omega_{23}+n\omega^{2}),\notag
\end{gather}
these surfaces $\bar S$ are completely determined up to a translation or a symmetry.

A singular solution of the system \eqref{eq:7.X.1}, \eqref{eq:7.X.2}, \eqref{eq:7.X.3} is completely determined by the data of two curves $C,\bar C$ and two circumscribed developable surfaces $\Sigma,\bar\Sigma$ satisfying the conditions
\begin{align*}
d\bar s&=m\,ds,&\frac{\sin\bar\varpi}{\bar\rho}&=\frac{\sin\varpi}{\rho},\\
\frac{\cos\bar\varpi}{\bar\rho}&=\varepsilon\left(\frac{\cos\varpi}{\rho}+n\right),&\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}&=\varepsilon\left(\frac{d\varpi}{ds}+\frac{1}{\tau}\right).
\end{align*}

These data will be characteristic if we have
\[
\frac{\cos\varpi}{\rho}=\frac{m^{2}n}{1-m^{2}},\qquad\frac{\cos\bar\varpi}{\bar\rho}=\varepsilon\frac{n}{1-m^{2}}.
\]


\subsection[Surfaces having the same third fundamental form as a given surface]{Surfaces having the same \\third fundamental form as a given surface}
\label{sec:surfaces-having-same}

\fsec In this case the curvature lines  correspond to each other on the two surfaces, and we attach at each point of the given surface $S$ a Darboux right trihedral: this will correspond a Darboux trihedral at the corresponding point of the surface $\bar S$ sought after for which we have
\begin{equation}
  \label{eq:7.XI.1}\tag{XI, 1}
  \bar\omega^{3}=0,\qquad\bar\omega^{1}=u\omega^{1},\qquad\bar\omega^{2}=v\omega^{2},\qquad(u>0,\ v>0),
\end{equation}
the relation
\[
\bar\omega^{1}\bar\omega_{23}-\bar\omega^{2}\bar\omega_{13}=\omega^{1}\omega_{23}-\omega^{2}\omega_{13}
\]
leads us to investigate, denoting by $a$ and $c$ the principal curvatures of $S$,
\begin{equation}
  \label{eq:7.XI.2}\tag{XI, 2}
  \bar\omega_{13}=\frac{\omega_{13}+w\omega^{1}}{v}=\frac{a+w}{v}\omega^{1},\qquad\bar\omega_{23}=\frac{\omega_{23}+w\omega^{2}}{u}=\frac{c+w}{u}\omega^{2}.
\end{equation}

The exterior differentiation of \eqref{eq:7.XI.1} and \eqref{eq:7.XI.2} gives the exterior quadratic equations
\begin{align*}
  [\omega^{1}du]+[\omega^{2}(u\omega_{12}-v\bar\omega_{12})]&=0,\\
  [\omega^{2}dv]+[\omega^{1}(u\bar\omega_{12}-v\omega_{12})]&=0,\\
  \left[\omega^{1}\left(dw-\frac{c+w}{u}du-\frac{a+w}{v}dv\right)\right]&=0,\\
  \left[\omega^{2}\left(dw-\frac{c+w}{u}du-\frac{a+w}{v}dv\right)\right]&=0.
\end{align*}

It follows from this the relation
\[
dw=\frac{c+w}{u}du+\frac{a+w}{v}dv
\]
which, under exterior differentiation, gives
\[
\left[dc\frac{du}{u}\right]+\left[da\frac{dv}{v}\right]-(a-c)\left[\frac{du}{u}\frac{dv}{v}\right]=0.
\]

Finally, the closed differential system of the problem is
\begin{equation}
  \label{eq:7.XI.3}\tag{XI, 3}
  \left\{
    \begin{gathered}
      \bar\omega^{3}=0,\qquad\bar\omega^{1}=u\omega^{1},\qquad\bar\omega^{2}=v\omega^{2},\\
      \bar\omega_{13}=\frac{a+w}{v}\omega^{1},\qquad\bar\omega_{23}=\frac{c+w}{u}\omega^{2},\\
      dw=\frac{c+w}{u}du+\frac{a+w}{v}dv,\\
      [\omega^{1}du]+[\omega^{2}(u\omega_{12}-v\bar\omega^{12})]=0,\\
      [\omega^{2}dv]+[\omega^{1}(u\bar\omega_{12}-v\omega^{12})]=0,\\
      \left[dc\frac{du}{u}\right]+\left[da\frac{dv}{v}\right]-(a-c)\left[\frac{du}{u}\frac{dv}{v}\right]=0.
    \end{gathered}
  \right.
\end{equation}

The generic two dimensional integral element, by setting
\begin{equation}
  \label{eq:7.XI.4}\tag{XI, 4}
  \omega_{12}=h\omega^{1}+k\omega^{2},\quad da=a_{1}\omega^{1}+(a-c)h\omega^{2},\quad dc=(a-c)k\omega^{1}+c_{2}\omega^{2},
\end{equation}
is given by
\begin{equation}
  \label{eq:7.XI.5}\tag{XI, 5}
  \left\{
    \begin{aligned}
      du&=\alpha\omega^{1}+\beta\omega^{2},\qquad dv=\gamma\omega^{1}+\delta\omega^{2},\\
      \bar\omega_{12}&=\frac{uh-\beta}{v}\omega^{1}+\frac{vk+\gamma}{u}\omega^{2},
    \end{aligned}
  \right.
\end{equation}
where the coefficients $\alpha,\beta,\gamma,\delta$ are related by the relation
\[
-(a-c)(\alpha\delta-\beta\gamma)+(a-c)(vk\beta-uh\gamma)+ua_{1}\delta-vc_{2}\alpha=0.
\]

The two dimensional integral element therefore depends on three arbitrary parameters.

As the quadratic equations \eqref{eq:7.XI.3} have three equations, the system is in involution and \emph{its general solution depends on three arbitrary functions of one argument.}

The characteristics annihilate the determinant of the polar matrix
\begin{align}
  \label{eq:7.XI.6}\tag{XI, 6}
  &\quad
  \begin{vmatrix}
    u\omega^{1}&0&-v\omega^{2}\\
    0&v\omega^{2}&u\omega^{1}\\
    dc+(a-c)\dfrac{dv}{v}&da-(a-c)\dfrac{du}{u}&0
  \end{vmatrix}\\
  &=u^{2}\left(da-\frac{a-c}{u}du\right)(\omega^{1})^{2}-v^{2}\left(dc+\frac{a-c}{v}dv\right)(\omega^{2})^{2}.\notag
\end{align}

\vspace{12pt}\fsec\emph{Singular solutions}. They are given by the two supplementary equations
\[
\frac{du}{u}=\frac{da}{a-c},\qquad\frac{dv}{v}=\frac{dc}{c-a}
\]
which, by exterior differentiation, leads to the condition
\[
[da\,dc]=0
\]
which the given surface $S$ must satisfy. \emph{It expresses that $S$ is a Weingarten surface.} If this is the case, we find, according to \eqref{eq:7.XI.4} and \eqref{eq:7.XI.5},
\begin{align*}
  \frac{u_{2}}{u}&=\frac{\beta}{u}=\frac{a_{2}}{a-c}=h,&\text{from which}&&uh&=\beta,\\
  \frac{v_{1}}{v}&=\frac{\gamma}{v}=\frac{c_{1}}{c-a}=-k,&\text{from which}&&vk+\gamma&=0.\\
\end{align*}
We then have $\bar\omega_{12}=0$, from which $[\bar\omega_{13}\bar\omega_{23}]=0$. This relation expresses that \emph{the surface $\bar S$ is developable}. The expressions of $\bar\omega_{13}$ and $\bar\omega_{23}$ given by the equations \eqref{eq:7.XI.3} then show that $(w+a)\times(w+c)=0$, for example
\[
w=-a,\qquad\bar\omega_{13}=0.
\]

We have, for determining the surface $\bar S$, the system
\begin{equation}
  \label{eq:7.XI.7}\tag{XI, 7}
  \frac{du}{u}=\frac{da}{a-c},\quad\frac{dv}{v}=\frac{dc}{c-a},\quad\bar\omega_{13}=0,\quad\bar\omega_{23}=\frac{c-a}{u}\omega^{2},\quad\bar\omega_{12}=0.
\end{equation}
This system is completely integrable. \emph{Therefore if $S$ is a Weingarten surface, the system \eqref{eq:7.XI.3} admits singular solutions, constituted by a family of developable surfaces (cylinders of revolution) depending on arbitrary constants.}

\vspace{12pt}\fsec\emph{General solution, the Cauchy problem}. Consider a curve $C$ on the surface $S$ and the trihedrals attached on the curve. We have a one dimensional solution of the system \eqref{eq:7.XI.3} giving us a curve $\bar C$ which must correspond to $C$, together with the developable surface $\bar\Sigma$ circumscribed to $\bar S$ along $\bar C$. We have, by denoting $\theta$ the angle between $C$ and the first principal tangent of $S$ and by $\bar\theta$ the analogous angle relative to $\bar C$,
\begin{equation}
  \label{eq:7.XI.8}\tag{XI, 8}
  \left\{
    \begin{aligned}
      d\bar s\cos\bar\theta&=u\,ds\cos\theta,\qquad d\bar s\sin\bar\theta=v\,ds\sin\theta,\\
      \frac{1}{\bar R_{n}}&=\frac{a+w}{uv}\cos^{2}\bar\theta+\frac{c+w}{uv}\sin^{2}\bar\theta\\
      &=\frac{ds^{2}}{d\bar s^{2}}\left[\frac{u}{v}(a+w)\cos^{2}\theta+\frac{v}{u}(c+w)\sin^{2}\theta\right],\\
      \frac{1}{\bar T_{g}}&=\frac{c-a}{uv}\sin\bar\theta\cos\bar\theta=\frac{ds^{2}}{d\bar s^{2}}\frac{1}{T_{g}},\\
      \frac{dw}{ds}&=\frac{c+w}{u}\frac{du}{ds}+\frac{a+w}{v}\frac{dv}{ds}.
    \end{aligned}
  \right.
\end{equation}

In these equations $1/R_{n},1/T_{g},\theta,a,c$ are known functions of $s$. Moreover we know the curvature $1/\bar\rho$, the torsion $1/\bar\tau$, as well as the angle $\bar\varpi$ as functions of $\bar s$. We still have to determine the functions $u,v,w$ at different points of $\bar C$. We  first establish the point correspondance between $C$ and $\bar C$ by the equation
\[
\frac{d\bar s^{2}}{\bar T_{g}}=\frac{ds^{2}}{T_{g}}.
\]

We have then
\begin{align*}
  u^{2}\cos^{2}\theta+v^{2}\sin^{2}\theta&=\frac{d\bar s^{2}}{ds^{2}},\\
  u^{2}(a+w)\cos^{2}\theta+v^{2}(c+w)\sin^{2}\theta&=\frac{uv}{\bar R_{n}}\frac{d\bar s^{2}}{ds^{2}}.
\end{align*}
Finally we have the differential equation furnished by the last equation of \eqref{eq:7.XI.8} which will completely determine $u, v, w$ up to an arbitrary constant. The data therefore involve three arbitrary functions, which is in accordance with the results obtained above.

We leave aside the question of determining the data with respect to a characteristic.


\subsection[{Two surfaces in conformal correspondence with the same asymptotic lines}]{Two surfaces in conformal correspondence\\with the same asymptotic lines}
\label{sec:some-surf-conf}

\fsec It is obvious that the conformal correspondance with the same asymptotic lines conserves the curvature lines. By attaching at each point of the first surface $S$ the most general right trihedrals whose vector $\vec e_{3}$ is normal to $S$, there will be corresponding trihedrals attached at $\bar S$ without ambiguity and we have the relations
\[
\omega^{3}=0,\qquad\bar\omega^{3}=0,\qquad\bar\omega^{1}=u\omega^{1},\qquad\bar\omega^{2}=u\omega^{2},\qquad(u>0).
\]
We then have
\[
\bar\omega^{1}\bar\omega_{13}+\bar\omega^{2}\bar\omega_{23}=uv(\omega^{1}\omega_{13}+\omega^{2}\omega_{23}),
\]
from which
\[
\bar\omega_{13}=v\omega_{13}+w\omega_{2},\qquad\bar\omega_{23}=v\omega_{23}-w\omega^{1},
\]
but the relation $[\bar\omega^{1}\bar\omega_{13}]+[\bar\omega^{2}\bar\omega_{23}]=0$ requires $w=0$.

The closed differential system of the problem is therefore
\begin{equation}
  \label{eq:7.XII.1}\tag{XII, 1}
  \left\{
    \begin{gathered}
      \omega^{3}=0,\quad\bar\omega^{3}=0,\quad\bar\omega^{1}=u\omega^{1},\quad\bar\omega^{2}=u\omega^{2},\\
      \bar\omega_{13}=v\omega_{13},\quad\bar\omega_{23}=v\omega_{23},\\
      [\omega^{1}\omega_{13}]+[\omega^{2}\omega_{23}]=0,\\
      \left[\omega^{1}\frac{du}{u}\right]+[\omega^{2}(\omega_{12}-\bar\omega_{12})]=0,\\
      \left[\omega^{2}\frac{du}{u}\right]-[\omega^{1}(\omega_{12}-\bar\omega_{12})]=0,\\
      \left[\omega_{13}\frac{dv}{v}\right]+[\omega_{23}(\omega_{12}-\bar\omega_{12})]=0,\\
      \left[\omega_{23}\frac{dv}{v}\right]-[\omega_{23}(\omega_{12}-\bar\omega_{12})]=0.
    \end{gathered}
  \right.
\end{equation}

The most general two dimensional integral element is given by
\begin{equation}
  \label{eq:7.XII.2}\tag{XII, 2}
  \left\{
    \begin{aligned}
      \omega_{13}&=a\omega^{1}+b\omega^{2},\\
      \omega_{23}&=b\omega^{1}+c\omega^{2},\\
      \omega_{12}-\bar\omega_{12}&=\beta\omega^{1}-\alpha\omega^{2},\\
      \frac{du}{u}&=\alpha\omega^{1}+\beta\omega^{2},\\
      \frac{dv}{v}&=\lambda\omega^{1}+\mu\omega^{2}.
    \end{aligned}
  \right.
\end{equation}
with the relations
\begin{equation}
  \label{eq:7.XII.3}\tag{XII, 3}
  \left\{
    \begin{aligned}
      a\alpha+b\beta-c\lambda+b\mu&=0,\\
      b\alpha+c\beta+b\lambda-a\mu&=0.
    \end{aligned}
  \right.
\end{equation}
It therefore depends on five arbitrary parameters. As $5$ is the number of independent quadratic equations of the system \eqref{eq:7.XII.1}, the system is in involution and \emph{its general solution depends on five arbitrary functions of one variable.}

The characteristics are obtained by setting to zero the determinant of the polar matrix. This matrix, whose columns correspond to the forms $\omega_{13},\omega_{23},\omega_{12}-\bar\omega_{12},du/u,dv/v$, is
\begin{align*}
  &\quad
  \begin{vmatrix}
    \omega^{1}&\omega^{2}&0&0&0\\
    0&0&\omega^{2}&\omega^{1}&0\\
    0&0&-\omega^{1}&\omega^{2}&0\\
    -\dfrac{dv}{v}&\bar\omega_{12}-\omega_{12}&\omega_{23}&0&\omega_{13}\\
    \omega_{12}-\bar\omega_{12}&-\dfrac{dv}{v}&-\omega_{13}&0&\omega_{23}
  \end{vmatrix}\\
&=[(\omega^{1})^{2}+(\omega^{2})^{2}]\left\{\frac{dv}{v}(\omega^{1}\omega_{13}+\omega^{2}\omega_{23})-(\omega_{12}-\bar\omega_{12})(\omega^{1}\omega_{23}-\omega^{2}\omega_{13})\right\}.
\end{align*}

Taking into account equations \eqref{eq:7.XII.2}, the determinant of the polar matrix is
\begin{gather}
  \label{eq:7.XII.4}\tag{XII, 4}
  [(\omega^{1})^{2}+(\omega^{2})^{2}]\{(a\lambda-b\beta)(\omega^{1})^{3}+(2b\lambda+a\mu+b\alpha+\overline{a-c}\beta)(\omega^{1})^{2}\omega^{2}\\
  +(c\lambda+2b\mu+\overline{c-a}\alpha+b\beta)\omega^{1}(\omega^{2})^{2}+(c\mu-b\alpha)(\omega^{2})^{3}\}.\notag
\end{gather}


\vspace{12pt}\fsec\emph{Singular solutions}. They are obtained by adjoining to the equations of the system the four linear equations in $\alpha,\beta,\lambda,\mu$
\begin{equation}
  \label{eq:7.XII.5}\tag{XII, 5}
  \left\{
    \begin{aligned}
      a\lambda-b\beta&=0,\\
      2b\lambda+a\mu+b\alpha+(a-c)\beta&=0,\\
      c\lambda+2b\mu+(c-a)\alpha+b\beta&=0,\\
      c\mu-b\alpha&=0,
    \end{aligned}
  \right.
\end{equation}
the determinant of the coefficients of these equations is
\[
(b^{2}-ac)[4b^{2}+(a-c)^{2}].
\]

Several cases can be distinguished.

1. \emph{The determinant is non-zero.} We then have $\alpha=\beta=\lambda=\mu=0$. It follows, according to \eqref{eq:7.XII.2},
\[
\frac{du}{u}=0,\qquad\frac{dv}{v}=0,\qquad\omega_{12}=\bar\omega_{12},
\]
from which, by exterior differentiation,
\[
[\bar\omega_{13}\bar\omega_{23}]-[\omega_{13}\omega_{23}]=0,\qquad\text{or}\qquad(v^{2}-1)[\omega_{13}\omega_{23}]=0.
\]

As $b^{2}-ac$ is assumed to be non-zero, we see that $v^{2}=1$, then the two surfaces are directly or inversely similar: this is a trivial solution.

2. \emph{The determinant is zero, with $b^{2}-ac\neq0$}. We then have $a=c,b=0$. The two surfaces are the planes $(a=0)$ or the spheres. In the first case the solution is trivial, and in the second case $(a\neq 0)$ we also have, according to \eqref{eq:7.XII.5}, $\lambda=\mu=0$, from which $dv=0$. These surfaces are any two sphere, this is obviously a solution of the problem.

3. \emph{The determinant is zero, with $b^{2}-ac=0$}. The two surfaces are developable. Given the surface $S$, we can assume the trihedrals are chosen in a manner to have $b=c=0,a\neq 0$. We then have according to \eqref{eq:7.XII.5} and \eqref{eq:7.XII.3}
\[
\alpha=\beta=\lambda=\mu=0.
\]
The functions $u,v$ are therefore constants and $\bar\omega_{12}=\omega_{12}$. The surface $\bar S$ is then given by the completely integrable system
\begin{gather}
  \label{eq:7.XII.6}\tag{XII, 6}
  \bar\omega^{3}=0,\quad\bar\omega^{1}=m\omega^{1},\quad\bar\omega^{2}=m\omega^{2},\\
  \bar\omega_{12}=\omega_{12},\quad\bar\omega_{13}=n\omega_{13},\quad\bar\omega_{23}=0,\notag
\end{gather}
where $m$ and $n$ are constants.

There exists between the curvilinear abscissas for the vertical edges $\Gamma,\bar\Gamma$ of the two surfaces $S, \bar S$ the relation $\bar s=ms$. If the curvature $1/\rho$ of $\Gamma$ is equal to $\varphi(s)$, the curvature $1/\bar\rho$ of $\bar\Gamma$ at the corresponding point is equal to $(1/m)\varphi(\bar s/m)$, and the torsion of $\bar\Gamma$ results from the torsion of $\Gamma$ at the corresponding point by multiplication by $n$.

\vspace{12pt}\fsec\emph{General solution. The Cauchy problem.} For every one dimensional solution of the system \eqref{eq:7.XII.1}, we can assume $\omega^{2}=0$. Continuing to use the usual notation, we have for two arbitrarily given corresponding curves $C$ and $\bar C$ as well as the two circumscribed developable surfaces $\Sigma$ and $\bar\Sigma$,
\[
d\bar s=u\,ds,\quad\frac{\cos\bar\varpi}{\bar\rho}d\bar s=v\frac{\cos\varpi}{\rho}ds,\quad\left(\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}\right)d\bar s=v\left(\frac{d\varpi}{ds}+\frac{1}{\tau}\right)ds.
\]

We know $1/\rho,1/\tau$ and $\varpi$ as functions of $s$, and $1/\bar\rho,1/\bar\tau$ and $\bar\varpi$ as functions of $\bar s$. We then have the relation between $\bar s$ and $s$ given by the relation
\[
\frac{1}{\bar R_{n}}\frac{1}{T_{g}}=\frac{1}{R_{n}}\frac{1}{\bar T_{g}},
\]
$\bar s$ being known as a function of $s$, we have $u=\frac{d\bar s}{ds}$ and the function $v$ can be deduced immediately.

The one dimensional solution considered will be characteristic if we have $a\lambda-b\beta=0$, i.e.,
\[
\frac{1}{R_{n}}\frac{d\log v}{ds}-\frac{1}{T_{g}}\left(\frac{1}{R_{g}}-\frac{u}{\bar R_{g}}\right)=0,
\]
on the other hand by eliminating $\mu$ between the two equations \eqref{eq:7.XII.3} we find $\lambda=-\alpha$, and then we have the supplementary condition
\[
\frac{d\log uv}{ds}=0.
\]
We can specify $1/\rho,1/\tau,\varpi$ and $u$ as functions of $s$, and $v$ will be determined up to a constant factor. We then have $1/\bar\rho,1/\bar\tau$ and $\bar\varpi$ as we know $1/\bar R_{n},1/\bar R_{g}$ and $1/\bar T_{g}$, i.e., $\cos\bar\varpi/\bar\rho,\sin\bar\varpi/\bar\rho$ and $d\bar\varpi/d\bar s+1/\bar\tau$. The data in this case involve only four arbitrary functions of one variable.


\vspace{12pt}\fsec\emph{Particular case. Minimal surfaces}.\index{minimal surface} If the surface $S$ is minimal, the same will be true for $\bar S$. The relations \eqref{eq:7.XII.3} reduce to $\lambda+\alpha=0,\mu+\beta=0$, from which it results the equation
\[
\frac{du}{u}+\frac{dv}{v}=0.
\]
We then see that the last two quadratic equations of \eqref{eq:7.XII.1} are consequences of the two previous equations. Given the minimal surface $S$, the surface $\bar S$ must satisfy the system
\begin{equation}
  \label{eq:7.XII.7}\tag{XII, 7}
  \left\{
    \begin{gathered}
      \bar\omega^{3}=0,\quad\bar\omega^{1}=u\omega^{1},\quad\bar\omega^{2}=u\omega^{2},\quad\bar\omega_{13}=\frac{m}{u}\omega_{13},\quad\bar\omega_{23}=\frac{m}{u}\omega_{23},\\
      \left[\omega^{1}\frac{du}{u}\right]+[\omega^{2}(\omega_{12}-\bar\omega_{12})]=0,\\
      \left[\omega^{2}\frac{du}{u}\right]-[\omega^{1}(\omega_{12}-\bar\omega_{12})]=0.\\
    \end{gathered}
  \right.
\end{equation}
This system is in involution and its solution is furnished by an arbitrary minimal surface. Tow arbitrary minimal surfaces therefore can be made to conformally correspond to each other with the asymptotic lines and the curvature lines conserved: this is a classical result.

If the surface $S$ is not minimal, the surface $\bar S$, when it exists, will at most depend on arbitrary constants.

\subsection{Two surfaces in point correspondence with the same curvature lines and second fundamental form}
\label{sec:some-surfaces-point-2}

\fsec By relating the two surfaces using their Darboux trihedrals,  we are led to the following closed differential system:
\begin{equation}
  \label{eq:7.XIII.1}\tag{XIII, 1}
  \left\{
    \begin{gathered}
      \omega^{3}=0,\quad\bar\omega^{3}=0,\quad\bar\omega^{1}=u\omega^{1},\quad\bar\omega^{2}=v\omega^{2},\quad(u,v>0),\\
      \omega_{13}=a\omega^{1},\quad\omega_{23}=c\omega^{2},\quad\bar\omega_{13}=\frac{a}{u}\omega^{1},\quad\bar\omega_{23}=\frac{c}{v}\omega^{2},\\
      [\omega^{1}da]+(a-c)[\omega^{2}\omega_{12}]=0,\\
      [\omega^{2}dc]+(a-c)[\omega^{1}\omega_{12}]=0,\\
      [\omega^{1}du]+[\omega^{2}(u\omega_{12}-v\bar\omega_{12})]=0,\\
      [\omega^{2}dv]+[\omega^{1}(u\bar\omega_{12}-v\omega_{12})]=0,\\
      \left[\omega^{1}\left(\frac{da}{a}-\frac{du}{u}\right)\right]+\left[\omega^{2}\left(\omega_{12}-\frac{cu}{av}\bar\omega_{12}\right)\right]=0,\\
      \left[\omega^{2}\left(\frac{dc}{c}-\frac{dv}{v}\right)\right]-\left[\omega^{1}\left(\omega_{12}-\frac{av}{cu}\bar\omega_{12}\right)\right]=0.
    \end{gathered}
  \right.
\end{equation}

The system is not in involution. Two simple linear combinations lead to the quadratic equations
\begin{align*}
  [\omega^{1}(\overline{a+c}\,uv\omega_{12}-\overline{av^{2}+cu^{2}}\,\bar\omega_{12})]&=0,\\
  [\omega^{2}(\overline{a+c}\,uv\omega_{12}-\overline{av^{2}+cu^{2}}\,\bar\omega_{12})]&=0,
\end{align*}
from which we get the new equation
\[
(av^{2}+cu^{2})\bar\omega_{12}=(a+c)uv\omega_{12}
\]
which allows us to suppress for example the last two equations in \eqref{eq:7.XIII.1}.

The new system we obtain is furnished by the equations
\begin{equation}
  \label{eq:7.XIII.2}\tag{XIII, 2}
  \left\{
    \begin{gathered}
      \omega^{3}=0,\quad\bar\omega^{3}=0,\quad\bar\omega^{1}=u\omega^{1},\quad\bar\omega^{2}=v\omega^{2},\\
      \omega_{13}=a\omega^{1},\quad\omega_{23}=c\omega^{2},\quad\bar\omega_{13}=\frac{a}{u}\omega^{1},\quad\bar\omega_{23}=\frac{c}{v}\omega^{2},\\
      (av^{2}+cu^{2})\bar\omega_{12}=(a+c)uv\omega_{12},\\
      [\omega^{1}da]+(a-c)[\omega^{2}\omega_{12}]=0,\\
      [\omega^{2}dc]+(a-c)[\omega^{1}\omega_{12}]=0,\\
      [\omega^{1}du]+cu\frac{u^{2}-v^{2}}{av^{2}+cu^{2}}[\omega^{2}\omega_{12}]=0,\\
      [\omega^{2}dv]+av\frac{u^{2}-v^{2}}{av^{2}+cu^{2}}[\omega^{1}\omega_{12}]=0,\\
      [\{uv(u^{2}-v^{2})(a\,dc-c\,da)+(a+c)(av^{2}-cu^{2})(u\,dv-v\,du)\}\omega_{12}]\\
      +ac\left[(a+c)uv-\frac{av^{2}+cu^{2}}{uv}\right](av^{2}+cu^{2})[\omega^{1}\omega^{2}]=0.
    \end{gathered}
  \right.
\end{equation}

The generic two dimensional integral element depends on $5$ arbitrary parameters. The system is therefore in involution and \emph{its general solution depends on $5$ arbitrary functions of one variable}.

The characteristics are given by the equation
\begin{gather}
  \label{eq:7.XIII.3}\tag{XIII, 3}
  2uv(u^{2}-v^{2})\frac{a^{2}v^{2}-c^{2}u^{2}}{av^{2}+cu^{2}}\omega_{12}[a(\omega^{1})^{2}-c(\omega^{2})^{2}]\omega^{1}\omega^{2}+\\
  [uv(u^{2}-v^{2})(a\,dc-c\,da)+(a+c)(av^{2}-cu^{2})(u\,dv-v\,du)](\omega^{1})^{2}(\omega^{2})^{2}=0.\notag
\end{gather}

Certain possibilities have been left aside. First, we ignore the case of developable surfaces. The last linear equation of \eqref{eq:7.XIII.2} would be an identity if we have $a+c=0$, $av^{2}+cu^{2}=0$, the two surfaces would be minimal with $v=u$, and they would be in conformal correspondance preserving the asymptotic lines, the case studied in the previous problem.

If we have $av^{2}+cu^{2}=0$ without $a+c=0$, the form $\omega_{12}$ will be zero, from which $ac=0$, the case we ignore.


\vspace{12pt}\fsec\emph{Singular solutions} If the form $\omega_{12}$ does not vanish identically, the equation \eqref{eq:7.XIII.3} can be an identity only if we have
\begin{align*}
  (u^{2}-v^{2})(a^{2}v^{2}-c^{2}u^{2})&=0,\\
  uv(u^{2}-v^{2})(a\,dc-c\,da)+(a+c)(av^{2}-cu^{2})(u\,dv-v\,du)&=0.
\end{align*}

If $v=u$, we return to the previous problem, but with one supplementary restriction: the two surfaces must have exactly the same second fundamental form. If the surfaces are not minimal, the last linear equation of \eqref{eq:7.XIII.2} shows that $\bar\omega_{12}=\omega_{12}$, from which $ac(1-uv)=0$ and then $u=1$. The two surfaces are equal, the trivial case.

If $a^{2}v^{2}=c^{2}u^{2}, av=\varepsilon cu$ and if the surfaces are not minimal, we have $\bar\omega_{12}=\varepsilon\omega_{12}$, from which $u=\sqrt{a/c}$, $v=\varepsilon\sqrt{c/a}$. The quadratic equations of \eqref{eq:7.XIII.2} show that the product $ac$ has a constant value. The two surfaces have the same contant positive curvature. The principal curvatures are the same at two corresponding points, but the principal tangent of the principal curvature $a$ in the surface $S$ corresponds to the principal tangent of the principal curvature $c$ in $\bar S$. The surface $S$ begin given, the surface $\bar S$ is determined up to translation.

We leave aside the investigation of the Cauchy problem \footnote{On the determination of surfaces admitting a given fundamental form, see an article by \textsc{E.~Cartan} (\emph{Bull.~Sc.~math.}, \textbf{58}, 1943, pp.~8--32).}.



\subsection[{Surfaces $\bar S$ in point correspondence with a given surface $S$, curvature lines of one correspond to asymptotic lines of the other}]{Surfaces $\bar S$ in point correspondence with a given surface $S$,\\curvature lines of one correspond to asymptotic lines of the other}
\label{sec:surfaces-bar-s}

\fsec We assume, naturally, that the two surfaces $S$ and $\bar S$ both have their two principal curvatures opposite in sign, and we relate their Darboux trihedrals. On the two surfaces will be harmonic conjugate tangents corresponding to each other, both with respect to the principal tangents and with respect to the asymptotes, i.e., tangents determined by the equations
\[
a(\omega^{1})^{2}-c(\omega^{2})=0,\qquad \bar a(\bar \omega^{1})^{2}-\bar c(\bar\omega^{2})^{2}=0.
\]

We therefore have a relation of the form
\[
\bar a (\bar \omega^{1})^{2}-\bar c(\bar \omega^{2})^{2}=\rho[a(\omega^{1})^{2}-c(\omega^{2})^{2}].
\]

On the one hand, the equation $\bar \omega^{1}=0$ is an equation of the asymptotic tangents of $S$. It follows for example, by taking into consideration the preceding relation,
\begin{equation}
  \label{eq:7.XIV.1}\tag{XIV, 1}
  \left\{
    \begin{aligned}
      \sqrt{\bar a}\,\bar\omega^{1}&=\lambda(\sqrt{a}\,\omega^{1}-\sqrt{-c}\,\omega^{2}),\\
      \sqrt{-\bar c}\,\bar\omega^{2}&=\lambda(\sqrt{a}\,\omega^{1}+\sqrt{-c}\,\omega^{2}),
    \end{aligned}
  \right.
\end{equation}
by supposing $a>0,c<0,\bar a>0,\bar c<0$. We deduce from this $\rho=2\lambda^{2}$. As a verification, we have
\begin{align*}
  \bar a(\bar\omega^{1})^{2}+\bar c(\bar \omega^{2})^{2}&=-4\lambda^{2}\sqrt{-ac}\,\omega^{1}\omega^{2},\\
  \sqrt{-\bar{a}\bar{c}}\,\bar\omega^{1}\bar\omega^{2}&=\lambda^{2}[a(\omega^{1})^{2}+c(\omega^{2})^{2}],
\end{align*}
formulae in accordance with the statement of the problem.

By adjoining the equations \eqref{eq:7.XIV.1} the linear equations
\begin{equation}
  \label{eq:7.XIV.2}\tag{XIV, 2}
  \bar\omega^{3}=0,\qquad\bar\omega_{13}=\bar a \bar\omega^{1},\qquad\bar\omega_{23}=\bar c\bar\omega^{2},
\end{equation}
and the quadratic exterior equations resulting by exterior differentiation of \eqref{eq:7.XIV.1} and \eqref{eq:7.XIV.2}, we obtain
\begin{equation}
  \label{eq:7.XIV.3}\tag{XIV, 3}
  \left\{
    \begin{aligned}{}
      [\bar\omega^{1}d\bar a]+(\bar a-\bar c)[\bar\omega^{2}\bar\omega_{12}]&=0,\\
      [\bar\omega^{2}d\bar c]+(\bar a-\bar c)[\bar \omega^{1}\bar \omega_{12}]&=0,\\
      \sqrt{\bar a}\left[\bar\omega^{1}\left(\frac{d\lambda}{\lambda}+\frac{\bar a+\bar c}{\bar a-\bar c}\frac{d\bar a}{2\bar a}\right)\right]-H[\bar \omega^{1}\bar \omega^{2}]&=0,\\
      \sqrt{-\bar c}\left[\bar\omega^{2}\left(\frac{d\lambda}{\lambda}-\frac{\bar a+\bar c}{\bar a-\bar c}\frac{d\bar c}{2\bar c}\right)\right]-K[\bar \omega^{1}\bar \omega^{2}]&=0.\\
    \end{aligned}
  \right.
\end{equation}

The closed differential system of the problem is constituted by the equations \eqref{eq:7.XIV.1}, \eqref{eq:7.XIV.2}, \eqref{eq:7.XIV.3}.

We have set
\begin{align*}
  H[\bar\omega^{1}\bar\omega^{2}]=\lambda\, d(\sqrt{a}\,\omega^{1}-\sqrt{-c}\,\omega^{2})&=\lambda\frac{a+c}{2}\left(\frac{h}{\sqrt{a}}+\frac{k}{\sqrt{-c}}\right)[\omega^{1}\omega^{2}]\\
  &=\frac{(a+c)\sqrt{-\bar{a}\bar{c}}}{4\lambda\sqrt{-ac}}\left(\frac{h}{\sqrt{a}}+\frac{k}{\sqrt{-c}}\right)[\bar\omega^{1}\bar\omega^{2}],\\
  K[\bar\omega^{1}\bar\omega^{2}]=\lambda\, d(\sqrt{a}\,\omega^{1}+\sqrt{-c}\,\omega^{2})&=\lambda\frac{a+c}{2}\left(\frac{h}{\sqrt{a}}-\frac{k}{\sqrt{-c}}\right)[\omega^{1}\omega^{2}]\\
  &=\frac{(a+c)\sqrt{-\bar{a}\bar{c}}}{4\lambda\sqrt{-ac}}\left(\frac{h}{\sqrt{a}}-\frac{k}{\sqrt{-c}}\right)[\bar\omega^{1}\bar\omega^{2}],
\end{align*}
where $h$ and $k$ denote the coefficients which enter in the form $\omega_{12}$:
\[
\omega_{12}=h\omega^{1}+k\omega^{2}.
\]

The two dimensional integral elements depend on four parameters and the number of quadratic equations of \eqref{eq:7.XIV.3} is also four. The system is in involution and \emph{its general solution depends on four arbitrary functions of one variable.}

The \emph{characteristics} are given by the following equation, obtained by setting to zero the determinant of the polar matrix whose columns correspond to the forms $d\bar a,d\bar c,\bar\omega_{12},d\lambda/\lambda$:
\[
\begin{vmatrix}
  \bar\omega^{1}&0&\bar\omega^{2}&0\\
  0&\bar\omega^{2}&\bar\omega^{1}&0\\
  \dfrac{1}{2\sqrt{\bar a}}\dfrac{\bar a+\bar c}{\bar a-\bar c}\bar\omega^{1}&0&0&\sqrt{\bar a}\,\bar\omega^{1}\\
  0&\dfrac{1}{2\sqrt{-\bar c}}\dfrac{\bar a+\bar c}{\bar a-\bar c}\bar\omega^{2}&0&\sqrt{-\bar c}\,\omega^{2}
\end{vmatrix}=0.
\]
Up to a factor that is neither zero nor infinity, this equation is
\[
(\bar a+\bar c)\bar \omega^{1}\bar \omega^{2}[\bar a(\bar\omega^{1})^{2}+\bar c(\bar \omega^{2})^{2}]=0.
\]

\emph{The characteristics are the curvature lines and the asymptotes of the integral surfaces}.

\vspace{12pt}\fsec\emph{Singular solutions}. These are the minimal integral surfaces $(\bar a+\bar c=0)$. It follows from the last two equations of \eqref{eq:7.XIV.3}, by an easy calculation,
\begin{equation}
  \label{eq:7.XIV.4}\tag{XIV, 4}
  \frac{2d\lambda}{\lambda}=\frac{a+c}{\sqrt{-ac}}(h\omega^{1}+k\omega^{2}).
\end{equation}

The function $\lambda$ exists only if the right hand side is an exact differential. If this is the case then $\lambda$ is defined up to an arbitrary constant factor. The surface $\bar S$ is then an arbitrary minimal surface, and the point correspondance between $S$ and $\bar S$ depends on the arbitrary constant that enters in the expression of $\lambda$. If the surface $S$ is minimal, $\lambda$ is a non-zero arbitrary constant.

We will not address the problem of determining the surfaces $S$ for which the equation \eqref{eq:7.XIV.4} is completely integrable, i.e., satisfying the equation
\[
\left[\omega^{1}d\left(h\frac{a+c}{\sqrt{-ac}}\right)\right]+\left[\omega^{2}d\left(k\frac{a+c}{\sqrt{-ac}}\right)\right]=0.
\]

\vspace{12pt}\fsec \emph{General solution. The Cauchy problem.} Let us make a correspondance between a given curve $C$ on $S$, which is neither a curvature line nor an asymptote, and a given curve $\bar C$ and its circumscribed developable surface with $1/\bar\rho,1/\bar\tau$ and the angle $\bar\varpi$ as given functions of $\bar s$. Let us further make a point correspondance between the two curves. The linear equations \eqref{eq:7.XIV.1} and \eqref{eq:7.XIV.2} give, by denoting by $\theta$ and $\bar\theta$ the angles between the positive tangents of the curves $C$ and $\bar C$ with the corresponding vectors $\vec e_{1}$,
\begin{equation}
  \label{eq:7.XIV.5}\tag{XIV, 5}
  \left\{
    \begin{aligned}
    \sqrt{\bar a}\,d\bar s\cos\bar\theta&=\lambda(\sqrt{a}\cos\theta-\sqrt{-c}\sin\theta)ds,\\
    \sqrt{-\bar c}\,d\bar s\sin\bar\theta&=\lambda(\sqrt{a}\cos\theta+\sqrt{-c}\sin\theta)ds,\\
    \frac{\cos\bar\varpi}{\bar\rho}&=\bar a\cos^{2}\bar\theta+\bar c\sin^{2}\bar\theta,\\
    \frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}&=(\bar c-\bar a)\sin\bar\theta\cos\bar\theta.
  \end{aligned}
  \right.
\end{equation}

The relation $d\bar s/ds$ as well as $\theta$ being known, we have
\[
\sqrt{\bar a}\cos\bar\theta=m\lambda,\qquad\sqrt{-\bar c}\sin\bar\theta=n\lambda,
\]
where $m$ and $n$ are known on the curve $C$. We then have
\[
(m^{2}-n^{2})\lambda^{2}=\frac{\cos\bar\varpi}{\bar\rho},
\]
equation which gives $\lambda$. We have, finally,
\[
m^{2}\lambda^{2}\tan\bar\theta+n^{2}\lambda^{2}\cot\bar\theta=-\frac{d\bar\varpi}{d\bar s}-\frac{1}{\bar\tau},
\]
from which we get $\bar \theta$. We deduce from this $\bar a$ and $\bar c$. The one dimensional solution of the system is then completely determined. As a verification, the data effectively depend on four arbitrary functions of one variable.

Suppose now that the line $C$ is an asymptote of $S$, with for example
\[
\sqrt{a}\cos\theta+\sqrt{-c}\sin\theta=0.
\]

The equations \eqref{eq:7.XIV.5} then give $\bar\theta=0$ and
\begin{equation}
  \label{eq:7.XIV.6}\tag{XIV, 6}
  \left\{
    \begin{aligned}
      \sqrt{\bar a}\,d\bar s&=2\lambda\sqrt{a}\cos\theta\, ds,\\
      \frac{\cos\bar\varpi}{\bar\rho}&=\bar a,\\
      \frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}&=0.
    \end{aligned}
  \right.
\end{equation}
There is a supplementary condition. The last equation of \eqref{eq:7.XIV.3} shows that we must have along $\bar C$
\begin{equation}
  \label{eq:7.XIV.7}\tag{XIV, 7}
  \frac{d\lambda}{\lambda}-\frac{\bar a+\bar c}{\bar a-\bar c}\frac{d\bar c}{2\bar c}+\frac{(a+c)\sqrt{\bar a}}{4\lambda\sqrt{-ac}}\left(\frac{h}{\sqrt{a}}-\frac{k}{\sqrt{-c}}\right)d\bar s=0.
\end{equation}

Let use then specify the line $\bar C$. The third equation of \eqref{eq:7.XIV.6} gives $\bar\varpi$ up to a constant, the second gives $\bar a$. If we now specify  the point correspondance between $\bar C$ and $C$, the first equation \eqref{eq:7.XIV.6} gives $\lambda$ and the equation \eqref{eq:7.XIV.7} gives a differential equation in $\bar c$, determining $\bar c$ up to an arbitrary constant. In this case the data depend on three arbitrary functions of one variable.

Finally suppose that the line $C$ is a curvature line of $S$, for example $\theta=0$. The equations \eqref{eq:7.XIV.5} then become
\begin{equation}
  \label{eq:7.XIV.8}\tag{XIV, 8}
  \left\{
    \begin{aligned}
      \sqrt{\bar a}\,d\bar s\cos\bar\theta&=\sqrt{-\bar c}\,d\bar s=\lambda\sqrt{a}\,ds,\\
      \frac{\cos\bar\varpi}{\bar\rho}&=\bar a\cos^{2}\theta+\bar c\sin^{2}\bar\theta=0,\\
      \frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}&=(\bar c-\bar a)\sin\bar\theta\cos\bar\theta.
    \end{aligned}
  \right.
\end{equation}

It is necessary to adjoin a supplementary condition. A linear combination of the quadratic equations \eqref{eq:7.XIV.3} leads to the equation
\[
\left[(\sqrt{\bar a}\,\bar\omega^{1}-\sqrt{-\bar c}\,\bar\omega^{2})\left(\frac{d\lambda}{\lambda}+\frac{\bar a+\bar c}{2\sqrt{-\bar{a}\bar{c}}}\bar \omega_{12}\right)\right]=(H-K)[\bar\omega^{1}\bar\omega^{2}],
\]
from which the condition we seek
\begin{equation}
  \label{eq:7.XIV.9}\tag{XIV, 9}
  \frac{d\lambda}{\lambda}+\frac{\bar a-\bar c}{2\sqrt{-\bar{a}\bar{c}}}\bar\omega_{12}=\frac{\lambda(H-K)}{\sqrt{-\bar{a}\bar{c}}}d\bar s=\frac{n}{\sqrt{-\bar{a}\bar{c}}}d\bar s,
\end{equation}
where $n$ is a known function.

Let us then specify the curve $\bar C$, with $\bar\varpi=\pi/2$, as well as point correspondance between $\bar C$ and $C$. We will have
\[
\sqrt{\bar a}\cos\bar\theta=\sqrt{-\bar c}\sin\bar\theta=m\lambda,
\]
$m$ being known, hence
\[
m^{2}\lambda^{2}=-\frac{1}{\bar\lambda}\sin\bar\theta\cos\bar\theta.
\]

By using the value of $\lambda^{2}$ in \eqref{eq:7.XIV.9}, where we replace $\bar\omega_{12}$ by its value $-d\bar\theta+(\sin\bar\varpi/\bar\rho)d\bar s$, we obtain a differential equation in $\bar\theta$, which determines $\bar\theta$ up to a constant, from which we obtain the values of $\lambda,\bar a$ and $\bar c$. The data again depend on three arbitrary functions of one variable.

\emph{Remark}. The celebrated transformation of S.~Lie which transforms straight lines on spheres into themselves transforms a surface $S$ to a surface $\bar S$ whose asymptotic lines correspond to the curvature lines of $S$ and \emph{vice versa}. But this transformation does not exist in the real domain, and hence the surfaces $\bar S$ corresponding to a surface $S$ depend only on arbitrary constants.


\subsection{Two convex surfaces in point correspondence, the asymptotic lines of one corresponding to the minimal lines of the other}
\label{sec:some-convex-surfaces}

\fsec We say that a surface is convex if its total curvature is positive everywhere. Clearly we can also confine ourselves to portions of surfaces enjoying this property.

The curvature lines correspond to each other on the two surfaces, since the principal tangents are harmonic conjugate with respect to the asymptotic tangents as well as to the minimal tangents.

We attach at different points of the two surfaces the corresponding right Darboux trihedrals. The closed differential system of the problem is formed by the linear equations
\begin{equation}
  \label{eq:7.XV.1}\tag{XV, 1}
  \left\{
    \begin{aligned}
      \omega^{3}&=0,&\bar\omega^{3}&=0,&\bar\omega^{1}&=u\sqrt{a}\,\omega^{1},&\bar\omega^{2}&=u\sqrt{c}\,\omega^{2},\\
      \omega_{13}&=a\omega^{1},&\omega_{23}&=c\omega^{2},&\bar\omega_{13}&=\frac{v}{\sqrt{a}}\omega^{1},&\bar\omega_{23}&=\frac{v}{\sqrt{c}}\omega^{2},
    \end{aligned}
  \right.
\end{equation}
we have assumed that the Darboux trihedrals of the first surface are chosen such that the principal curvatures are positive. We can also assume that $u>0$.

The differential system is completed by the quadratic equations
\begin{equation}
  \label{eq:7.XV.2}\tag{XV, 2}
  \left\{
    \begin{aligned}{}
      [\omega^{1}da]+(a-c)[\omega^{2}\omega_{12}]&=0,\\
      [\omega^{2}dc]+(a-c)[\omega^{1}\omega_{12}]&=0,\\
      \left[\omega^{1}\frac{du}{u}\right]+\left[\omega^{2}\left(\frac{a+c}{2a}\omega_{12}-\sqrt{\frac{c}{a}}\,\bar\omega_{12}\right)\right]&=0,\\
      \left[\omega^{2}\frac{du}{u}\right]+\left[\omega^{1}\left(\sqrt{\frac{a}{c}}\,\bar\omega_{12}-{\frac{a+c}{2c}}\omega_{12}\right)\right]&=0,\\
      \left[\omega^{1}\frac{dv}{v}\right]+\left[\omega^{2}\left(\frac{3a-c}{2a}\omega_{12}-\sqrt{\frac{a}{c}}\,\bar\omega_{12}\right)\right]&=0,\\
      \left[\omega^{2}\frac{dv}{v}\right]+\left[\omega^{1}\left(\sqrt{\frac{c}{a}}\,\bar\omega_{12}-{\frac{3c-a}{2c}}\omega_{12}\right)\right]&=0.
    \end{aligned}
  \right.
\end{equation}

The generic two dimensional integral element depends on six arbitrary parameters. The system is therefore in involution since there are six independent quadratic equations of \eqref{eq:7.XV.2}, and \emph{the general solution depends on six arbitrary functions of one variable.}

The determinant of the polar matrix, whose columns correspond to the forms $da/(a-c),dc/(a-c),du/u,dv/v,\omega_{12},\bar\omega_{12}$ is, after dividing the first two rows by $a-c$,
\[
\begin{vmatrix}
  \omega^{1}&0&0&0&\omega^{2}&0\\
  0&\omega^{2}&0&0&\omega^{1}&0\\
  0&0&\omega^{1}&0&\dfrac{a+c}{2a}\omega^{2}&-\sqrt{\dfrac{c}{a}}\,\omega^{2}\\
  0&0&\omega^{2}&0&-\dfrac{a+c}{2a}\omega^{1}&\sqrt{\dfrac{a}{c}}\,\omega^{1}\\
  0&0&0&\omega^{1}&\dfrac{3a-c}{2a}\omega^{2}&-\sqrt{\dfrac{a}{c}}\,\omega^{2}\\
  0&0&0&\omega^{2}&-\dfrac{3a-c}{2a}\omega^{1}&\sqrt{\dfrac{c}{a}}\,\omega^{1}
\end{vmatrix},
\]
its value is
\[
\frac{(a-c)^{2}}{2ac\sqrt{ac}}\omega^{1}\omega^{2}[(\omega^{1})^{2}+(\omega^{2})^{2}][a(\omega^{1})^{2}+c(\omega^{2})^{2}].
\]

The only real characteristics correspond to the curvature lines of the two surfaces.

There is no singular solution: according to the way we have analytically posed the problem, we exclude portions of the surface containing umbilical points. Nonetheless it is obvious that any pair of spheres constitute a solution of the problem.

\vspace{12pt}\fsec \emph{The Cauchy problem.} Let us specify two curves $C$ and $\bar C$ as well as the correspondance between these two curves. Let us also specify the angles $\varpi$ and $\bar \varpi$, i.e., the circumscribed developable surfaces $\Sigma$ and $\bar\Sigma$. The equations \eqref{eq:7.XV.1} which involve the four unknown functions $a,c,u,v$ as well as the angles $\theta$ and $\bar\theta$ between the tangents of the curves $C$ and $\bar C$ with the corresponding vectors $\vec e_{1}$ can be written here as
\begin{align*}
  d\bar s\cos\bar\theta&=u\sqrt{a}\,ds\cos\theta,&d\bar s\sin\bar\theta&=u\sqrt{c}\,ds\sin\theta,\\
  \frac{\cos\varpi}{\rho}&=a\cos^{2}\theta+c\sin^{2}\theta,&\frac{d\varpi}{ds}+\frac{1}{\tau}&=(c-a)\sin\theta\cos\theta,\\
  \frac{\cos\bar\varpi}{\bar\rho}&=\frac{v}{ua}\cos^{2}\bar\theta+\frac{v}{uc}\sin^{2}\bar\theta,&\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}&=\frac{v}{u}\frac{a-c}{ac}\sin\bar\theta\cos\bar\theta.
\end{align*}
We therefore have six algebraic equations furnishing six unknowns $\theta,\bar\theta,a,c,u,v$.

If the angle $\theta$ is zero ($C$ being a curvature line of $S$), the angle $\bar\theta$ will be zero as well, and the equations reduce to
\begin{equation}
  \label{eq:7.XV.3}\tag{XV, 3}
  \left\{
    \begin{aligned}
      d\bar s&=u\sqrt{a}\,ds,&a&=\frac{\cos\varpi}{\rho},&\frac{d\varpi}{ds}+\frac{1}{\tau}&=0,\\
      &&\frac{v}{ua}&=\frac{\cos\bar\varpi}{\bar\rho},&\frac{d\bar\varpi}{d\bar s}+\frac{1}{\bar\tau}&=0.
    \end{aligned}
  \right.
\end{equation}
But it is necessary to add a supplementary condition that is obtained by finding a linear combination of the equations \eqref{eq:7.XV.2} containing no $\omega^{1}$. We find the new condition
\begin{equation}
  \label{eq:7.XV.4}\tag{XV, 4}
  a\frac{dv}{v}-c\frac{du}{u}+\frac{c-a}{2c}dc=0.
\end{equation}

Let us specify the two curves $C, \bar C$ as well as the point correspondance connecting them. The third and fifth equations of \eqref{eq:7.XV.3} given $\varpi$ and $\bar\varpi$, the second gives $a$, the first gives $u$, the fourth gives $v$ and the equation \eqref{eq:7.XV.4} gives $c$ by means of a differential equation. The data depend on five arbitrary functions of one variable. We must use only the portions of the corresponding curves $C$ and $\bar C$ with positive functions $a$ and $c$.


\vspace{12pt}\fsec \emph{Remark}. The problem which we just investigated and the previous one have great similarities, however there is an essential difference between the two. Given a surface $S$ with inverse curvature, there always exists an infinite number of surfaces $\bar S$ that can be made in point correspondance with $S$ such to have the asymptotes of one surface correspond to the curvature lines of the other. On the contrary given a convex surface $S$, it is in general impossible to find a surface $\bar S$ which can be made in point correspondance with $S$ such that the asymptotes of one correspond to the minimal lines of the other.

\subsection*{General remark}
\label{sec:general-remark}

\fsec The problems that we have investigated have been treated in Euclidean geometry, but they can also arise in non-Euclidean geometry, where the analytic tools are not substantially different, and the results do not need essential changes. The only difference comes from the structural equations. Instead of the equations
\[
d\omega_{23}=[\omega_{12}\omega_{31}],\qquad d\omega_{31}=[\omega_{23}\omega_{12}],\qquad d\omega_{12}=[\omega_{31}\omega_{23}],
\]
we have
\begin{align*}
  d\omega_{23}&=[\omega_{12}\omega_{31}]-\mathbf{C}[\omega^{2}\omega^{3}],\\
  d\omega_{31}&=[\omega_{23}\omega_{12}]-\mathbf{C}[\omega^{3}\omega^{1}],\\
  d\omega_{12}&=[\omega_{31}\omega_{23}]-\mathbf{C}[\omega^{1}\omega^{2}],
\end{align*}
in these formulae $\mathbf{C}$ denotes the constant curvature of the space.

In the applications that we have used the structural formulae, the form $\omega^{3}$ is always zero, and therefore the only change is in the expression of $d\omega_{12}$.

The notions of fundamental forms on a surface do not undergo any changes. The formulae
\begin{align*}
  \omega_{13}\cos\theta+\omega_{23}\sin\theta&=ds\frac{\cos\varpi}{\rho},\\
  \omega_{23}\cos\theta-\omega_{13}\sin\theta&=d\varpi+\frac{ds}{\tau},\\
  d\theta+\omega_{12}&=ds\frac{\sin\varpi}{\rho}
\end{align*}
remain the same, without modification.

Some of the problems treated also make sense in three dimensional Riemannian spaces. We can equally treat the problems in affine, projective, conformal differential geometries, etc., by in each case using the structure equations of the group of the corresponding geometry \footnote{See \textsc{E.~Cartan}, \emph{La th\'eorie des groupes finis et continus et la g\'eom\'etrie diff\'erentielle} (Paris, Gauthier-Villars, 1937).}.

\chapter{Geometric problems with more than two independent variables}
\label{cha:geom-probl-with}

\section{Triple orthogonal systems}
\label{sec:orth-three-syst}

\fsec The search for triple orthogonal systems\index{orthogonal system} in ordinary three dimensional space leads to a very simple closed differential system where we take the unknowns to be the right rectangular trihedrals attached at the differential points $A$ of the space and whose unit basis vectors $\vec e_{1},\vec e_{2},\vec e_{3}$ are respectively normal to three surfaces  of the system passing through $A$.

First recall Darboux's equations (structural equations) of chapter VII:
\begin{equation}
  \label{eq:8.I.1}\tag{I, 1}
  \left\{
    \begin{aligned}
      d\omega^{1}&=-[\omega^{2}\omega_{12}]+[\omega^{3}\omega_{31}],\\
      d\omega^{2}&=-[\omega^{3}\omega_{23}]+[\omega^{1}\omega_{12}],\\
      d\omega^{3}&=-[\omega^{1}\omega_{31}]+[\omega^{2}\omega_{23}],
    \end{aligned}
  \right.
\end{equation}
where $\omega^{1},\omega^{2},\omega^{3}$ are the projections of the vector $\overrightarrow{AA'}$ joining $A$ to an infinitesimally close $A$ onto the axes of the trihedral at the origin $A$, and $\omega_{23},\omega_{31},\omega_{12}$ are the components of the infinitesimally small vector which represent the rotation that rotates the trihedral attached at the point $A$ to be equipollent to the trihedral attached at the point $A'$.

We have to require that each of the equations $\omega^{1}=0,\omega^{2}=0,\omega^{3}=0$ is completely integrable, which gives
\[
[\omega^{1}d\omega^{1}]=0,\qquad[\omega^{2}d\omega^{2}]=0,\qquad[\omega^{3}d\omega^{3}]=0.
\]
We deduce from them immediately, by taking into account \eqref{eq:8.I.1}, the three equations
\begin{equation}
  \label{eq:8.I.2}\tag{I, 2}
  [\omega^{2}\omega^{3}\omega_{23}]=0,\qquad[\omega^{3}\omega^{1}\omega_{31}]=0,\qquad[\omega^{1}\omega^{2}\omega_{12}]=0.
\end{equation}

There is no need to exterior differentiate these equations since it would lead to fourth degree equations, which are always identically satisfied on three dimensional element.

It is clear that the system \eqref{eq:8.I.2} does not impose any condition on the one or two dimensional linear elements for them to be integral. We therefore have, for the reduced characters $s_{0}$ and $s_{1}$,
\[
s_{0}=0,\qquad s_{1}=0.
\]

Now consider a two dimensional element, which establish between $\omega^{1},\omega^{2},\omega^{3}$ a linear relation \footnote{The coefficients $u_{1},u_{2},u_{3}$ are just the Pl\"ucker coordinates $u^{23},u^{31},u^{12}$ of a bivector formed by the two dimensional integral element under consideration with respect to the trihedral at the origin $A$.}
\[
u_{1}\omega^{1}+u_{2}\omega^{2}+u_{3}\omega^{3}=0,
\]
the reduced polar system of this integral element manifestly reduces to
\begin{equation}
  \label{eq:8.I.3}\tag{I, 3}
  u_{1}\omega_{23}=0,\qquad u_{2}\omega_{31}=0,\qquad u_{3}\omega_{12}=0.
\end{equation}

As its rank is $3$, we have $s_{2}=3$, and hence $s_{3}=0$.

On the other hand, the two dimensional integral elements depend on six arbitrary parameters, since the equations \eqref{eq:8.I.2} give
\begin{equation}
  \label{eq:8.I.4}\tag{I, 4}
  \left\{
    \begin{aligned}
      \omega_{23}&=\alpha_{1}\omega_{2}-\beta_{1}\omega_{3},\\
      \omega_{31}&=\alpha_{2}\omega_{3}-\beta_{2}\omega_{1},\\
      \omega_{12}&=\alpha_{3}\omega_{1}-\beta_{3}\omega_{2}.
    \end{aligned}
  \right.
\end{equation}
As the sum $s_{1}+2s_{2}+3s_{3}=2s_{2}$ is equal to $6$, it follows that the system \eqref{eq:8.I.2} is in involution and \emph{its general solution depends on three arbitrary functions of two variables.}

\vspace{12pt}\fsec \emph{The Cauchy problem}. Every non characteristic two dimensional solution of the system \eqref{eq:8.I.2} uniquely determines a triple orthogonal system. Such a solution is obtained by specifying an arbitrary analytic surface $\Sigma$ and attaching at each of its points an arbitrary rectangular trihedral, i.e., by specifying at each point $A$ three rectangular unit vectors $\vec e_{1},\vec e_{2},\vec e_{3}$. There exists, in a sufficiently small neighbourhood of $\Sigma$, a triple orthogonal system such that at the point $A$ of $\Sigma$ the surfaces of the three families passing through $A$ are respectively normal to $\vec e_{1},\vec e_{2},\vec e_{3}$. In particular if we take $\Sigma$ to be a given plane, we will then obtain the most general triple orthogonal systems, each once and only once, and the data effectively depend on three arbitrary functions of the points of $\Sigma$.

\emph{Case where the data are characteristic}. According to \eqref{eq:8.I.3}, the data are characteristic if one of the coefficients $u_{1},u_{2},u_{3}$ of the equation $u_{1}\omega^{1}+u_{2}\omega^{2}+u_{3}\omega^{3}=0$ of the tangent plane of a generic point of $\Sigma$ is zero, or if at each point $A$ of $\Sigma$ the trihedral attached at the point has one of its axes, the first for example, tangent to $\Sigma$. In this case it is easy to see that the problem is in general unsolvable. This follows from the equations \eqref{eq:8.I.4}. If we move in the direction $\vec e_{1}$ on $\Sigma$, we have $\omega^{2}=\omega^{3}=0$, and then the form $\omega_{23}$ must be zero. Therefore for the problem to be solvable it is \emph{necessary} that in moving on $\Sigma$ along one trajectory of vector $\vec e_{1}$, we have the relation
\[
\vec e_{2}\cdot d\vec e_{3}=0.
\]
This relation expresses that by moving along the trajectory of $\vec e_{1}$ (i.e., along the intersection of the surfaces of the last two families of the unknown triple system) the vector $\vec e_{3}$ generates a developable surface. And the same is true for the vector $\vec e_{2}$: this is a consequence of Dupin's theorem\index{Dupin's theorem}.

If two of the coefficients in the equation of the tangent plane to $\Sigma$ at a generic point is zero, then one of the axes of the trihedrals attached at the point, for example the third, is normal to $\Sigma$. Then for the problem to be solvable it is necessary that by moving on $\Sigma$ in the direction $\vec e_{1}$ the form $\omega_{23}$ is zero and by moving in the direction $\vec e_{2}$ the form $\omega_{31}$ is zero. A \emph{necessary} condition for possibility is therefore that the two axes of the trihedral attached at $A$ are the principal tangents at the point of the surface $\Sigma$. This condition is at the same time sufficient, since the family of surfaces parallel to $\Sigma$ and the two families of developable normals of $\Sigma$ constitute a triple orthogonal system corresponding to the data. \emph{This does not mean that they are the only solutions of the problem.}

Finally, observe that given a triple orthogonal system the \emph{characteristic surfaces} of the system are formed by the surfaces formed by the curves of the intersections of two families of triple systems.

\emph{Remark}. We have only used the first structural equations \eqref{eq:8.I.1} of the space. It follows that all the results obtained are valid in spaces of constant curvature as well as general Riemannian spaces.

\section{Triple systems at constant angles}
\label{sec:three-systems-with}

\fsec We can generalise the problem of triple orthogonal system by seeking three one-parameter families os surfaces, with any two families intersect at given constant angles. If we attach at each point $A$ of the space a trihedral whose unit base vector are tangent to the three intersection curves formed by the intersections of two families of the system passing through the point, the faces of the trihedrals are at constant angles, the cosines of which de denote by $\alpha,\beta,\gamma$.

We then have the relations
\begin{equation}
  \label{eq:8.II.1}\tag{II, 1}
  \left\{
    \begin{aligned}
      dA&=\omega^{i}\vec e_{i},\\
      de_{i}&=\omega_{i}{}^{j}\vec e_{j},
    \end{aligned}
  \right.
\end{equation}
with $9$ coefficients $\omega_{i}{}^{j}$ satisfying $6$ linear relations with constant coefficients, which obtain by differentiating the relations
\[
\vec e_{i}{}^{2}=1,\qquad\vec e_{2}\cdot \vec e_{3}=\alpha,\qquad\vec e_{3}\cdot\vec e_{1}=\beta,\qquad\vec e_{1}\cdot\vec e_{2}=\gamma,
\]
which give
\begin{equation}
  \label{eq:8.II.2}\tag{II, 2}
  \left\{
    \begin{aligned}
      \omega_{1}{}^{1}+\gamma\omega_{1}{}^{2}+\beta\omega_{1}{}^{3}&=0,\\
      \omega_{2}{}^{2}+\alpha\omega_{2}{}^{3}+\gamma\omega_{2}{}^{1}&=0,\\
      \omega_{3}{}^{3}+\beta\omega_{3}{}^{1}+\alpha\omega_{3}{}^{2}&=0,\\
      \omega_{2}{}^{3}+\omega_{3}{}^{2}+\alpha(\omega_{2}{}^{2}+\omega_{3}{}^{3})+\beta\omega_{2}{}^{1}+\gamma\omega_{3}{}^{1}&=0,\\
      \omega_{3}{}^{1}+\omega_{1}{}^{3}+\beta(\omega_{3}{}^{3}+\omega_{1}{}^{1})+\gamma\omega_{3}{}^{2}+\alpha\omega_{1}{}^{2}&=0,\\
      \omega_{1}{}^{2}+\omega_{2}{}^{1}+\gamma(\omega_{1}{}^{1}+\omega_{2}{}^{2})+\alpha\omega_{1}{}^{3}+\beta\omega_{2}{}^{3}&=0.
    \end{aligned}
  \right.
\end{equation}

There are therefore three independent forms in $\omega_{i}{}^{j}$, which agrees with the fact that  a moving trihedral which is always identical to itself depends on three parameters.

The structural equations which result from exterior differentiation of the first equation of \eqref{eq:8.II.1} (we have no need for the others) are
\begin{equation}
  \label{eq:8.II.3}\tag{II, 3}
  \left\{
    \begin{aligned}
      d\omega^{1}&=[\omega^{1}\omega_{1}{}^{1}]+[\omega^{2}\omega_{2}{}^{1}]+[\omega^{3}\omega_{3}{}^{1}]=[\omega^{i}\omega_{i}{}^{1}],\\
      d\omega^{2}&=[\omega^{1}\omega_{1}{}^{2}]+[\omega^{2}\omega_{2}{}^{2}]+[\omega^{3}\omega_{3}{}^{2}]=[\omega^{i}\omega_{i}{}^{2}],\\
      d\omega^{3}&=[\omega^{1}\omega_{1}{}^{3}]+[\omega^{2}\omega_{2}{}^{3}]+[\omega^{3}\omega_{3}{}^{3}]=[\omega^{i}\omega_{i}{}^{3}].
    \end{aligned}
  \right.
\end{equation}

\emph{Setting up equations for the problem}. We express, as in the case of triple orthogonal systems, that each of the equations $\omega^{1}=0,\omega^{2}=0,\omega^{3}=0$ is completely integrable, which give, according to \eqref{eq:8.II.3},
\begin{equation}
  \label{eq:8.II.4}\tag{II, 4}
  \left\{
    \begin{aligned}{}
      [\omega^{1}\omega^{2}\omega_{2}{}^{1}]-[\omega^{3}\omega^{1}\omega_{3}{}^{1}]&=0,\\
      [\omega^{2}\omega^{3}\omega_{3}{}^{2}]-[\omega^{1}\omega^{2}\omega_{1}{}^{2}]&=0,\\
      [\omega^{3}\omega^{1}\omega_{1}{}^{3}]-[\omega^{2}\omega^{3}\omega_{2}{}^{3}]&=0.
    \end{aligned}
  \right.
\end{equation}

Differentiating again does not produce new equations and the equations \eqref{eq:8.II.4} form the closed differential system of the problem.

Every one dimensional or two dimensional element is integral. The polar system of the two dimensional integral element $u_{1}\omega^{1}+u_{2}\omega^{2}+u_{3}\omega^{3}=0$ is
\begin{equation}
  \label{eq:8.II.5}\tag{II, 5}
  \left\{
    \begin{aligned}
      u_{3}\omega_{2}{}^{1}-u_{2}\omega_{3}{}^{1}&=0,\\
      u_{1}\omega_{3}{}^{2}-u_{3}\omega_{1}{}^{2}&=0,\\
      u_{2}\omega_{1}{}^{3}-u_{1}\omega_{2}{}^{3}&=0.
    \end{aligned}
  \right.
\end{equation}

Its rank is equal to $3$, and hence we have
\[
s_{1}=0,\qquad s_{2}=3,\qquad s_{3}=0.
\]

On the other hand, the generic three dimensional integral element depends  on $s_{1}+2s_{2}+3s_{3}=6$ arbitrary parameters \footnote{There are actually $9$ forms $\omega_{i}{}^{j}$ and their expression as functions of $\omega^{1},\omega^{2},\omega^{3}$ introduce $27$ coefficients. Each of the equations \eqref{eq:8.II.2} furnishes three relations between the coefficients, which amounts to $18$ in total, and each of the equations \eqref{eq:8.II.5} furnish a new relation, which amounts to \emph{at most} $18+3=21$ relations. It therefore remains \emph{at least} $27-21=6$ arbitrary parameters. However, we know on the other hand that the number $s_{1}+2s_{2}+3s_{3}$ cannot be exceeded.}. The system is therefore in involution and  \emph{its general solution depends on three arbitrary functions of two variables.}

\vspace{12pt}\fsec\emph{The Cauchy problem}. Every \emph{non characteristic} two dimensional solution of the system \eqref{eq:8.II.4} uniquely determines a solution of the problem. We obtain such a solution by specifying an arbitrary surface $\Sigma$ and a trihedral identical to the trihedrals under consideration on each point of this surface. If we take $\Sigma$ to be a fixed plane, the data effectively depend on three arbitrary functions of two variables.

The data are characteristic if the equations \eqref{eq:8.II.5} contain less than three independent equations where $u_{1},u_{2},u_{3}$ are the parameters for the tangent plane of $\Sigma$ referred with respect to the trihedral attached at each point, or if the rank of the system formed by the nine equations \eqref{eq:8.II.2} and \eqref{eq:8.II.5} is less than $9$, or if the determinant of the coefficients of $\omega_{i}{}^{j}$ in these nice equations is zero.

To form the equation which expresses that the determinant is zero, we can first resolve the equations \eqref{eq:8.II.5} by setting
\begin{align*}
  \omega_{2}{}^{1}&=\lambda^{1}u_{2},&\omega_{3}{}^{1}&=\lambda^{1}u_{3},\\
  \omega_{3}{}^{2}&=\lambda^{2}u_{3},&\omega_{1}{}^{2}&=\lambda^{2}u_{1},\\
  \omega_{1}{}^{3}&=\lambda^{3}u_{1},&\omega_{2}{}^{3}&=\lambda^{3}u_{2},
\end{align*}
where $\lambda^{1},\lambda^{2},\lambda^{3}$ denote three auxiliary unknowns. The first three equations of \eqref{eq:8.II.2} then give
\begin{align*}
  \omega_{1}{}^{1}&=-(\beta\lambda^{3}+\gamma\lambda^{2})u_{1},\\
  \omega_{2}{}^{2}&=-(\gamma\lambda^{1}+\alpha\lambda^{3})u_{2},\\
  \omega_{3}{}^{3}&=-(\alpha\lambda^{2}+\beta\lambda^{1})u_{3},
\end{align*}
by substituting in the last three equations \eqref{eq:8.II.2}, we obtain three equations in $\lambda^{1},\lambda^{2},\lambda^{3}$:
\begin{align*}
  [(\beta-\alpha\gamma)u_{2}+(\gamma-\alpha\beta)u_{3}]\lambda^{1}+(a-\alpha^{2})u_{3}\lambda^{2}+(1-\alpha^{2})u_{2}\lambda^{3}&=0,\\
  (1-\beta^{2})u_{3}\lambda^{1}+[(\gamma-\alpha\beta)u_{3}+(\alpha-\beta\gamma)u_{1}]\lambda^{2}+(1-\beta^{2})u_{1}\lambda^{3}&=0,\\
  (1-\gamma^{2})u_{2}\lambda^{1}+(1-\gamma^{2})u_{1}\lambda^{2}+[(\alpha-\beta\gamma)u_{1}+(\beta-\gamma\alpha)u_{2}]\lambda^{3}&=0.
\end{align*}

Eliminating $\lambda^{1},\lambda^{2},\lambda^{3}$ leads to the equation that we seek
\begin{equation}
  \label{eq:8.II.6}\tag{II, 6}
  \begin{vmatrix}
    (\beta-\gamma\alpha)u_{2}+(\gamma-\alpha\beta)u_{3}&(1-\alpha^{2})u_{3}&(1-\alpha^{2})u_{2}\\
    (1-\beta^{2})u_{3}&(\gamma-\alpha\beta)u_{3}+(\alpha-\beta\gamma)u_{1}&(1-\beta^{2})u_{1}\\
    (1-\gamma^{2})u_{2}&(1-\gamma^{2})u_{1}&(\alpha-\beta\gamma)u_{1}+(\beta-\gamma\alpha)u_{2}
  \end{vmatrix}=0.
\end{equation}

This is the equation of a cone of third class (characteristic cone\index{characteristic cone}). The singular integral plane elements at each point $A$ are the tangent planes to the characteristic cone attached at the point. Given one of the triple systems under consideration, the corresponding characteristic surfaces are those whose tangent plane at each point is tangent to the (characteristic) cone attached at the point.

The characteristic cone degenerates into the three axes of the trihedral in the case of triple orthogonal systems. It also degenerates when two faces of the trihedral form a right angle. If for example the vector $\vec e_{3}$ is perpendicular to the two vectors $\vec e_{1},\vec e_{2}$, the cone degenerates into the line along $\vec e_{3}$ and  two lines in the plane $[A\vec e_{1}\vec e_{2}]$ which make an angle of $\pi/4$ with the bisector of the angle formed by the two vectors $\vec e_{1},\vec e_{2}$. There are no other cases where the characteristic cone degenerates.



\section{$p$-tuple orthogonal systems in $p$ dimensional space}
\label{sec:p-tuple-orthogonal}

\fsec The problem is about finding $p$ families of $p-1$ dimensional hypersurfaces intersecting orthogonally among themselves in a $p$ dimensional Euclidean space. We are going to employ the same method as in three dimensional space by attaching at each point $A$ of the space a rectangular $p$-frame formed by $p$ unit vectors $\vec e_{1},\vec e_{2},\dots,\vec e_{p}$ which are perpendicular among themselves and each of them is orthogonal to one of the $p$ hypersurfaces  of the system passing through $A$.

By moving from $A$ to an infinitesimally close point, we have the formulae
\begin{equation}
  \label{eq:8.III.1}\tag{III, 1}
  \left\{
    \begin{aligned}
      dA&=\omega^{i}\vec e_{i},\\
      d\vec e_{i}&=\omega_{ij}\vec e_{j},\qquad(i=1,2,\dots,p),
    \end{aligned}
  \right.
\end{equation}
with the forms $\omega_{ij}=-\omega_{ji}$, the last relations express simply that the vectors $\vec e_{i}$ are of constant length and intersect orthogonally.

The \emph{structural equations} of the space are obtained by exterior differentiation of \eqref{eq:8.III.1} and give
\begin{equation}
  \label{eq:8.III.2}\tag{III, 2}
  \left\{
    \begin{aligned}
      d\omega^{i}&=[\omega^{k}\omega_{ki}],\\
      d\omega_{ij}&=[\omega_{ik}\omega_{kj}].
    \end{aligned}
  \right.
\end{equation}


\vspace{12pt}\fsec Granted these, the differential equations of the problem simply express that each of the equations $\omega^{i}=0$ is completely integrable. The equation $[\omega^{1}d\omega^{1}]$, for example, can be written, on account of \eqref{eq:8.III.2},
\[
[\omega^{1}\omega^{k}\omega_{k1}]=0.
\]

The analogous system of $p$ equations is not in involution. But as the calculations done in the $p=3$ case, it follows that if we set $\omega^{4}=\omega^{5}=\dots=\omega^{p}=0$, the form $\omega_{12}$ does not depend on $\omega^{3}$, and hence it does not depend on $\omega^{4},\dots,\omega^{p}$ either. Then, for every solution of the problem considered, we must have
\begin{equation}
  \label{eq:8.III.3}\tag{III, 3}
  [\omega^{i}\omega^{j}\omega_{ij}]=0,\qquad(i,j=1,2,\dots,p),\qquad\text{(no sum)}.
\end{equation}

Conversely, the equations \eqref{eq:8.III.3} imply the complete integrability of each of the equations $\omega^{i}=0$.

To finish putting the question into equations, we must adjoin to the equations \eqref{eq:8.III.3} those that are deduced by exterior differentiation, which are
\begin{equation}
  \label{eq:8.III.4}\tag{III, 4}
  [\omega^{j}\omega^{k}\omega_{ki}\omega_{ij}]-[\omega^{i}\omega^{k}\omega_{kj}\omega_{ij}]+[\omega^{i}\omega^{j}\omega_{ik}\omega_{kj}]=0,\qquad(i,j=1,2,\dots,p),
\end{equation}
\emph{in these equations the summation is only with respect to the index $k$.}

First, every $p$ dimensional integral element is defined, according to \eqref{eq:8.III.3}, by relations of the form
\begin{equation}
  \label{eq:8.III.5}\tag{III, 5}
  \omega_{ji}=\alpha_{ij}\omega_{j}-\alpha_{ji}\omega_{i},\qquad(i,j=1,2,\dots,p),
\end{equation}
and it is easy to see that, whatever the numerical values of the $p(p-1)$ coefficients $\alpha_{ij}(i\neq j)$, the equations \eqref{eq:8.III.4} are consequences of the equations \eqref{eq:8.III.5}. Indeed, each term on the left hand side of one of the equations of \eqref{eq:8.III.4} is of the form $[\omega^{i}\omega^{j}\omega_{ik}\omega_{jk}]$, where $i,j,k$ denote three fixed  indices taken from the set $1,2,\dots,p$. According to \eqref{eq:8.III.5}, the monomial $[\omega^{i}\omega^{j}\omega_{ik}]$ is a multiple of $[\omega^{i}\omega^{j}\omega^{k}]$ and its exterior product with $\omega_{jk}$ is zero, since $\omega_{jk}$ is linear in $\omega^{j}$ and $\omega^{k}$.

\emph{The $p$ dimensional integral elements therefore depend on $p(p-1)$ arbitrary parameters.}

Let us now seek the reduced characters of the system \eqref{eq:8.III.3}, \eqref{eq:8.III.4}. Every two dimensional element is integral. The polar element of a two dimensional integral element of Pl\"ucker coordinates $u^{ij}$ is defined by the reduced equations
\[
u^{ij}\omega_{ij}=0,\qquad(i,j=1,2,\dots,p),
\]
the rank of this system is
\[
s_{2}=\frac{p(p-1)}{2}.
\]

As $p(p-1)/2$ is the number of forms of $\omega_{ij}$ distinct from the $\omega^{i}$, we have
\[
s_{1}=0,\qquad s_{2}=\frac{p(p-1)}{2},\qquad s_{3}=\dots=s_{p}=0,
\]
and then
\[
s_{1}+2s_{2}+3s_{3}+\dots+ps_{p}=p(p-1),
\]
the number of arbitrary parameters which the generic $p$ dimensional integral element depends on.

The system \eqref{eq:8.III.3}, \eqref{eq:8.III.4} is therefore in involution and \emph{its general solution depends on $p(p-1)/2$ arbitrary functions of two variables.}

\vspace{12pt}\fsec\emph{The Cauchy problem}. Every two dimensional non characteristic solution of the system \eqref{eq:8.III.3}, \eqref{eq:8.III.4} furnishes one and only one solution of the problem. As every two dimensional element is integral, we have the most general two dimensional solution by specifying an arbitrary two dimensional (analytic) surface $\Sigma$ and, at each point $A$ of this surface, a rectangular $p$-frame following an arbitrary analytic rule. If the data are not characteristic, there will exist in the neighbourhood of $\Sigma$ one and only one $p$-tuple orthogonal system that at each point $A$ of $\Sigma$ the $p$ hypersurfaces of this system are respectively normal to the basis vectors of the corresponding $p$-frame. Fixing the surface $\Sigma$, the data will involve $p(p-1)/2$ functions of the two curvilinear coordinates of the surface $\Sigma$.

The data are characteristic if the rank of the polar system of the integral element tangent to $\Sigma$ is less than $p(p-1)/2$, i.e., if at least one of the components $u^{ij}$ is zero. We then need the supplementary conditions necessary for the problem to be solvable.

Suppose for example there is only one vanishing component, say $u^{12}$. This signifies that at each point $A$ of $\Sigma$, at least one of the tangents at $\Sigma$ is perpendicular to the plane determined by $A$ and the two vectors $\vec e_{1}, \vec e_{2}$. There can be no more than one, since otherwise all components $u^{1i}$ and $u^{2i}$ will vanish, the case what we have excluded. This granted, there exists on $\Sigma$ a family of curves enjoying the property that at each of their points the tangent is perpendicular to the plane determined by the point $A$ and the vectors $\vec e_{1},\vec e_{2}$. By moving along one such curve, we have $\omega^{1}=\omega^{2}=0$ and then, according to \eqref{eq:8.III.5}, we have $\omega_{12}=0$, i.e., $\vec e_{1}\cdot d\vec e_{2}=0$. \emph{This is one necessary condition for the possibility of the problem}. It can be expressed by saying that \emph{when we move along any curve of $\Sigma$ orthogonal to the planes $[A\vec e_{1}\vec e_{2}]$}, the trajectory of every point $M=A+x^{1}\vec e_{1}+x^{2}\vec e_{2}$ of fixed coordinates $x^{1},x^{2}$ is orthogonal to the plane $[A\vec e_{1}\vec e_{2}]$ which contains it: we have
\[
\vec e_{1}d\vec M=\omega^{1}+x^{2}\omega_{21}=0,\qquad \vec e_{2}d\vec M=\omega^{2}+x^{1}\omega_{12}=0.
\]

\vspace{12pt}\fsec There can be more complicated cases. Let us confine ourselves to examine what happens in the case $p=4$. We see easily that the possibilities can be divided into the following six cases:

1. one component of $u^{ij}$ is zero, which we take to be $u^{12}$;

2. two components are zero, which we take to be $u^{12},u^{34}$;

3.~and 4. three components of $u^{ij}$ are zero which we take to be $u^{23},u^{31},u^{12}$, or $u^{14},u^{24},u^{34}$;

5. four components of $u^{ij}$ are zero, which we take to be $u^{23},u^{31},u^{12},u^{34}$;

6. five components of $u^{ij}$ are zero, which we take to be $u^{12},u^{13},u^{14},u^{23},u^{24}$.

We are going to see that in each case, there are as many supplementary possibility conditions as there are vanishing components of $u^{ij}$, with the exception of case 6, which has six conditions.

The first two cases have essentially already been considered.

3. \emph{Suppose $u^{23}=u^{31}=u^{12}=0$.} The equations of the tangent plane element at a point of $\Sigma$ are
\[
\frac{\omega^{1}}{u^{14}}=\frac{\omega^{2}}{u^{24}}=\frac{\omega^{3}}{u^{34}},
\]
there is then at each point of $A$ on $\Sigma$ a tangent determined along which we have $\omega^{1}=\omega^{2}=\omega^{3}$, the three forms $\omega_{23},\omega_{31},\omega_{12}$ then become zero. It therefore exists a family of lines $(C)$ of one parameter along which the quantities $\vec e_{2}d\vec e_{3},\vec e_{3}d\vec e_{1},\vec e_{1}d\vec e_{2}$ must be zero for the problem to be solvable. This again means that \emph{if in the three dimensional space $[A\vec e_{1}\vec e_{2}\vec e_{3}]$ attached at each point $A$ of $\Sigma$ we consider a point $M$ of relative coordinates $x^{1},x^{2},x^{3}$ fixed, the location of this point, when the point $A$ describes a curve $(C)$, is a trajectory orthogonal to the corresponding spaces $[A\vec e_{1}\vec e_{2}\vec e_{3}]$.}

4. \emph{Suppose now $u^{14}=u^{24}=u^{34}=0$.} The equations of the tangent plane element at a point of $\Sigma$ are
\[
u^{23}\omega^{1}+u^{31}\omega^{2}+u^{12}\omega^{3}=0,\qquad\omega^{4}=0.
\]
There exists on the surface $\Sigma$ three families of curves of one parameter. The first is formed by the lines $(C_{1})$ along which we have $\omega^{1}=0$, the second the lines $(C_{2})$ along which $\omega^{2}=0$, the third the lines $(C_{3})$ along which $\omega^{3}=0$. The form $\omega_{i4}$ is zero if we move on the lines $(C_{i})$ $(i=1,2,3)$. This therefore gives three possibility conditions each of which has a geometric interpretation analogous to that which has been given in the case where a single component of $u^{ij}$ is zero. \emph{These are the necessary conditions for the existence of a hypersurface passing through $\Sigma$ containing a orthogonal quadruple system.}

5. \emph{Suppose $u^{23}=u^{31}=u^{12}=u^{34}=0$.} The equations of the tangent plane element to $\Sigma$ are
\[
u^{14}\omega^{2}-u^{24}\omega^{1}=0,\qquad\omega^{3}=0.
\]
There exists on $\Sigma$ a family of curves $(C_{1})$ along which we have $\omega^{1}=\omega^{2}=0$ and a family of curves $(C_{2})$ along which we have $\omega^{4}=0$. Along the curves $(C_{1})$ the forms $\omega_{12},\omega_{13},\omega_{23}$ are zero, and along the curves $(C_{2})$ the form $\omega_{34}$ is zero. The consideration of the lines $(C_{1})$ furnishes three consistency conditions, that of the curve $(C_{2})$ the fourth condition.

6. \emph{Finally suppose $u^{12}=u^{13}=u^{23}=u^{14}=u^{24}=0$.} The equations of the plane tangent element on $\Sigma$ are
\[
\omega^{1}=\omega^{2}=0.
\]
There exists on $\Sigma$ a family of curves $(C_{1})$ along which we have $\omega^{3}=0$, and a family of curves $(C_{2})$ along which we have $\omega^{4}=0$. Along the curves $(C_{1})$ the forms $\omega_{12},\omega_{13},\omega_{23}$ are zero, and along the curves $(C_{2})$ the forms $\omega_{12},\omega_{14},\omega_{24}$ are zero. This gives six consistency conditions. \emph{These are six necessary conditions for the surface $\Sigma$ to be regarded as the intersection of two hypersurfaces of a orthogonal quadruple system} (here between a hypersurface of the first family and a hypersurface of the second family).


\vspace{12pt}\fsec\emph{Characteristic varieties}. Given a $p$-tuple orthogonal system, the characteristic varieties of the system are those for which the two dimensional tangent elements are all singular, i.e., those which at least one of their components $u^{ij}$ is zero. Suppose for simplicity $p=4$, and denote by $\xi^{1},\xi^{2},\xi^{3},\xi^{4}$ the parameters of the hypersurfaces of each of these four families of the system. To the six cases considered in the preceding section there correspond six classes of characteristic varieties.

1. $u^{12}=0$. We have the varieties satisfying one equation $f(\xi^{1},\xi^{2})=0$.

2. $u^{12}=u^{34}=0$. We have the varieties defined by two equations
\[
f(\xi^{1},\xi^{2})=0,\qquad\varphi(\xi^{3},\xi^{4})=0.
\]

3. $u^{23}=u^{31}=u^{12}=0$. We have the varieties satisfying the two independent equations
\[
f(\xi^{1},\xi^{2},\xi^{3})=0,\qquad\varphi(\xi^{1},\xi^{2},\xi^{3})=0.
\]

4. $u^{14}=u^{24}=u^{34}=0$. We have the varieties $\xi^{4}=\text{constant}$, i.e., the hypersurfaces of one of the four families of the given system and all the two dimensional varieties contained in such a hypersurface.

5. $u^{23}=u^{31}=u^{12}=u^{34}=0$. We have the varieties satisfying the equations
\[
\xi^{3}=\text{constant},\qquad f(\xi^{1},\xi^{2})=0.
\]

6. $u^{12}=u^{13}=u^{23}=u^{14}=u^{24}=0$. We have the intersections of two hypersurfaces of the quadruple orthogonal system given.

\vspace{12pt}\fsec\emph{Remark}. Nothing changes in the solution of the problem of $p$-tuple orthogonal system if the $p$ dimensional space is of constant curvature. But this is no longer the case if we are in an arbitrary Riemannian space \footnote{See the note in \textsection\textbf{67}.}. The reason is that the equations \eqref{eq:8.III.4} involve the exterior differentials of the forms $\omega_{ij}$ and in general a three dimensional element satisfying equations \eqref{eq:8.III.3} does not satisfy equations \eqref{eq:8.III.4}. This inconvenience does not arise in the problem of triple orthogonal systems. The existence of a $p$-tuple orthogonal system in a $p$ dimensional Riemannian space entails the possibility of represent $ds^{2}$ of the space as a quadratic form
\[
g_{1}(d\xi^{1})^{2}+g_{2}(d\xi^{2})^{2}+\dots+g_{p}(d\xi^{p})^{2},
\]
where the most general $p$ dimensional $ds^{2}$ contains $p(p+1)/2$ arbitrary functions of $p$ variables. It is true that we can always apply a change of variables in a manner to reduce $p$ of the coefficients to fixed numerical values, but it is in general impossible to fix $p(p-1)/2$ of these coefficients, since $p(p-1)/2$ is greater than $p$ when $p$ is greater than $3$.


\section{Realisation of three dimensional Riemannian spaces as varieties of Euclidean space}
\label{sec:real-riem-space}

\fsec We have already considered this problem in the applications of surfaces  (in \textsection\textbf{20} and \textsection\textbf{21}, pp.~\pageref{s20}--\pageref{s21}), which can be considered as the problem of searching a surface having a given $ds^{2}$. An analogous problem arises if we specify a quadratic differential form defined  by three variables: can we find a three dimensional variety in Euclidean space whose $ds^{2}$ is precisely the given form? We are going to show that the problem is always solvable in six dimensional Euclidean space, but it is in general unsolvable in five or four dimensional space.

In Riemannian geometry \footnote{See \textsc{E.~Cartan}, \emph{Le\c{c}on sur la g\'eom\'etrie des espaces de Riemann} (Paris, Gauthier-Villars, 1928, revised and expanded 2nd edition, 1946).}, we can attach at each point $A$ of a three dimensional space a rectangular trihedral as in Euclidean geometry. The infinitesimal displacement of the trihedral can, by a suitable convention, also be defined by six forms $\varpi^{1},\varpi^{2},\varpi^{3},\varpi_{23}=-\varpi_{32},\varpi_{31}=-\varpi_{13},\varpi_{12}=-\varpi_{21}$. The $ds^{2}$ of the space is equal to the sum of squares $(\varpi^{1})^{2}+(\varpi^{2})^{2}+(\varpi^{3})^{2}$. We furthermore have the formulae
\begin{equation}
  \label{eq:8.IV.1}\tag{IV, 1}
  \left\{
    \begin{aligned}
      d\vec A&=\varpi^{1}\vec e_{1}+\varpi^{2}\vec e_{2}+\varpi^{3}\vec e_{3},\\
      D\vec e_{1}&=\varpi_{12}\vec e_{2}-\varpi_{31}\vec e_{3},\\
      D\vec e_{2}&=-\varpi_{12}\vec e_{1}+\varpi_{23}\vec e_{3},\\
      D\vec e_{3}&=\varpi_{31}\vec e_{1}-\varpi_{23}\vec e_{2},
    \end{aligned}
  \right.
\end{equation}
the symbole $D\vec e_{i}$ being a symbol of \emph{covariant differentiation}\index{covariant differentiation}. The structural equations also generalise partially, and they are written as
\begin{equation}
  \label{eq:8.IV.2}\tag{IV, 2}\left\{
  \begin{aligned}
    d\varpi^{1}&=-[\varpi^{2}\varpi_{12}]+[\varpi^{3}\varpi_{31}],\\
    d\varpi^{2}&=[\varpi^{1}\varpi_{12}]-[\varpi^{3}\varpi_{23}],\\
    d\varpi^{3}&=-[\varpi^{1}\varpi_{31}]+[\varpi^{2}\varpi_{23}],\\
    d\varpi_{23}&=[\varpi_{12}\varpi_{31}]-K_{11}[\varpi^{2}\varpi^{3}]-K_{12}[\varpi^{3}\varpi^{1}]-K_{13}[\varpi^{1}\varpi^{2}],\\
    d\varpi_{31}&=[\varpi_{23}\varpi_{12}]-K_{21}[\varpi^{2}\varpi^{3}]-K_{22}[\varpi^{3}\varpi^{1}]-K_{23}[\varpi^{1}\varpi^{2}],\\
    d\varpi_{12}&=[\varpi_{31}\varpi_{23}]-K_{31}[\varpi^{2}\varpi^{3}]-K_{32}[\varpi^{3}\varpi^{1}]-K_{33}[\varpi^{1}\varpi^{2}].\\
  \end{aligned}\right.
\end{equation}

The coefficients $K_{ij}=K_{ji}$ define the \emph{Riemannian curvature}\index{Riemannian curvature} of the space.

If at each point of the space we attach a determined rectangular trihedral, the forms $\varpi^{i},\varpi_{ij}$ are then linear combinations of the differentials of the coordinates $u^{1},u^{2},u^{3}$ of a point in the space, coordinates defined according to some given rule. If on the contrary we attach at each point the most general rectangular trihedral having the point as its origin, the forms $\varpi_{ij}$ then depend linearly on the differentials of three new parameters, distinct from the coordinates of the point of the origin, which serve to fix the orientation of the trihedral.  The formulae \eqref{eq:8.IV.1} and \eqref{eq:8.IV.2} are valid in both cases.

\vspace{12pt}\fsec Before addressing the problem of realising a given Riemannian space as a three dimensional variety in the six dimensional Euclidean space, let us recall the fundamental formulae in the method of the moving rectangular hexahedral, formulae which are a particular case of the formulae \eqref{eq:8.III.1} and \eqref{eq:8.III.2} in \textsection\textbf{61}. By attaching at each point $A$  of the space a rectangular hexahedral defined by six unit rectangular vectors $\vec e_{i}$, we have the relations
\begin{equation}
  \label{eq:8.IV.3}\tag{IV, 3}
  \left\{
    \begin{aligned}
      dA&=\omega^{i}\vec e_{i},\\
      d\vec e_{i}&=\omega_{ij}\vec e_{j},\qquad(\omega_{ij}=-\omega_{ji}),
    \end{aligned}
  \right.
\end{equation}
with the structural equations
\begin{equation}
  \label{eq:8.IV.4}\tag{IV, 4}
  \left\{
    \begin{aligned}
      d\omega^{i}&=[\omega^{k}\omega_{ki}],&(i&=1,2,\dots,6),\\
      d\omega_{ij}&=[\omega_{ik}\omega_{kj}]&(i,j&=1,2,\dots,6).
    \end{aligned}
  \right.
\end{equation}

\vspace{12pt}\fsec This granted, let us specify a three dimensional Riemannian space $\mathcal{E}$ and its structural equations \eqref{eq:8.IV.2} relative to a choice of rectangular trihedrals attached at its different points. The problem that we are investigating consists of finding a correspondance between each of these trihedrals and a rectangular hexahedral in the six dimensional Euclidean space $E_{6}$, whose origin point lies in a three dimensional variety $V$ such that the three vectors $\vec e_{1},\vec e_{2},\vec e_{3}$ of the hexahedral are tangent to $V$, the three others $\vec e_{4},\vec e_{5},\vec e_{6}$ are normal, and finally, under infinitesimal displacement of the hexahedral which corresponds to the infinitesimal displacement of the trihedral of the Riemannian space, we have
\[
(\omega^{1})^{2}+(\omega^{2})^{2}+(\omega^{3})^{2}=(\varpi^{1})^{2}+(\varpi^{2})^{2}+(\varpi^{3})^{2}.
\]

This relation shows that we can pass from the forms $\varpi^{i}$ to $\omega^{i}$ by an orthogonal substitution, which is to say we can, in the three dimensional tangent space to $V$ at a point $M$ in $V$, rotate the vectors $\vec e_{1},\vec e_{2},\vec e_{3}$ from the origin around $M$ so as to have $\omega^{i}=\varpi^{i}$.

Let us denote by the latin letters $i,j,\dots$ the indices $1,2,3$ and by the greek indices $\alpha,\beta,\dots$ the indices $4,5,6$. The problem stated becomes the integration of the system
\begin{equation}
  \label{eq:8.IV.5}\tag{IV, 5}
  \omega^{i}=\varpi^{i},\quad(i=1,2,3),\qquad\omega^{\alpha}=0,\quad(\alpha=4,5,6).
\end{equation}

The exterior differentiation of the first three equations gives, according to \eqref{eq:8.IV.4} and \eqref{eq:8.IV.2},
\begin{align*}
  [\varpi^{2}(\omega_{12}-\varpi_{12})]-[\varpi^{3}(\omega_{31}-\varpi_{31})]&=0,\\
  [\varpi^{3}(\omega_{23}-\varpi_{23})]-[\varpi^{1}(\omega_{12}-\varpi_{12})]&=0,\\
  [\varpi^{1}(\omega_{31}-\varpi_{31})]-[\varpi^{2}(\omega_{23}-\varpi_{23})]&=0,
\end{align*}
from which we deduce
\begin{equation}
  \label{eq:8.IV.6}\tag{IV, 6}
  \omega_{23}=\varpi_{23},\qquad\omega_{31}=\varpi_{31},\qquad\omega_{12}=\varpi_{12}.
\end{equation}

As for the last three equations \eqref{eq:8.IV.5}, they give, by exterior differentiation,
\[
[\varpi^{k}\omega_{k\alpha}]=0,\qquad(\alpha=1,2,3),
\]
where the index of summation $k$ take the values $1,2,3$.

Finally, differentiating the equations \eqref{eq:8.IV.6} gives
\[
[\omega_{ik}\omega_{jk}]+[\omega_{i\lambda}\omega_{j\lambda}]=[\varpi_{ik}\varpi_{jk}]+K_{l1}[\varpi_{2}\varpi_{3}]+K_{l2}[\varpi_{3}\varpi_{1}]+K_{l3}[\varpi_{1}\varpi_{2}],
\]
by denoting by $l$ the latin index that, with the two indices $i,j$, determines an even permutation $(i,j,l)$ of the three indices $1,2,3$.

At last we obtain the closed differential system
\begin{equation}
  \label{eq:8.IV.7}\tag{IV, 7}
  \left\{
    \begin{aligned}
      \omega^{i}&=\varpi^{i},\qquad\omega^{\alpha}=0,\\
      \omega_{ij}&=\varpi_{ij},\\
      [\varpi^{k}\omega_{k\alpha}]&=0,\\
      [\omega_{i\lambda}\omega_{j\lambda}]&=K_{l1}[\varpi_{2}\varpi_{3}]+K_{l2}[\varpi_{3}\varpi_{1}]+K_{l3}[\varpi_{1}\varpi_{2}].
    \end{aligned}
  \right.
\end{equation}
In these $15$ equations, $9$ are linear and $6$ are quadratic, and the three independent variables are the coordinates of a point in the given Riemannian space. The forms $\varpi^{i},\varpi_{ij}$ are known linear combinations of the differentials of these coordinates, and the first three are independent. There is $21$ unknown functions, namely the parameters of the most general rectangular hexahedral in the $6$ dimensional Euclidean space. The forms $\omega^{i},\omega^{\alpha},\omega_{ij},\omega_{i\alpha}$ are $18$ linear independent forms constructed with these $21$ parameters and their differentials. However, in reality, the number of \emph{nonparasitic} unknown functions is equal to $18$, since the equations
\[
\omega^{i}=\omega^{\alpha}=\omega_{ij}=\omega_{i\alpha}=0
\]
express that the origin point of the moving hexahedral as well as the vectors $\vec e_{1},\vec e_{2},\vec e_{3}$ remain fixed. These equations form a completely integrable system whose general solution is formed by the set of figures composed by a point and the three unit rectangular vectors from the point, figures that depend effectively on $18$ parameters ($6$ for the coordinates of the point, $5$ for the components of $\vec e_{1}$, $4$ for those of $\vec e_{2}$ and $3$ for those of $\vec e_{3}$).

Therefore the system \eqref{eq:8.IV.7}  actually involves only the point of $V$, the three dimensional space tangent to each point and in the tangent space the rectangular trihedral defined by the vectors $\vec e_{1},\vec e_{2},\vec e_{3}$. The positions of the vectors $\vec e_{4},\vec e_{5},\vec e_{6}$ furnish the unknown parasites\index{parasite}. Therefore there is actually only $18$ unknown functions, whose differentials appear in the $18$ linearly independent forms $\omega^{i},\omega^{\alpha},\omega_{ij},\omega_{i\alpha}$.


\vspace{12pt}\fsec \emph{Three dimensional integral elements}. We obtain them by resolving the $6$ quadratic equations \eqref{eq:8.IV.7} with respect to the $9$ forms $\omega_{i\alpha}$. The first three quadratic equations give
\begin{equation}
  \label{eq:8.IV.8}\tag{IV, 8}
  \left\{
    \begin{aligned}
      \omega_{14}&=a_{11}\varpi^{1}+a_{12}\varpi^{2}+a_{13}\varpi^{3},\\
      \omega_{24}&=a_{21}\varpi^{1}+a_{22}\varpi^{2}+a_{23}\varpi^{3},\\
      \omega_{34}&=a_{31}\varpi^{1}+a_{32}\varpi^{2}+a_{33}\varpi^{3},\\
      \omega_{15}&=b_{11}\varpi^{1}+b_{12}\varpi^{2}+b_{13}\varpi^{3},\\
      \omega_{25}&=b_{21}\varpi^{1}+b_{22}\varpi^{2}+b_{23}\varpi^{3},\\
      \omega_{35}&=b_{31}\varpi^{1}+b_{32}\varpi^{2}+b_{33}\varpi^{3},\\
      \omega_{16}&=c_{11}\varpi^{1}+c_{12}\varpi^{2}+c_{13}\varpi^{3},\\
      \omega_{26}&=c_{21}\varpi^{1}+c_{22}\varpi^{2}+c_{23}\varpi^{3},\\
      \omega_{36}&=c_{31}\varpi^{1}+c_{32}\varpi^{2}+c_{33}\varpi^{3},
    \end{aligned}
  \right.
\end{equation}
with $a_{ij}=a_{ji}$, $b_{ij}=b_{ji}$, $c_{ij}=c_{ji}$. The $\omega_{i4}$ are the demi-partial derivatives with respect to $\varpi^{i}$ of the quadratic forms
\[
\Phi_{4}=a_{ij}\varpi^{i}\varpi^{j},
\]
the $\omega_{i5}$ and $\omega_{i6}$ also derive from the two quadratic forms
\[
\Phi_{5}=b_{ij}\varpi^{i}\varpi^{j},\qquad\Phi_{6}=c_{ij}\varpi^{i}\varpi^{j}.
\]

The geometric significance of these forms are the following. If we consider a curve traced on the variety $V$ and if we denote by $(\overrightarrow{1/R_{n}})$ the projection of the curvature vector on the normal three dimensional space we have
\[
\overrightarrow{\left(\frac{1}{R_{n}}\right)}ds^{2}=\Phi_{4}\vec e_{4}+\Phi_{5}\vec e_{5}+\Phi_{6}\vec e_{6}.
\]

The other quadratic equations \eqref{eq:8.IV.7} establish between the coefficients of $\Phi_{4},\Phi_{5},\Phi_{6}$ the relations
\begin{equation}
  \label{eq:8.IV.9}\tag{IV, 9}
  A_{ij}+B_{ij}+C_{ij}=K_{ij},
\end{equation}
where $A_{ij},B_{ij},C_{ij}$ denote the minors relative to the elements $a_{ij},b_{ij},c_{ij}$ in the determinants formed with the coefficients of the forms $\Theta_{4},\Theta_{5},\Theta_{6}$.

It follows from this that the generic three dimensional integral element depends on $6\times 3=18$ parameters linked by $6$ (independent) relations, which gives $12$ independent parameters.

\vspace{12pt}\fsec \emph{Calculation of the reduced characters}. Take an linear integral element which we can always assume to satisfy the relations $\varpi^{2}=\varpi^{3}=0$ because of the arbitrary trihedrals that we can attach at the Riemannian space. The reduced polar system of this integral element is
\begin{equation}
  \label{eq:8.IV.10}\tag{IV, 10}
  \left\{
    \begin{aligned}
      \omega_{14}=\omega_{15}=\omega_{16}&=0,\\
      a_{11}\omega_{24}+b_{11}\omega_{25}+c_{11}\omega_{26}&=0,\\
      a_{11}\omega_{34}+b_{11}\omega_{35}+c_{11}\omega_{36}&=0,\\
      a_{12}\omega_{34}+b_{12}\omega_{35}+c_{12}\omega_{36}&-(a_{13}\omega_{24}+b_{13}\omega_{25}+c_{13}\omega_{26})=0.     
    \end{aligned}
  \right.
\end{equation}

The rank of this system is $s_{1}=6$.

If now we take a two dimensional integral element satisfying $\varpi^{3}=0$, the reduced equations of its polar element are
\begin{equation}
  \label{eq:8.IV.11}\tag{IV, 11}
  \left\{
    \begin{aligned}
      \omega_{14}=\omega_{15}=\omega_{16}&=0,\\
      \omega_{24}=\omega_{25}=\omega_{26}&=0,\\
      a_{11}\omega_{34}+b_{11}\omega_{35}+c_{11}\omega_{36}&=0,\\
      a_{12}\omega_{34}+b_{12}\omega_{35}+c_{12}\omega_{36}&=0,\\
      a_{22}\omega_{34}+b_{22}\omega_{35}+c_{22}\omega_{36}&=0.
    \end{aligned}
  \right.
\end{equation}
We have $s_{1}+s_{2}=9$, from which $s_{2}=3$ and then $s_{3}=0$.

As the sum $s_{1}+2s_{2}+3s_{3}=12$ is equal to the number of arbitrary parameters that the generic three dimensional integral element depends on, \emph{the system is in involution and its general solution depends on three arbitrary functions of two variables.}


\vspace{12pt}\fsec \emph{The Cauchy problem}. The problem is about when we specify a surface $\Sigma$ in six dimensional Euclidean space realising a given two dimension variety $S$ in a Riemannian space. We can always suppose that at each point of $S$ we have attached a rectangular trihedral whose third vector is normal to $S$. We therefore will have $\varpi^{3}=0$ for the two dimensional solution of the differential system \eqref{eq:8.IV.7} considered. At each point of $\Sigma$ we must attach a rectangular hexahedral whose vectors $\vec e_{1}$ and $\vec e_{2}$ are well determined and are tangent to $\Sigma$, in a way to satisfy the equations
\begin{equation}
  \label{eq:8.IV.12}\tag{IV, 12}
  \omega^{1}=\varpi^{1},\qquad\omega^{2}=\varpi^{2}.
\end{equation}

We choose the other unit vectors of the hexahedral in any manner and we have $\omega^{3}=\omega^{4}=\omega^{5}=\omega^{6}=0$. The two equations \eqref{eq:8.IV.12} entail
\[
[\varpi^{2}(\omega_{12}-\varpi_{12})]=0,\qquad[\varpi^{1}(\omega_{12}-\varpi_{12})]=0,
\]
from which
\[
\omega_{12}=\varpi_{12}.
\]
For the linear equations of \eqref{eq:8.IV.7} to hold, it is necessary to also have
\[
\omega_{13}=\varpi_{13},\qquad\omega_{23}=\varpi_{23}.
\]

Now the equation $\varpi^{3}=0$ results in, according to the third equation of \eqref{eq:8.IV.2},
\[
[\varpi^{1}\varpi_{13}]+[\varpi^{2}\varpi_{23}]=0,
\]
from which
\[
\varpi_{13}=a\varpi^{1}+b\varpi^{2},\qquad\varpi_{23}=b\varpi^{1}+c\varpi^{2}.
\]

To determine the two dimensional solutions, it is therefore necessary to choose the vector $\vec e_{3}$ normal to $\Sigma$ in a manner to have
\begin{equation}
  \label{eq:8.IV.13}\tag{IV, 13}
  \omega_{13}=a\varpi^{1}+b\varpi^{2},\qquad\omega_{23}=b\varpi^{1}+c\varpi^{2}.
\end{equation}

To make the choice, first suppose we have chosen, following a certain rule, the vectors $\vec e_{3},\vec e_{4},\vec e_{5},\vec e_{6}$ normal to $\Sigma$. Let us denote them by $\vec\varepsilon_{3},\vec\varepsilon_{4},\vec\varepsilon_{5},\vec\varepsilon_{6}$ and set
\begin{align*}
  d\vec e_{1}&=\hat\omega_{13}\vec\varepsilon_{3}+\hat\omega_{14}\vec\varepsilon_{4}+\hat\omega_{15}\vec\varepsilon_{5}+\hat\omega_{16}\vec\varepsilon_{6},\\
  d\vec e_{2}&=\hat\omega_{23}\vec\varepsilon_{3}+\hat\omega_{24}\vec\varepsilon_{4}+\hat\omega_{25}\vec\varepsilon_{5}+\hat\omega_{26}\vec\varepsilon_{6},
\end{align*}
with
\[
\hat\omega_{1\alpha}=h_{\alpha}\varpi^{1}+k_{\alpha}\varpi^{2},\qquad\hat\omega_{2\alpha}=k_{\alpha}\varpi^{1}+l_{\alpha}\varpi^{2},\qquad(\alpha=3,4,5,6).
\]

The vector $\vec e_{3}$ we seek will be the form
\[
\vec e_{3}=x^{3}\vec\varepsilon_{3}+x^{4}\vec\varepsilon_{4}+x^{5}\vec\varepsilon_{5}+x^{6}\vec\varepsilon_{6},
\]
with
\begin{equation}
  \label{eq:8.IV.14}\tag{IV, 14}
  \left\{
    \begin{aligned}
      (x^{3})^{2}+(x^{4})^{2}+(x^{5})^{2}+(x^{6})^{2}&=1,\\
      h_{3}x^{3}+h_{4}x^{4}+h_{5}x^{5}+h_{6}x^{6}&=a,\\
      k_{3}x^{3}+k_{4}x^{4}+k_{5}x^{5}+k_{6}x^{6}&=b,\\
      l_{3}x^{3}+l_{4}x^{4}+l_{5}x^{5}+l_{6}x^{6}&=c.
    \end{aligned}
  \right.
\end{equation}

These four equations of four unknowns can be reduced to linear equations by first calculating the determinant
\begin{equation}
  \label{eq:8.IV.15}\tag{IV, 15}
  \delta=
  \begin{vmatrix}
    x^{3}&x^{4}&x^{5}&x^{6}\\
    h_{3}&h_{4}&h_{5}&h_{6}\\
    k_{3}&k_{4}&k_{5}&k_{6}\\
    l_{3}&l_{4}&l_{5}&l_{6}
  \end{vmatrix}
\end{equation}
whose square is equation to
\[
\Delta=
\begin{vmatrix}
  1&a&b&c\\
  a&\vec h^{2}&\vec h\cdot\vec k&\vec h\cdot\vec l\\
  b&\vec k\cdot\vec h&\vec k^{2}&\vec k\cdot\vec l\\
  c&\vec l\cdot\vec h&\vec l\cdot\vec k&\vec l^{2}
\end{vmatrix},
\]
where $\vec h,\vec k,\vec l$ denote, in the four dimensional space normal to $\Sigma$, three vectors having respectively the components $h_{i},k_{i},l_{i}$ $(i=3,4,5,6)$.

Several cases are possible.

First suppose that the vectors $\vec h,\vec k, \vec l$ are linearly independent. If $\Delta$ is negative, it is clear that the equations \eqref{eq:8.IV.14} do not admit any real solution. If $\Delta$ is positive, they admit two distinct real solutions. We will see in the following section that these data are not characteristic and there passes through the surface $\Sigma$ two three dimensional varieties realising the given Riemannian space $\mathcal{E}$ in a neighbourhood of $\Sigma$. Finally, if $\Delta$ is zero, the equations \eqref{eq:8.IV.14} admit one and only one solution which corresponds to a two dimensional solution of the differential system \eqref{eq:8.IV.7}, but which, as we are going to see in the following section, is characteristic.

If the vectors $\vec h,\vec k,\vec l$ are linearly dependent, and if the system \eqref{eq:8.IV.14} is compatible, each of its solutions corresponds to a two dimensional solution of the system \eqref{eq:8.IV.7}, but, as we are going to see, they are characteristic.


\vspace{12pt}\fsec \emph{Two dimensional characteristic solutions of the system \eqref{eq:8.IV.7}}. The two dimensional integral element $\varpi^{3}=0$ is singular is the rank of the system  \eqref{eq:8.IV.11} is less than nine, which is to say if the determinant
\[
\delta'=
\begin{vmatrix}
  a_{11}&b_{11}&c_{11}\\
  a_{12}&b_{12}&c_{12}\\
  a_{22}&b_{22}&c_{22}
\end{vmatrix}
\]
is zero. Let us start with a two dimensional solution of the system \eqref{eq:8.IV.7} as determined in the preceding section. If we take the vectors $\vec\varepsilon_{3},\vec\varepsilon_{4},\vec\varepsilon_{5},\vec\varepsilon_{6}$ to be the vectors $\vec e_{3},\vec e_{4},\vec e_{5},\vec e_{6}$, the unknowns $x^{3},x^{4},x^{5},x^{6}$ of the equations \eqref{eq:8.IV.14} will have the values $1,0,0,0$, and the vectors $\vec h, \vec k, \vec l$ will have the components
\[
\begin{matrix}
  a,&a_{11},&b_{11},&c_{11};\\
  b,&a_{11},&b_{12},&c_{12};\\
  c,&a_{22},&b_{22},&c_{22}.
\end{matrix}
\]

Granted this, the determinant $\delta'$ is equal to the determinant $\delta$ of \eqref{eq:8.IV.15}, whose square is $\Delta$. For the two dimensional variety considered to be characteristic, it is therefore necessary and sufficient that $\Delta$ is zero. This can happen in two manners: if the vectors $\vec h,\vec k,\vec l$ are linearly independent (the first case examined in \textsection\textbf{72}), or if they are linearly dependent (second case examined in \textsection\textbf{72}).

If the two dimensional solution obtained for the system \eqref{eq:8.IV.7} is characteristic, we are going to show that the solution must satisfy the supplementary conditions for the possibility of the problem. These conditions will be provided by the considerations of equations \eqref{eq:8.IV.9} of \textsection\textbf{70}. We can suppose that, for $\varpi^{3}=0$, the quadratic form $\Phi_{6}$ is identically zero, i.e., the coefficients $c_{11},c_{12},c_{22}$ are zero. The three equations
\begin{align*}
  A_{11}+B_{11}+C_{11}&=K_{11},\\
  A_{12}+B_{12}+C_{12}&=K_{12},\\
  A_{22}+B_{22}+C_{22}&=K_{22},
\end{align*}
are linear in $a_{33}$ and $b_{33}$. The can actually be written as
\begin{equation}
  \label{eq:8.IV.16}\tag{IV, 16}
  \left\{
    \begin{aligned}
      a_{22}a_{33}+b_{22}b_{33}&=a^{2}{}_{23}+b^{2}{}_{23}+c^{2}{}_{13}+K_{11},\\
      a_{12}a_{33}+b_{12}b_{33}&=a_{13}a_{23}+b_{13}b_{23}+c_{13}c_{23}-K_{12},\\
      a_{11}a_{33}+b_{11}b_{33}&=a^{2}{}_{13}+b^{2}{}_{13}+c^{2}{}_{13}+K_{22}.
    \end{aligned}
  \right.
\end{equation}

Now given the two dimensional variety, we know at each point all the coefficients of the linear forms $\Phi_{4},\Phi_{5},\Phi_{6}$ except $a_{33},b_{33},c_{33}$ (The coefficient $a_{13}$ for example is known because of the relation $d\vec e_{3}=a_{31}\vec e_{1}+a_{32}\vec e_{2}$ which is valid on the two dimensional variety). By eliminating $a_{33}$ and $b_{33}$ from the three equations \eqref{eq:8.IV.16}, \emph{we then obtain the necessary condition which every two dimensional characteristic solution must satisfy for there to pass through the variety a three dimensional variety realising the space $\mathcal{E}$ \footnote{This condition generalises the well known condition that a curve $(C)$ in ordinary space must satisfy for there to pass through $C$ a surface with a given $ds^{2}$ when $(C)$ is an asymptotic line: the square of its torsion must be equal to the negative of the Riemannian curvature of the given $ds^{2}$.}.} This condition reduces to the equation
\begin{equation}
  \label{eq:8.IV.17}\tag{IV, 17}
  \begin{vmatrix}
    a_{22}&b_{22}&a^{2}_{23}+b^{2}_{23}+c^{2}_{23}+K_{11}\\
    a_{12}&b_{12}&a_{13}a_{23}+b_{13}b_{23}+c_{13}c_{23}-K_{12}\\
    a_{11}&b_{11}&a^{2}_{13}+b^{2}_{13}+c^{2}_{13}+K_{22}
  \end{vmatrix}=0.
\end{equation}



\vspace{12pt}\fsec \emph{The characteristic varieties of a three dimensional variety realising a given $ds^{2}$}. If the three dimensional variety $V$ in the $6$ dimensional Euclidean space realises a given $ds^{2}$ (a given Riemannian space $\mathcal{E}$), we obtain two dimensional characteristic varieties on it by expressing that the two dimensional tangent plane element $u_{1}\omega^{1}+u_{2}\omega^{2}+u_{3}\omega^{3}=0$ enjoys the property that when displacing along this plane element, the forms $\Phi_{4},\Phi_{5},\Phi_{6}$ are linearly dependent, that is to say there exists three coefficients $\lambda^{4},\lambda^{5},\lambda^{6}$, not all zero, such that the quadratic form $\lambda^{4}\Phi_{4}+\lambda^{5}\Phi_{5}+\lambda^{6}\Phi_{6}$ is divisible by $u_{1}\varpi^{1}+u_{2}\varpi^{2}+u_{3}\varpi^{3}$. This leads to the relation
\begin{equation}
  \label{eq:8.IV.18}\tag{IV, 18}
  \begin{vmatrix}
    u_{1}&0&0&0&u_{3}&u_{2}\\
    0&u_{2}&0&u_{3}&0&u_{1}\\
    0&0&u_{3}&u_{2}&u_{1}&0\\
    a_{11}&a_{22}&a_{33}&2a_{23}&2a_{31}&2a_{12}\\
    b_{11}&b_{22}&b_{33}&2b_{23}&2b_{31}&2b_{12}\\
    c_{11}&c_{22}&c_{33}&2c_{23}&2c_{31}&2c_{12}
  \end{vmatrix}=0.
\end{equation}

If this relation is not identically satisfied, i.e., if the variety $V$ does not constitute a singular solution of the system \eqref{eq:8.IV.7} we obtain a cubic equation in $u_{1},u_{2},u_{3}$ which defines at each point of $V$ a cone of third class. \emph{The characteristic varieties we seek are therefore solutions of the first order partial differential equations defined by the equation \eqref{eq:8.IV.18}}, the tangent plane at each point are tangent to the cone of third class defined by this equation.

\vspace{12pt}\fsec \emph{Singular solutions}. These are the solutions for which the equation \eqref{eq:8.IV.18} in $u_{1},u_{2},u_{3}$ is an identity. This condition permits us to characterise the singular solutions by a purely projective property.

Let us consider in the integral variety $V$ a curve $(C)$ whose tangent at each point $A$ has the directional parameters $\varpi^{1},\varpi^{2},\varpi^{3}$. The plane variety containing both the triplane (the three dimensional plane variety) tangent to $V$ at $A$ and the osculating biplane to $(C)$ at $A$ contains the vector
\[
\Phi_{4}\vec e_{4}+\Phi_{5}\vec e_{5}+\Phi_{6}\vec e_{6},
\]
as the calculation of $d\vec A$ and $d^{2}\vec A$ shows immediately. Granted this, if the biplane $\Pi$ tangent to $V$ at the point $A$ is a singular integral element, this signifies that the places where the osculating plane to the tangent curves to $\Pi$ at the point $A$ are situated in a five dimensional hyperplane. The variety $V$ is a singular solution if this happens regardless of the plane element $\Pi$ tangent to $V$. This is the projective property for the singular solutions we are searching for.

Two cases are then possible, according to whether the five dimensional hyperplane corresponding to the biplane $\Pi$ is independent of the biplane or not. In the first case the hyperplane is what we call the \emph{osculating hyperplane} at $A$ to the variety $V$.

\emph{First case}. In this case we can assume that the osculating hyperplane is normal to the vector $\vec e_{6}$ at each point, in other words the form $\Phi_{6}$ is identically zero. The equations
\[
\omega_{16}=0,\qquad\omega_{26}=0,\qquad\omega_{36}=0
\]
entails, by exterior differentiation,
\begin{align*}
  [\omega_{14}\omega_{46}]+[\omega_{15}\omega_{56}]&=0,\\
  [\omega_{24}\omega_{46}]+[\omega_{25}\omega_{56}]&=0,\\
  [\omega_{34}\omega_{46}]+[\omega_{35}\omega_{56}]&=0.
\end{align*}

It follows, \emph{in general}, $\omega_{46}=\omega_{56}=0$, unless the two conics $\Phi_{4}=0,\Phi_{5}=0$ are bitangents or are in second order contact. \emph{If we leave aside these two cases}, we see that $d\vec e_{6}=0$ and then the osculating hyperplane is fixed. \emph{The corresponding Riemannian spaces are those that may be realised by a three dimensional variety in five dimensional Euclidean space.} We can show that in general their realisation in five dimension is solvable in only one way, up to a translation or a symmetry.

It could happen that the two forms $\Phi_{5}$ and $\Phi_{6}$ are zero: then the Riemannian space may be realised in four dimensional space. If the three forms $\Phi_{4},\Phi_{5},\Phi_{6}$ are zero, the Riemannian space $\mathcal{E}$ will be of zero Riemannian curvature, and it will be, at least locally, realised by a three dimensional Euclidean space.

\emph{Second case}. We will say little about the singular solutions in the second case. We can show easily that the forms $\Phi_{4},\Phi_{5},\Phi_{6}$ all decompose into two factors of first degrees, one of the factors being the same for all three forms, and the other varies. These singular solutions exist only for the spaces $\mathcal{E}$ whose Riemannian quadratic form of coefficients $K_{ij}$ has its discriminant zero. The integral varieties have a very simple geometrical definition: \emph{each of them is generated by an arbitrary one-parameter family of two dimensional plane varieties}. Therefore they depend on $11$ arbitrary functions of one variable, which highlights the very exceptional nature of the Riemannian spaces for which the differential system \eqref{eq:8.IV.7} admits singular solutions in the second case.

As we see, there are still points to be elucidated in the theory of singular solutions of differential systems giving three dimensional varieties in six dimensional Euclidean space realising a given three dimensional Riemannian space.

As for the $n$ dimensional Riemannian spaces, we know that they are realisable by embedded varieties in Euclidean space of $n(n+1)/2$ dimensions. The data of one $n-1$ dimensional non characteristic solution of the differential system of the problem determines, as for $n=3$, a finite number of $n$ dimensional solutions \footnote{For the general problem, see \textsc{M.~Janet} (Annales Soc.~pol.~Math., 5, 1926, pp.~38-43), and \textsc{E.~Cartan} (same journal, 6, 1927, pp.~1--7).}.


{
\chapter*{Bibliography}
\addcontentsline{toc}{chapter}{Bibliography}
\small
\begin{enumerate}
\item \textsc{E.~Cartan}, Sur certaines expressions diff\'erentielles et le probl\`eme de Pfaff (\emph{Annales Ecole Normale}, 3rd series, 16, 1899, p.~230--332).
\item \textsc{E.~Cartan}, Sur l'int\'egration des syst\`emes d'\'equations aux diff\'erentielles totales (\emph{Annales Ecole Normale}, 3rd series, 18, 1901, p.~241--311).
\item \textsc{E.~Cartan}, Sur la structure des groupes infinis de transformations (\emph{Annales Ecole Normale}, 3rd series, 21, 1904, chapter 1).
\item \textsc{C.~Riquier}, Les syst\`emes d'\'equations aux d\'eriv\'ees partielles (Paris, 1910).
\item \textsc{E.~Goursat}, Sur certains syst\`emes d'\'equations aux diff\'erentielles totales et sur une g\'en\'eralisation du probl\`eme de Pfaff (\emph{Annales Fac.~Sc.~Toulouse}, 3rd series, 7, 1915, p.~1--58).
\item \textsc{G.~Cerf}, Remarques sur une g\'en\'eralisation du probl\`eme de Pfaff (\emph{C.~R.}, 170, 1920, p.~374--376).
\item \textsc{E.~Goursat}, Le\c{c}ons sur le probl\`eme de Pfaff (Paris, Hermann, 1922, chapter 8).
\item \textsc{E.~Cartan}, Le\c{c}ons sur les invariants int\'egraux (Paris, Hermann, 1922).
\item \textsc{C.~Riquier}, La m\'ethode des fonctions majorantes et les syst\`emes d'\'equations aux d\'eriv\'ees partielles (M\'em.~Sc.~Math., XXII, 1928).
\item \textsc{M.~Janet}, Le\c{c}ons sur les syst\`emes d'\'equations aux d\'eriv\'ees partielles (Paris, Gauthier-Villars, 1929).
\item \textsc{G.~Fubini} and \textsc{E.~\v{C}ech}, Introduction \`a la g\'eom\'etrie projective diff\'erentielle des surfaces (Paris, Gauthier-Villars, 1931, chapter 12 to 14).
\item \textsc{E.~K\"ahler}, Einf\"uhrung in die Theorie der Systeme von Differentialgleichungen (\emph{Hamburger Math.~Einzelschrifte}, 16, 1934).
\item \textsc{J.~M.~Thomas}, An existence Teorem for generalized pfaffian Systems (\emph{Bull.\ Amer.\ Math.\ Soc.}, 40, 1934, p~309--315).
\item \textsc{J.~M.~Thomas}, Differential Systems (\emph{Amer.~Math.~Soc.~Colloqium Public.}, XXI, 1937).
\item \textsc{J.~A.~Schouten} and \textsc{W.~van der Kulk}, Beitr\"age zur Theorie der Systeme Pfaff'scher Gleichungen (\emph{Proc.~Akad.~Amsterdam}, 43 and 44, 1940; 45, 1941).
\item \textsc{P.~Gillis}, Sur les formes diff\'erentielles et la formule de Stokes (\emph{Th\`ese Fac.~Sc.~Li\'ege}, 1942).
\end{enumerate}

}

%\chapter*{Translator's note}
%\addcontentsline{toc}{chapter}{Translator's note}

%In the preparation of this translation, care has been taken to preserve as much flavour of the original work as possible, including some typographical idiosyncrasies. An index, which contains some anachronistic entries and hence strictly speaking should not be considered part of the work, has been prepared for the convenience of the reader as well.

%\hfill Ziyang \textsc{Hu}

\newpage
\thispagestyle{empty}
\mbox{}

{
\printindex
\addcontentsline{toc}{chapter}{Index}
}

\end{document}
